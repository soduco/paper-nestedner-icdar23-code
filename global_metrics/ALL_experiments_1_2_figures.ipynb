{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation scores on level-1 entities segmentation and classification with joint-labels method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6Oayttuiff2"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ENV_IS_GOOGLE_COLAB = True if 'google.colab' in str(get_ipython()) else False\n",
    "os.environ[\"ENV_IS_GOOGLE_COLAB\"] = str(ENV_IS_GOOGLE_COLAB)\n",
    "\n",
    "if ENV_IS_GOOGLE_COLAB:\n",
    "  from google.colab import drive\n",
    "  mountpoint = Path(\"/content/drive\")\n",
    "  drive.mount(str(mountpoint)) # Mount gdrive to BASE\n",
    "  base = mountpoint / \"MyDrive/article_icdar_2023\" # Adapt this to your situation\n",
    "  sys.path.append(str(base)) # Add BASE to Python Path\n",
    "  BASE = Path(base).resolve() # Make BASE absolute\n",
    "  DATASETS =  BASE / \"dataset_ICDAR\"\n",
    "  OUT_BASE = BASE / \"res_ICDAR\"\n",
    "else:\n",
    "  BASE = Path().resolve() # Directory of this approach\n",
    "  #Adapt this to your situation\n",
    "  DATASETS = Path('../dataset_ICDAR').resolve() #Where your data are located befor Dataset object creation\n",
    "  OUT_BASE = Path('../res_ICDAR').resolve() #Where you save the results of this notebook\n",
    "\n",
    "print(sys.path)\n",
    "print(BASE)\n",
    "print(DATASETS)\n",
    "print(OUT_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TRAINSET_SIZE = 6084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6Oayttuiff2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "def compile_metrics(path): \n",
    "    rundirs = [f for f in os.listdir(path)]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for run_dir in rundirs:\n",
    "        if 'run' in run_dir:\n",
    "            run_path = path / run_dir\n",
    "            nrun = re.search(\"\\d+\",run_dir)[0]\n",
    "\n",
    "            files = [f for f in os.listdir(run_path) if \"test_\" in f and 'json' in f]\n",
    "            sizes = [int(re.search(\"\\d+\",f)[0]) for f in files]\n",
    "\n",
    "            for file, size in zip(files,sizes):\n",
    "                file_path = run_path / file\n",
    "                dftmp = pd.read_json(file_path, typ='series')\n",
    "                dftmp = pd.DataFrame([dftmp])\n",
    "                dftmp[\"trainsize\"] = size \n",
    "                dftmp[\"run\"] = nrun\n",
    "                dftmp[\"trainsize_p\"] = round(100 * size / MAX_TRAINSET_SIZE, 1)\n",
    "                df = pd.concat([df, dftmp])\n",
    "\n",
    "    return df.groupby([\"run\",\"trainsize\"]).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6Oayttuiff2"
   },
   "source": [
    "# Load metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR_REF_M1 = OUT_BASE / \"method_1/m1-110-experiment_1_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Camembert IO metrics from metrics jsons\n",
    "camembert_io_ref_m1 = compile_metrics(METRICS_DIR_REF_M1 / \"111-camembert-ner-multihead-io\")\n",
    "camembert_iob2_ref_m1 = compile_metrics(METRICS_DIR_REF_M1 / \"112-camembert-ner-multihead-iob2\")\n",
    "prtn_camembert_io_ref_m1 = compile_metrics(METRICS_DIR_REF_M1 / \"113-pretrained-camembert-ner-multihead-io\")\n",
    "prtn_camembert_iob2_ref_m1 = compile_metrics(METRICS_DIR_REF_M1 / \"114-pretrained-camembert-multihead-iob2\")\n",
    "metrics_raw_m1_ref = pd.concat([camembert_io_ref_m1,camembert_iob2_ref_m1,prtn_camembert_io_ref_m1,prtn_camembert_iob2_ref_m1], keys=[\"CmBERT IO\", \"CmBERT IOB2\", \"CmBERT+ptrn IO\", \"CmBERT+ptrn IOB2\"],names=[\"Test\"])\n",
    "metrics_raw_m1_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR_OCR_M1 = OUT_BASE / \"method_1/m1-120-experiment_2_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Camembert IO metrics from metrics jsons\n",
    "camembert_io_ocr_m1 = compile_metrics(METRICS_DIR_OCR_M1 / \"121-camembert-ner-multihead-io\")\n",
    "camembert_iob2_ocr_m1 = compile_metrics(METRICS_DIR_OCR_M1 / \"122-camembert-ner-multihead-iob2\")\n",
    "prtn_camembert_io_ocr_m1 = compile_metrics(METRICS_DIR_OCR_M1 / \"123-pretrained-camembert-ner-multihead-io\")\n",
    "prtn_camembert_iob2_ocr_m1 = compile_metrics(METRICS_DIR_OCR_M1 / \"124-pretrained-camembert-multihead-iob2\")\n",
    "metrics_raw_m1_ocr = pd.concat([camembert_io_ocr_m1,camembert_iob2_ocr_m1,prtn_camembert_io_ocr_m1,prtn_camembert_iob2_ocr_m1], keys=[\"CmBERT IO\", \"CmBERT IOB2\", \"CmBERT+ptrn IO\", \"CmBERT+ptrn IOB2\"],names=[\"Test\"])\n",
    "metrics_raw_m1_ocr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadM2M3IOMetrics(path,model_name,res,keys):\n",
    "    if os.path.exists(f\"{path}/run_2\"):\n",
    "        df = compile_metrics(path)\n",
    "        df[\"eval_precision-l1l2\"] = df[\"eval_precision\"]\n",
    "        df[\"eval_recall-l1l2\"] = df[\"eval_recall\"]\n",
    "        df[\"eval_f1-l1l2\"] = df[\"eval_f1\"]\n",
    "        df[\"eval_accuracy-l1l2\"] = df[\"eval_accuracy\"]\n",
    "        res.append(df)\n",
    "        keys.append(model_name)\n",
    "    \n",
    "def loadM2M3IOB2Metrics(path,model_name,res,keys):\n",
    "    if os.path.exists(f\"{path}/run_2\"):\n",
    "        df = compile_metrics(path)\n",
    "        res.append(df)\n",
    "        keys.append(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR_REF_M2 = OUT_BASE / \"method_2/m2-210-experiment_1_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_m2_ref = []\n",
    "keys_m2_ref = []\n",
    "\n",
    "camembert_ner_io_ref_m2 = METRICS_DIR_REF_M2 / \"211-camembert-ner-joint-labelling-io\"\n",
    "loadM2M3IOMetrics(camembert_ner_io_ref_m2,\"CmBERT IO\",res_m2_ref,keys_m2_ref)\n",
    "\n",
    "camembert_ner_iob2_ref_m2 = METRICS_DIR_REF_M2 / \"212-camembert-ner-joint-labelling-iob2\"\n",
    "loadM2M3IOB2Metrics(camembert_ner_iob2_ref_m2,\"CmBERT IOB2\",res_m2_ref,keys_m2_ref)\n",
    "\n",
    "ptrn_camembert_ner_io_ref_m2 = METRICS_DIR_REF_M2 / \"213-pretrained-camembert-ner-joint-labelling-io\"\n",
    "loadM2M3IOMetrics(ptrn_camembert_ner_io_ref_m2,\"CmBERT+ptrn IO\",res_m2_ref,keys_m2_ref)\n",
    "\n",
    "ptrn_camembert_ner_iob2_ref_m2 = METRICS_DIR_REF_M2 / \"214-pretrained-camembert-ner-joint-labelling-iob2\"\n",
    "loadM2M3IOB2Metrics(ptrn_camembert_ner_iob2_ref_m2,\"CmBERT+ptrn IOB2\",res_m2_ref,keys_m2_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_raw_m2_ref = pd.concat(res_m2_ref, keys=keys_m2_ref,names=[\"Test\"])\n",
    "metrics_raw_m2_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR_OCR_M2 = OUT_BASE / \"method_2/m2-220-experiment_2_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_m2_ocr = []\n",
    "keys_m2_ocr = []\n",
    "\n",
    "camembert_ner_io_ocr_m2 = METRICS_DIR_OCR_M2 / \"221-camembert-ner-joint-labelling-io\"\n",
    "loadM2M3IOMetrics(camembert_ner_io_ocr_m2,\"CmBERT IO\",res_m2_ocr,keys_m2_ocr)\n",
    "\n",
    "camembert_ner_iob2_ocr_m2 = METRICS_DIR_OCR_M2 / \"222-camembert-ner-joint-labelling-iob2\"\n",
    "loadM2M3IOB2Metrics(camembert_ner_iob2_ocr_m2,\"CmBERT IOB2\",res_m2_ocr,keys_m2_ocr)\n",
    "\n",
    "ptrn_camembert_ner_io_ocr_m2 = METRICS_DIR_OCR_M2 / \"223-pretrained-camembert-ner-joint-labelling-io\"\n",
    "loadM2M3IOMetrics(ptrn_camembert_ner_io_ocr_m2,\"CmBERT+ptrn IO\",res_m2_ocr,keys_m2_ocr)\n",
    "\n",
    "ptrn_camembert_ner_iob2_ocr_m2 = METRICS_DIR_OCR_M2 / \"224-pretrained-camembert-ner-joint-labelling-iob2\"\n",
    "loadM2M3IOB2Metrics(ptrn_camembert_ner_iob2_ocr_m2,\"CmBERT+ptrn IOB2\",res_m2_ocr,keys_m2_ocr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_raw_m2_ocr = pd.concat(res_m2_ocr, keys=keys_m2_ocr,names=[\"Test\"])\n",
    "metrics_raw_m2_ocr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR_REF_M3 = OUT_BASE / \"method_3/m3-310-experiment_1_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_m3_ref = []\n",
    "keys_m3_ref = []\n",
    "\n",
    "camembert_ner_io_ref_m3 = METRICS_DIR_REF_M3 / \"311-camembert-ner-hierarchical-loss-io\"\n",
    "loadM2M3IOMetrics(camembert_ner_io_ref_m3,\"CmBERT IO\",res_m3_ref,keys_m3_ref)\n",
    "\n",
    "camembert_ner_iob2_ref_m3 = METRICS_DIR_REF_M3 / \"312-camembert-ner-hierarchical-loss-iob2\"\n",
    "loadM2M3IOB2Metrics(camembert_ner_iob2_ref_m3,\"CmBERT IOB2\",res_m3_ref,keys_m3_ref)\n",
    "\n",
    "ptrn_camembert_ner_io_ref_m3 = METRICS_DIR_REF_M3 / \"313-pretrained-camembert-ner-hierarchical-loss-io\"\n",
    "loadM2M3IOMetrics(ptrn_camembert_ner_io_ref_m3,\"CmBERT+ptrn IO\",res_m3_ref,keys_m3_ref)\n",
    "\n",
    "ptrn_camembert_ner_iob2_ref_m3 = METRICS_DIR_REF_M3 / \"314-pretrained-camembert-ner-hierarchical-loss-iob2\"\n",
    "loadM2M3IOB2Metrics(ptrn_camembert_ner_iob2_ref_m3,\"CmBERT+ptrn IOB2\",res_m3_ref,keys_m3_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_raw_m3_ref = pd.concat(res_m3_ref, keys=keys_m3_ref,names=[\"Test\"])\n",
    "metrics_raw_m3_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR_OCR_M3 = OUT_BASE / \"method_3/m3-320-experiment_2_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_m3_ocr = []\n",
    "keys_m3_ocr = []\n",
    "\n",
    "camembert_ner_io_ocr_m3 = METRICS_DIR_OCR_M3 / \"321-camembert-ner-hierarchical-loss-io\"\n",
    "loadM2M3IOMetrics(camembert_ner_io_ocr_m3,\"CmBERT IO\",res_m3_ocr,keys_m3_ocr)\n",
    "\n",
    "camembert_ner_iob2_ocr_m3 = METRICS_DIR_OCR_M3 / \"322-camembert-ner-hierarchical-loss-iob2\"\n",
    "loadM2M3IOB2Metrics(camembert_ner_iob2_ocr_m3,\"CmBERT IOB2\",res_m3_ocr,keys_m3_ocr)\n",
    "\n",
    "ptrn_camembert_ner_io_ocr_m3 = METRICS_DIR_OCR_M3 / \"323-pretrained-camembert-ner-hierarchical-loss-io\"\n",
    "loadM2M3IOMetrics(ptrn_camembert_ner_io_ocr_m3,\"CmBERT+ptrn IO\",res_m3_ocr,keys_m3_ocr)\n",
    "\n",
    "ptrn_camembert_ner_iob2_ocr_m3 = METRICS_DIR_OCR_M3 / \"324-pretrained-camembert-ner-hierarchical-loss-iob2\"\n",
    "loadM2M3IOB2Metrics(ptrn_camembert_ner_iob2_ocr_m3,\"CmBERT+ptrn IOB2\",res_m3_ocr,keys_m3_ocr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_raw_m3_ocr = pd.concat(res_m3_ocr, keys=keys_m3_ocr,names=[\"Test\"])\n",
    "metrics_raw_m3_ocr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = [metrics_raw_m1_ref,metrics_raw_m2_ref,metrics_raw_m3_ref]\n",
    "keys_ref = [\"M1\",\"M2\",\"M3\"]\n",
    "metrics_raw_ref = pd.concat(ref, keys=keys_ref,names=[\"Approach\"])\n",
    "\n",
    "ocr = [metrics_raw_m1_ocr, metrics_raw_m2_ocr,metrics_raw_m3_ocr]\n",
    "keys_ocr = [\"M1\",\"M2\",\"M3\"]\n",
    "metrics_raw_ocr = pd.concat(ocr, keys=keys_ocr,names=[\"Approach\"])\n",
    "\n",
    "datasets = [metrics_raw_ref,metrics_raw_ocr]\n",
    "keys = [\"Reference\",\"OCR\"]\n",
    "metrics_raw = pd.concat(datasets, keys=keys,names=[\"Dataset\"])\n",
    "metrics_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNQ7QaOVBoPV"
   },
   "source": [
    "## 231.1 Build the averaged table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ = []\n",
    "for elem in metrics_raw.columns:\n",
    "    if 'eval_p' in elem or 'eval_re' in elem or 'eval_f' in elem or 'eval_ac' in elem:\n",
    "        eval_.append(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_raw = metrics_raw.groupby(level=(0,1,2)).mean()\n",
    "metrics_raw[eval_] = metrics_raw[eval_].multiply(100., axis=1)\n",
    "metrics_raw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEwY8jAYB8yd"
   },
   "outputs": [],
   "source": [
    "averaged = metrics_raw.copy()\n",
    "averaged = averaged[[\"eval_f1-all\",\"eval_f1-l1l2\",\"eval_f1-l1\",\"eval_f1-l2\",\"eval_f1\",\"eval_f1-das\"]]\n",
    "\n",
    "# Set pretty names\n",
    "averaged.index.names = ['Dataset','Approach',\"Model and tags\"]\n",
    "averaged.rename(columns={\"eval_f1-all\":\"All\",\n",
    "                        \"eval_f1-l1\":\"L1\",\n",
    "                        \"eval_f1-l2\":\"L2\",\n",
    "                         \"eval_f1-l1l2\":\"L1&L2\",\n",
    "                         \"eval_f1\":\"P-L1+P-L2\",\n",
    "                         \"eval_f1-das\":\"Flat\",\n",
    "                         }, errors=\"raise\", inplace=True)\n",
    "#averaged.rename(mapper={\"camembert_io_ref\": \"CmBERT IO\",\"camembert_iob2_ref\": \"CmBERT IOB2\",\"prtn_camembert_io_ref\": \"Ptrn CmBERT IO\",\"prtn_camembert_iob2_ref\": \"Ptrn CmBERT IOB2\"}, errors=\"ignore\", inplace=True, axis=0)\n",
    "averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_ref = averaged.loc[['Reference']]\n",
    "averaged_ref = averaged_ref.reset_index(0)\n",
    "del averaged_ref[\"Dataset\"]\n",
    "latex_table = averaged_ref.copy()\n",
    "caption = \"F1 score measured for each approach, pre-trained model and tag format (mean of 5 runs) on the reference dataset.\"\n",
    "print(latex_table.to_latex(float_format=\"%.1f\", multirow=True, caption=caption))\n",
    "latex_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_ref = averaged.loc[['OCR']]\n",
    "averaged_ref = averaged_ref.reset_index(0)\n",
    "del averaged_ref[\"Dataset\"]\n",
    "latex_table = averaged_ref.copy()\n",
    "caption = \"F1 score measured for each approach, pre-trained model and tag format (mean of 5 runs) on the noisy dataset.\"\n",
    "print(latex_table.to_latex(float_format=\"%.1f\", multirow=True, caption=caption))\n",
    "latex_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtJTnFAyjLte"
   },
   "source": [
    "## 231.2 Create the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811
    },
    "id": "02lAmQ9sn8Es",
    "outputId": "eb6500fa-8eee-4475-f084-4e7ca34ed39d"
   },
   "outputs": [],
   "source": [
    "latex_table = averaged.copy()\n",
    "\n",
    "caption = \"F1 score measured for each approach, pre-trained model and tag format (mean of 5 runs).\"\n",
    "print(latex_table.to_latex(float_format=\"%.1f\", multirow=True, caption=caption))\n",
    "latex_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 232 - Experiments 1 & 2: tables by classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "\n",
    "classes = ['eval_PER','eval_ACT','eval_ACT_L1','eval_ACT_L2','eval_DESC','eval_TITREH','eval_TITREP','eval_SPAT','eval_LOC','eval_CARDINAL','eval_FT'\n",
    " #'eval_TITRE'\n",
    "]\n",
    "\n",
    "def compile_metrics_by_classes(path, classes): \n",
    "    rundirs = [f for f in os.listdir(path)]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for run_dir in rundirs:\n",
    "        if 'run' in run_dir:\n",
    "            run_path = path / run_dir\n",
    "            nrun = re.search(\"\\d+\",run_dir)[0]\n",
    "\n",
    "            files = [f for f in os.listdir(run_path) if \"test_\" in f and 'json' in f]\n",
    "            sizes = [int(re.search(\"\\d+\",f)[0]) for f in files]\n",
    "                \n",
    "            for file, size in zip(files,sizes):\n",
    "                file_path = run_path / file\n",
    "                dftmp = pd.read_json(file_path)\n",
    "                classes_dict = {key: dftmp[key] for key in classes}\n",
    "                dftmp = pd.DataFrame.from_dict(classes_dict)\n",
    "                dftmp = dftmp.T\n",
    "                dftmp['number'] = dftmp['number'].astype(int)\n",
    "                dftmp[\"trainsize\"] = size \n",
    "                dftmp[\"run\"] = nrun\n",
    "                dftmp[\"trainsize_p\"] = round(100 * size / MAX_TRAINSET_SIZE, 1)\n",
    "                df = pd.concat([df, dftmp])\n",
    "                df[\"classe\"] = df.index\n",
    "                \n",
    "    return df.groupby([\"run\",\"classe\"]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatbyClasses(df,classes,metric_name):\n",
    "    m = df[[metric_name]].reset_index(2)\n",
    "    m = m.pivot(columns='classe')\n",
    "    classes = [classe + '-' + metric_name for classe in classes]\n",
    "    m.columns = classes\n",
    "    return m\n",
    "\n",
    "def byClassesDf(metrics_raw_classes,classes):\n",
    "    precision = formatbyClasses(metrics_raw_classes,classes,'precision')\n",
    "    recall = formatbyClasses(metrics_raw_classes,classes,'recall')\n",
    "    f1 = formatbyClasses(metrics_raw_classes,classes,'f1')\n",
    "    number = formatbyClasses(metrics_raw_classes,classes,'number')\n",
    "    tmp = precision.join(recall)\n",
    "    tmp = tmp.join(f1)\n",
    "    tmp = tmp.join(number)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR_OCR_M1 = OUT_BASE / \"method_1/m1-110-experiment_1_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models metrics from metrics jsons\n",
    "camembert_io_ref_m1 = compile_metrics_by_classes(METRICS_DIR_REF_M1 / \"111-camembert-ner-multihead-io\", classes)\n",
    "camembert_iob2_ref_m1 = compile_metrics_by_classes(METRICS_DIR_REF_M1 / \"112-camembert-ner-multihead-iob2\", classes)\n",
    "prtn_camembert_io_ref_m1 = compile_metrics_by_classes(METRICS_DIR_REF_M1 / \"113-pretrained-camembert-ner-multihead-io\",classes)\n",
    "prtn_camembert_iob2_ref_m1 = compile_metrics_by_classes(METRICS_DIR_OCR_M1 / \"114-pretrained-camembert-multihead-iob2\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_m1_ref = pd.concat([camembert_io_ref_m1,camembert_iob2_ref_m1,prtn_camembert_io_ref_m1,prtn_camembert_iob2_ref_m1], keys=[\"CmBERT IO\", \"CmBERT IOB2\", \"CmBERT+ptrn IO\", \"CmBERT+ptrn IOB2\"])\n",
    "metrics_raw_classes_m1_ref = byClassesDf(tmp_m1_ref,classes)\n",
    "metrics_raw_classes_m1_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR_OCR_M1 = OUT_BASE / \"method_1/m1-120-experiment_2_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models metrics from metrics jsons\n",
    "camembert_io_pero_m1 = compile_metrics_by_classes(METRICS_DIR_OCR_M1 / \"121-camembert-ner-multihead-io\", classes)\n",
    "camembert_iob2_pero_m1 = compile_metrics_by_classes(METRICS_DIR_OCR_M1 / \"122-camembert-ner-multihead-iob2\", classes)\n",
    "prtn_camembert_io_pero_m1 = compile_metrics_by_classes(METRICS_DIR_OCR_M1 / \"123-pretrained-camembert-ner-multihead-io\",classes)\n",
    "prtn_camembert_iob2_pero_m1 = compile_metrics_by_classes(METRICS_DIR_OCR_M1 / \"124-pretrained-camembert-multihead-iob2\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_m1_pero = pd.concat([camembert_io_pero_m1,camembert_iob2_pero_m1,prtn_camembert_io_pero_m1,prtn_camembert_iob2_pero_m1], keys=[\"CmBERT IO\", \"CmBERT IOB2\", \"CmBERT+ptrn IO\", \"CmBERT+ptrn IOB2\"])\n",
    "metrics_raw_classes_m1_pero = byClassesDf(tmp_m1_pero,classes)\n",
    "metrics_raw_classes_m1_pero.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR_REF_M2 = OUT_BASE / \"method_2/m2-210-experiment_1_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models metrics from metrics jsons\n",
    "res_classes_m2_ref = []\n",
    "keys_classes_m2_ref = []\n",
    "\n",
    "if os.path.exists(METRICS_DIR_REF_M2 / \"211-camembert-ner-joint-labelling-io/run_2\"):\n",
    "    camembert_ner_io = compile_metrics_by_classes(METRICS_DIR_REF_M2 / \"211-camembert-ner-joint-labelling-io\",classes)\n",
    "    res_classes_m2_ref.append(camembert_ner_io)\n",
    "    keys_classes_m2_ref.append(\"CmBERT IO\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_REF_M2 / \"212-camembert-ner-joint-labelling-iob2/run_2\"):\n",
    "    camembert_ner_iob2 = compile_metrics_by_classes(METRICS_DIR_REF_M2 / \"212-camembert-ner-joint-labelling-iob2\",classes)\n",
    "    res_classes_m2_ref.append(camembert_ner_iob2)\n",
    "    keys_classes_m2_ref.append(\"CmBERT IOB2\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_REF_M2 / \"213-pretrained-camembert-ner-joint-labelling-io/run_2\"):\n",
    "    ptrn_camembert_ner_io = compile_metrics_by_classes(METRICS_DIR_REF_M2 / \"213-pretrained-camembert-ner-joint-labelling-io\",classes)\n",
    "    res_classes_m2_ref.append(ptrn_camembert_ner_io)\n",
    "    keys_classes_m2_ref.append(\"CmBERT+ptrn IO\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_REF_M2 / \"214-pretrained-camembert-ner-joint-labelling-iob2/run_2\"):\n",
    "    ptrn_camembert_ner_iob2 = compile_metrics_by_classes(METRICS_DIR_REF_M2 / \"214-pretrained-camembert-ner-joint-labelling-iob2\",classes)\n",
    "    res_classes_m2_ref.append(ptrn_camembert_ner_iob2)\n",
    "    keys_classes_m2_ref.append(\"CmBERT+ptrn IOB2\")\n",
    "    \n",
    "tmp_m2_ref = pd.concat(res_classes_m2_ref,keys=keys_classes_m2_ref,names=[\"Test\"])\n",
    "metrics_raw_classes_m2_ref = byClassesDf(tmp_m2_ref,classes)\n",
    "metrics_raw_classes_m2_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR_PERO_M2 = OUT_BASE / \"method_2/m2-220-experiment_2_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models metrics from metrics jsons\n",
    "res_classes_m2_pero = []\n",
    "keys_classes_m2_pero = []\n",
    "\n",
    "if os.path.exists(METRICS_DIR_PERO_M2 / \"221-camembert-ner-joint-labelling-io/run_2\"):\n",
    "    camembert_ner_io = compile_metrics_by_classes(METRICS_DIR_PERO_M2 / \"221-camembert-ner-joint-labelling-io\",classes)\n",
    "    res_classes_m2_pero.append(camembert_ner_io)\n",
    "    keys_classes_m2_pero.append(\"CmBERT IO\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_PERO_M2 / \"222-camembert-ner-joint-labelling-iob2/run_2\"):\n",
    "    camembert_ner_iob2 = compile_metrics_by_classes(METRICS_DIR_PERO_M2 / \"222-camembert-ner-joint-labelling-iob2\",classes)\n",
    "    res_classes_m2_pero.append(camembert_ner_iob2)\n",
    "    keys_classes_m2_pero.append(\"CmBERT IOB2\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_PERO_M2 / \"223-pretrained-camembert-ner-joint-labelling-io/run_2\"):\n",
    "    ptrn_camembert_ner_io = compile_metrics_by_classes(METRICS_DIR_PERO_M2 / \"223-pretrained-camembert-ner-joint-labelling-io\",classes)\n",
    "    res_classes_m2_pero.append(ptrn_camembert_ner_io)\n",
    "    keys_classes_m2_pero.append(\"CmBERT+ptrn IO\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_PERO_M2 / \"224-pretrained-camembert-ner-joint-labelling-iob2/run_2\"):\n",
    "    ptrn_camembert_ner_iob2 = compile_metrics_by_classes(METRICS_DIR_PERO_M2 / \"224-pretrained-camembert-ner-joint-labelling-iob2\",classes)\n",
    "    res_classes_m2_pero.append(ptrn_camembert_ner_iob2)\n",
    "    keys_classes_m2_pero.append(\"CmBERT+ptrn IOB2\")\n",
    "    \n",
    "tmp_m2_pero = pd.concat(res_classes_m2_pero,keys=keys_classes_m2_pero,names=[\"Test\"])\n",
    "metrics_raw_classes_m2_pero = byClassesDf(tmp_m2_pero,classes)\n",
    "metrics_raw_classes_m2_pero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR_REF_M3 = OUT_BASE / \"method_3/m3-310-experiment_1_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models metrics from metrics jsons\n",
    "res_classes_m3_ref = []\n",
    "keys_classes_m3_ref = []\n",
    "\n",
    "if os.path.exists(METRICS_DIR_REF_M3 / \"311-camembert-ner-hierarchical-loss-io/run_2\"):\n",
    "    camembert_ner_io = compile_metrics_by_classes(METRICS_DIR_REF_M3 / \"311-camembert-ner-hierarchical-loss-io\",classes)\n",
    "    res_classes_m3_ref.append(camembert_ner_io)\n",
    "    keys_classes_m3_ref.append(\"CmBERT IO\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_REF_M3 / \"312-camembert-ner-hierarchical-loss-iob2/run_2\"):\n",
    "    camembert_ner_iob2 = compile_metrics_by_classes(METRICS_DIR_REF_M3 / \"312-camembert-ner-hierarchical-loss-iob2\",classes)\n",
    "    res_classes_m3_ref.append(camembert_ner_iob2)\n",
    "    keys_classes_m3_ref.append(\"CmBERT IOB2\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_REF_M3 / \"313-pretrained-camembert-ner-hierarchical-loss-io/run_2\"):\n",
    "    ptrn_camembert_ner_io = compile_metrics_by_classes(METRICS_DIR_REF_M3 / \"313-pretrained-camembert-ner-hierarchical-loss-io\",classes)\n",
    "    res_classes_m3_ref.append(ptrn_camembert_ner_io)\n",
    "    keys_classes_m3_ref.append(\"CmBERT+ptrn IO\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_REF_M3 / \"314-pretrained-camembert-ner-hierarchical-loss-iob2/run_2\"):\n",
    "    ptrn_camembert_ner_iob2 = compile_metrics_by_classes(METRICS_DIR_REF_M3 / \"314-pretrained-camembert-ner-hierarchical-loss-iob2\",classes)\n",
    "    res_classes_m3_ref.append(ptrn_camembert_ner_iob2)\n",
    "    keys_classes_m3_ref.append(\"CmBERT+ptrn IOB2\")\n",
    "    \n",
    "tmp_m3_ref = pd.concat(res_classes_m3_ref,keys=keys_classes_m3_ref,names=[\"Test\"])\n",
    "metrics_raw_classes_m3_ref = byClassesDf(tmp_m3_ref,classes)\n",
    "metrics_raw_classes_m3_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR_PERO_M3 = OUT_BASE / \"method_3/m3-320-experiment_2_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models metrics from metrics jsons\n",
    "res_classes_m3_pero = []\n",
    "keys_classes_m3_pero = []\n",
    "\n",
    "if os.path.exists(METRICS_DIR_PERO_M3 / \"321-camembert-ner-hierarchical-loss-io/run_2\"):\n",
    "    camembert_ner_io = compile_metrics_by_classes(METRICS_DIR_PERO_M3 / \"321-camembert-ner-hierarchical-loss-io\",classes)\n",
    "    res_classes_m3_pero.append(camembert_ner_io)\n",
    "    keys_classes_m3_pero.append(\"CmBERT IO\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_PERO_M3 / \"322-camembert-ner-hierarchical-loss-iob2/run_2\"):\n",
    "    camembert_ner_iob2 = compile_metrics_by_classes(METRICS_DIR_PERO_M3 / \"322-camembert-ner-hierarchical-loss-iob2\",classes)\n",
    "    res_classes_m3_pero.append(camembert_ner_iob2)\n",
    "    keys_classes_m3_pero.append(\"CmBERT IOB2\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_PERO_M3 / \"323-pretrained-camembert-ner-hierarchical-loss-io/run_2\"):\n",
    "    ptrn_camembert_ner_io = compile_metrics_by_classes(METRICS_DIR_PERO_M3 / \"323-pretrained-camembert-ner-hierarchical-loss-io\",classes)\n",
    "    res_classes_m3_pero.append(ptrn_camembert_ner_io)\n",
    "    keys_classes_m3_pero.append(\"CmBERT+ptrn IO\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_PERO_M3 / \"324-pretrained-camembert-ner-hierarchical-loss-iob2/run_2\"):\n",
    "    ptrn_camembert_ner_iob2 = compile_metrics_by_classes(METRICS_DIR_PERO_M3 / \"324-pretrained-camembert-ner-hierarchical-loss-iob2\",classes)\n",
    "    res_classes_m3_pero.append(ptrn_camembert_ner_iob2)\n",
    "    keys_classes_m3_pero.append(\"CmBERT+ptrn IOB2\")\n",
    "    \n",
    "tmp_m3_pero = pd.concat(res_classes_m3_pero,keys=keys_classes_m3_pero,names=[\"Test\"])\n",
    "metrics_raw_classes_m3_pero = byClassesDf(tmp_m3_pero,classes)\n",
    "metrics_raw_classes_m3_pero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_raw_classes_m2_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_classes = [metrics_raw_classes_m1_ref,metrics_raw_classes_m2_ref,metrics_raw_classes_m3_ref]\n",
    "keys_ref = [\"M1\",\"M2\",\"M3\"]\n",
    "metrics_raw_classes_ref = pd.concat(ref_classes, keys=keys_ref,names=[\"Approach\"])\n",
    "\n",
    "ocr_classes = [metrics_raw_classes_m1_pero,metrics_raw_classes_m2_pero,metrics_raw_classes_m3_pero]\n",
    "keys_ocr = [\"M1\",\"M2\",\"M3\"]\n",
    "metrics_raw_classes_ocr = pd.concat(ocr_classes, keys=keys_ocr,names=[\"Approach\"])\n",
    "\n",
    "datasets = [metrics_raw_classes_ref,metrics_raw_classes_ocr]\n",
    "keys = [\"Reference\",\"OCR\"]\n",
    "metrics_raw_classes = pd.concat(datasets, keys=keys,names=[\"Dataset\"])\n",
    "metrics_raw_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ = []\n",
    "for elem in metrics_raw_classes.columns:\n",
    "    if 'number' not in elem and 'f1' in elem:\n",
    "        eval_.append(elem)\n",
    "        \n",
    "metrics_raw_classes = metrics_raw_classes[eval_].groupby(level=(0,1,2)).mean()\n",
    "metrics_raw_classes[eval_] = metrics_raw_classes[eval_].multiply(100., axis=1)\n",
    "metrics_raw_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_classes = metrics_raw_classes.copy()\n",
    "\n",
    "start_classes = list(averaged_classes.columns)\n",
    "final_classes = [classe.replace('eval_','') for classe in classes]\n",
    "columns_names = {start_classes[i]: final_classes[i] for i in range(len(final_classes))}\n",
    "\n",
    "# Set pretty names\n",
    "averaged_classes.index.names = ['Dataset','Approach',\"Model & tags\"]\n",
    "averaged_classes.rename(columns=columns_names, errors=\"raise\", inplace=True)\n",
    "\n",
    "classes_f = ['PER','ACT','DESC','TITREH','TITREP','SPAT','LOC','CARDINAL','FT']\n",
    "averaged_classes = averaged_classes[classes_f]\n",
    "averaged_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table_classes = averaged_classes.copy()\n",
    "\n",
    "caption = \"F1 score measured for each approach, pre-trained model and tag format (mean of 5 runs) for each entity type.\"\n",
    "print(latex_table_classes.to_latex(float_format=\"%.1f\", multirow=True, caption=caption))\n",
    "latex_table_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_classes_ref = averaged_classes.loc[['Reference']]\n",
    "averaged_classes_ref = averaged_classes_ref.reset_index(0)\n",
    "del averaged_classes_ref[\"Dataset\"]\n",
    "latex_table = averaged_classes_ref.copy()\n",
    "caption = \"F1 score measured for each approach, pre-trained model and tag format (mean of 5 runs) on the reference dataset for each entity type.\"\n",
    "print(latex_table.to_latex(float_format=\"%.1f\", multirow=True, caption=caption))\n",
    "latex_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_classes_ocr = averaged_classes.loc[['OCR']]\n",
    "averaged_classes_ocr = averaged_classes_ocr.reset_index(0)\n",
    "del averaged_classes_ocr[\"Dataset\"]\n",
    "latex_table = averaged_classes_ocr.copy()\n",
    "caption = \"F1 score measured for each approach, pre-trained model and tag format (mean of 5 runs) on the noisy dataset for each entity type.\"\n",
    "print(latex_table.to_latex(float_format=\"%.1f\", multirow=True, caption=caption))\n",
    "latex_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DAS22-experiment1-figures.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
