{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda9d76b",
   "metadata": {},
   "source": [
    "# 40 - Flat NER - Qualitative analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53ce29-cd5f-4295-ac93-2677c2d07034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ENV_IS_GOOGLE_COLAB = True if 'google.colab' in str(get_ipython()) else False\n",
    "os.environ[\"ENV_IS_GOOGLE_COLAB\"] = str(ENV_IS_GOOGLE_COLAB)\n",
    "\n",
    "if ENV_IS_GOOGLE_COLAB:\n",
    "  from google.colab import drive\n",
    "  mountpoint = Path(\"/content/drive\")\n",
    "  drive.mount(str(mountpoint)) # Mount gdrive to BASE\n",
    "  base = mountpoint / \"MyDrive/article_icdar_2023\" # Adapt this to your situation\n",
    "  sys.path.append(str(base)) # Add BASE to Python Path\n",
    "  BASE = Path(base).resolve() # Make BASE absolute\n",
    "  DATASETS =  BASE / \"dataset_ICDAR\"\n",
    "  OUT_BASE = BASE / \"res_ICDAR/method_0\"\n",
    "else:\n",
    "  BASE = Path().resolve() # Directory of this approach\n",
    "  #Adapt this to your situation\n",
    "  DATASETS = Path('../dataset_ICDAR').resolve() #Where your data are located befor Dataset object creation\n",
    "  OUT_BASE = Path('../res_ICDAR/method_0').resolve() #Where you save the results of this notebook\n",
    "\n",
    "print(sys.path)\n",
    "print(BASE)\n",
    "print(DATASETS)\n",
    "print(OUT_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2021c2a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9b68e",
   "metadata": {},
   "source": [
    "Choose a fine-tuned model to perform qualitative analysis. Load models from the HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec608aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936652c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['nlpso/m0_flat_ner_ref_cmbert_io',\n",
    "                 'nlpso/m0_flat_ner_ref_ptrn_cmbert_io',\n",
    "                 'nlpso/m0_flat_ner_ocr_cmbert_io',\n",
    "                 'nlpso/m0_flat_ner_ocr_ptrn_cmbert_io']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = widgets.RadioButtons(\n",
    "            options=models,\n",
    "            layout={'width': 'max-content'}\n",
    "        )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d03d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a9fd81",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT = 'IO'\n",
    "MODEL = model.value\n",
    "\n",
    "if 'ref' in str(MODEL):\n",
    "    SET = \"ref\"\n",
    "elif 'ocr' in str(MODEL):\n",
    "    SET = \"ocr\"\n",
    "\n",
    "if 'ptrn' in MODEL:\n",
    "    MODEL_TYPE = 'pretrained_camembert_ner'\n",
    "else:\n",
    "    MODEL_TYPE = 'camembert_ner'\n",
    "    \n",
    "if MODEL == 'nlpso/m0_flat_ner_ref_cmbert_io':\n",
    "    DATASET = 'nlpso/m0_qualitative_analysis_ref_cmbert_io'\n",
    "elif MODEL == 'nlpso/m0_flat_ner_ref_ptrn_cmbert_io':\n",
    "    DATASET = 'nlpso/m0_qualitative_analysis_ref_ptrn_cmbert_io'\n",
    "elif MODEL == 'nlpso/m0_flat_ner_ocr_cmbert_io':\n",
    "    DATASET = 'nlpso/m0_qualitative_analysis_ocr_cmbert_io'\n",
    "elif MODEL == 'nlpso/m0_flat_ner_ocr_ptrn_cmbert_io':\n",
    "    DATASET = 'nlpso/m0_qualitative_analysis_ocr_ptrn_cmbert_io'\n",
    "    \n",
    "print(f\"MODEL : {MODEL}\")\n",
    "print(f\"MODEL TYPE : {MODEL_TYPE}\")\n",
    "print(f\"FORMAT : {FORMAT}\")\n",
    "print(f\"SET : {SET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb597f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa81965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "from datasets import load_dataset\n",
    "\n",
    "TRAINSETS_SIZES = [6084]\n",
    "train_dev_test = load_dataset(DATASET)\n",
    "test = train_dev_test[\"test\"]\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd571ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PATH = f\"{DATASETS}/qualitative_analysis/test_entries_{SET}.txt\"\n",
    "with open(PATH, 'r',encoding='utf8') as ex:\n",
    "    lines = ex.read()\n",
    "    lines = lines.split('\\n')\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba547f",
   "metadata": {},
   "source": [
    "## Use model on entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b00a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "\n",
    "LIMIT = 100\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL)\n",
    "\n",
    "#Classification des entit√©s\n",
    "nlp = TokenClassificationPipeline(model=model, tokenizer=tokenizer, aggregation_strategy=None, ignore_labels=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e412721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aggregate_predictions import xmlize, group_sub_entities, get_tag, group_entities\n",
    "\n",
    "all_res = []\n",
    "stats = []\n",
    "idx = 0\n",
    "for i in range(len(lines)):\n",
    "    res = nlp(lines[i])\n",
    "    xml = xmlize(lines[i],group_entities(res,tokenizer))\n",
    "    preds_tokens, preds_tags = [],[]\n",
    "    preds_tokens = [d['word'] for d in res]\n",
    "    preds_tags = [f['entity'] for f in res]\n",
    "    try:\n",
    "        assert len(test[i][\"tokens\"]) == len(preds_tokens)\n",
    "        assert len(test[i][\"ner_tags\"]) == len(preds_tags)\n",
    "        stats.append([idx,lines[i],xml,test[i][\"tokens\"],test[i][\"ner_tags\"],preds_tokens,preds_tags])\n",
    "    except:\n",
    "        print(idx)\n",
    "        print(lines[i][:-1])\n",
    "        print(res)\n",
    "    all_res.append(res)\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd552df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = [\"index\",\"entry\",\"entry_xml\",\"spans_gold\",\"tags_gold\",\"spans_preds\",\"tags_preds\"]\n",
    "df = pd.DataFrame(stats,columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd9c68",
   "metadata": {},
   "source": [
    "## F1-Score ranking\n",
    "F1-Score is calculated for each entry using seqeval library :\n",
    "* entities are rebuild using joint-labels with seqeval lib\n",
    "* f1-score is calculted for each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f979bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "scores = []\n",
    "count = 0\n",
    "for i in range(len(df)):\n",
    "    y_preds = df.iloc[i][\"tags_preds\"]\n",
    "    y_true = df.iloc[i][\"tags_gold\"]\n",
    "    try:\n",
    "        f1 = f1_score([y_true],[y_preds])\n",
    "        scores.append(f1)\n",
    "    except:\n",
    "        count += 1\n",
    "        print(df.iloc[i])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"f1\"] = scores\n",
    "df = df.sort_values(by=['f1']).reset_index()\n",
    "del df[\"level_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b17082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "\n",
    "df.hist(column='f1',bins=40,sharey=True, sharex=True)\n",
    "pl.suptitle('Entery-scale F1-Score distribution over test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6b85b",
   "metadata": {},
   "source": [
    "### 15-Top worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccccc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,16):\n",
    "    print(f\"INDEX {i}\")\n",
    "    print(df.iloc[i][\"entry\"])\n",
    "    print(df.iloc[i][\"entry_xml\"])\n",
    "    print(f\"F1-Score : {df.iloc[i]['f1']}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b631e",
   "metadata": {},
   "source": [
    "### 15-Top best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc515768",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = df.sort_values(by='f1', ascending=False)\n",
    "for i in range(0,16):\n",
    "    print(rdf.iloc[i][\"entry\"])\n",
    "    print(rdf.iloc[i][\"entry_xml\"])\n",
    "    print(f\"F1-Score : {rdf.iloc[i]['f1']}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1581a2",
   "metadata": {},
   "source": [
    "## Sub-word global analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_preds = []\n",
    "flat_labels = []\n",
    "for i in range(len(df)):\n",
    "    flat_preds += df[\"tags_preds\"][i]\n",
    "    flat_labels += df[\"tags_gold\"][i]\n",
    "flat_preds = pd.Series(flat_preds, name=\"Predictions\")\n",
    "flat_labels = pd.Series(flat_labels, name=\"Gold\")\n",
    "\n",
    "global_confusion = pd.crosstab(flat_labels, flat_preds,normalize='index').multiply(100., axis=1)\n",
    "global_confusion.columns = ['ACT','CARDINAL','FT','LOC','PER','TITRE','O']\n",
    "global_confusion.index = ['ACT','CARDINAL','FT','LOC','PER','TITRE','O']\n",
    "global_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605f65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Init figure\n",
    "plt.figure(figsize=(34, 16))\n",
    "\n",
    "#Create heatmap\n",
    "snsfig = sns.heatmap(\n",
    "    global_confusion, \n",
    "    annot = True, #Display labels\n",
    "    cmap=sns.color_palette(\"rocket_r\", as_cmap=True), #Color\n",
    "    fmt=\".1f\",\n",
    "    cbar=False,\n",
    "    annot_kws={\"fontsize\":40}\n",
    ")\n",
    "\n",
    "#Rename label axis and set their style\n",
    "plt.xlabel('Predictions',weight = 'bold',fontsize=35) # x-axis label with fontsize 15\n",
    "plt.ylabel('Gold',weight = 'bold',fontsize=35) # y-axis label with fontsize 15\n",
    "\n",
    "#Set x labels position to top\n",
    "snsfig.xaxis.tick_top()\n",
    "snsfig.xaxis.set_label_position('top')\n",
    "\n",
    "#Rotate y ticks horizontaly\n",
    "plt.yticks(rotation=0) \n",
    "\n",
    "#Change ticks size\n",
    "snsfig.set_xticklabels(snsfig.get_xmajorticklabels(), fontsize = 32)\n",
    "snsfig.set_yticklabels(snsfig.get_ymajorticklabels(), fontsize = 32)\n",
    "\n",
    "print(\"Confusion matrix of reference and predicted tokens types.\")\n",
    "print(\"Values are normalized by row (percentage of each reference classe and its resultants predictions)\")\n",
    "print(\"Last row represent percentage of each class in gold.\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Save figure\n",
    "fig = snsfig.get_figure()\n",
    "fig.tight_layout()\n",
    "#fig.savefig(f\"{OUT_BASE}/tokenscaleanalysis-{MODEL_TYPE}-{SET}-{FORMAT}.pdf\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9342a",
   "metadata": {},
   "source": [
    "## Entry scale analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987574e1",
   "metadata": {},
   "source": [
    "Please choose one entry giving its index to perform an entry scale analysis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad6d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca834a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.iloc[i][\"entry\"])\n",
    "print(df.iloc[i][\"entry_xml\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8275b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "y_preds = df.iloc[i][\"tags_preds\"]\n",
    "y_true = df.iloc[i][\"tags_gold\"]\n",
    "print(classification_report([y_true],[y_preds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c619eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_true = pd.Series(df[\"tags_gold\"].iloc[i], name='Gold')\n",
    "y_pred = pd.Series(df[\"tags_preds\"].iloc[i], name='Predictions')\n",
    "entry_confusion = pd.crosstab(y_true, y_pred)\n",
    "entry_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(34, 16))\n",
    "\n",
    "#Create heatmap\n",
    "snsfig = sns.heatmap(\n",
    "    entry_confusion, \n",
    "    annot = True, #Display labels\n",
    "    cmap=sns.color_palette(\"rocket_r\", as_cmap=True), #Color\n",
    "    fmt=\"g\",\n",
    "    cbar=False,\n",
    "    annot_kws={\"fontsize\":40}\n",
    ")\n",
    "\n",
    "#Rename label axis and set their style\n",
    "plt.xlabel('Predictions',weight = 'bold',fontsize=25) # x-axis label with fontsize 15\n",
    "plt.ylabel('Gold',weight = 'bold',fontsize=25) # y-axis label with fontsize 15\n",
    "\n",
    "#Set x labels position to top\n",
    "snsfig.xaxis.tick_top()\n",
    "snsfig.xaxis.set_label_position('top')\n",
    "\n",
    "#Rotate y ticks horizontaly\n",
    "plt.yticks(rotation=0) \n",
    "\n",
    "#Change ticks size\n",
    "snsfig.set_xticklabels(snsfig.get_xmajorticklabels(), fontsize = 40)\n",
    "snsfig.set_yticklabels(snsfig.get_ymajorticklabels(), fontsize = 40)\n",
    "\n",
    "print(\"Confusion matrix of reference and predicted tokens types.\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Save figure\n",
    "fig = snsfig.get_figure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
