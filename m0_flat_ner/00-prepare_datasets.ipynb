{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8189029c-0b96-45a0-8d89-6c49ffdda9b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 00 - Flat NER datasets generation\n",
    "\n",
    "Execute this notebook twice to create datasets (set **MODEL_NAME** to *camember_ner* and *pretrained_camembert_ner*)\n",
    "\n",
    "**Outputs**\n",
    "- In `01-experiment_1_prepared_ref_dataset_camembert_ner` : train, dev, test for clean data with CamemBERT NER tokenizer\n",
    "- In `01-experiment_1_prepared_ref_dataset_pretrained_camembert_ner` : train, dev, test for clean data with Pretrained CamemBERT NER tokenizer\n",
    "- In `02-experiment_2_prepared_pero_ocr_dataset_camembert_ner` : train, dev & test datasets for noisy OCR data (Pero-OCR) with CamemBERT NER tokenizer\n",
    "- In `02-experiment_2_prepared_pero_ocr_dataset_pretrained_camembert_ner` : train, dev & test datasets for noisy OCR data (Pero-OCR) with Pretrained CamemBERT NER tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77ed69-9fa6-4462-9b1b-00ecb3f17392",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff04646-29cc-4cb7-a931-45b18016ed64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ENV_IS_GOOGLE_COLAB = True if 'google.colab' in str(get_ipython()) else False\n",
    "os.environ[\"ENV_IS_GOOGLE_COLAB\"] = str(ENV_IS_GOOGLE_COLAB)\n",
    "\n",
    "if ENV_IS_GOOGLE_COLAB:\n",
    "  from google.colab import drive\n",
    "  mountpoint = Path(\"/content/drive\")\n",
    "  drive.mount(str(mountpoint)) # Mount gdrive to BASE\n",
    "  base = mountpoint / \"MyDrive/article_icdar_2023\" # Adapt this to your situation\n",
    "  sys.path.append(str(base)) # Add BASE to Python Path\n",
    "  BASE = Path(base).resolve() # Make BASE absolute\n",
    "  DATASETS =  BASE / \"dataset_ICDAR\"\n",
    "  OUT_BASE = BASE / \"res_ICDAR/method_0\"\n",
    "else:\n",
    "  BASE = Path().resolve() # Directory of this approach\n",
    "  #Adapt this to your situation\n",
    "  DATASETS = Path('../dataset_ICDAR').resolve() #Where your data are located befor Dataset object creation\n",
    "  OUT_BASE = Path('../res_ICDAR/method_0').resolve() #Where you save the results of this notebook\n",
    "\n",
    "print(sys.path)\n",
    "print(BASE)\n",
    "print(DATASETS)\n",
    "print(OUT_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ab2a8-d126-4a36-87c1-5bd934a2f5c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16874c6b-e422-4e07-b31c-856841a71300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GLOBAL CONSTANTS\n",
    "import config\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "config.SPLIT_SEED = 42 # Random seed used in train/dev/test. Do not change it if you want to recreate the paper results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4998741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "model = widgets.RadioButtons(\n",
    "            options=['camembert_ner','pretrained_camembert_ner'],\n",
    "            layout={'width': 'max-content'}\n",
    "        )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = model.value\n",
    "print(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71010115-8bac-43d9-9638-15f56f029493",
   "metadata": {},
   "source": [
    "# 01. Experiment #1 : Ground-truth datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de597797-0541-404e-b721-8667077eb7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "GOLD_REF = DATASETS / \"41-ner_ref_from_pero/gold.csv\"\n",
    "assert GOLD_REF.exists()\n",
    "\n",
    "with open(GOLD_REF,'r',encoding='utf8') as f:\n",
    "    lines = f.readlines()\n",
    "    res = []\n",
    "    for line in lines:\n",
    "        l = line.split('\", \"')\n",
    "        res.append([l[0][1:],l[1][:-2]])\n",
    "gold_reference = pd.DataFrame(res,columns=[\"ner_xml\",\"book\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031fb44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TITRE-H and TITRE-P labels to transformers NER Pipeline\n",
    "for i in range(len(gold_reference)):\n",
    "    if '<TITRE-H>' in gold_reference['ner_xml'][i]:\n",
    "        gold_reference['ner_xml'][i] = gold_reference['ner_xml'][i].replace('TITRE-H','TITREH')\n",
    "    if '<TITRE-P>' in gold_reference['ner_xml'][i]:\n",
    "        gold_reference['ner_xml'][i] = gold_reference['ner_xml'][i].replace('TITRE-P','TITREP')\n",
    "gold_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5bdd42-8847-48e5-833f-8cbb8106e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_util import train_dev_test_split, unwrap # Local imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CONSTANTS\n",
    "MIN_TRAINSET_SIZE = 30\n",
    "\n",
    " # Split 72/8/20% w. stratified sampling on directories names\n",
    "train, dev, test = train_dev_test_split(gold_reference.to_numpy())\n",
    "\n",
    "# Iteratively split the trainset in half to create smaller trainsets\n",
    "exp1_trainsets = [train]\n",
    "t_len = len(train)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        current = exp1_trainsets[-1]\n",
    "        _, groups = unwrap(current)\n",
    "        smaller, rest = train_test_split(\n",
    "            current,\n",
    "            train_size=0.5,\n",
    "            shuffle=True,\n",
    "            random_state=config.SPLIT_SEED,\n",
    "            stratify=groups,\n",
    "        )\n",
    "        t_len = len(rest)\n",
    "        if t_len < MIN_TRAINSET_SIZE:\n",
    "            break\n",
    "        exp1_trainsets.append(smaller)\n",
    "\n",
    "    except ValueError:\n",
    "        # Stop now if we encounter the error \"The least populated class in y has only 1 member\".\n",
    "        break\n",
    "\n",
    "[len(s) for s in exp1_trainsets] # Should be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f4fd33-72da-4eb8-bec2-c90aa3798c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Dev set should contain 676 examples\n",
    "assert len(dev) == 676\n",
    "\n",
    "# Test set should contain 1685 examples\n",
    "assert len(test) == 1685\n",
    "\n",
    "# Lenghts of exp1_trainsets should be fixed\n",
    "assert sorted([len(s) for s in exp1_trainsets] ) == sorted([6084, 3042, 1521, 760, 380, 190, 95, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85046df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import createStatsTab\n",
    "\n",
    "createStatsTab(train,dev,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd88d23-c062-4772-b53f-610927523397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on disk\n",
    "from dataset_util import save_dataset # Local import\n",
    "\n",
    "output_directory = OUT_BASE / f\"01-experiment_1_prepared_ref_dataset_{MODEL_NAME}\"\n",
    "output_directory.mkdir(exist_ok=True, parents=True) # Create if necessary\n",
    "   \n",
    "for train in exp1_trainsets:\n",
    "    datasets = [train, dev, test]\n",
    "    save_dataset(output_directory, datasets, [\"train\",\"dev\",\"test\"], suffix=len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e623237-eb40-4bef-80d2-7a21a334023f",
   "metadata": {},
   "source": [
    "# 02. Experiment #2 : Noisy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "GOLD_REF = DATASETS / \"31-ner_align_pero/gold.csv\"\n",
    "gold_reference = pd.read_csv(GOLD_REF, header=None, names=[\"ner_xml\",\"book\"],skipinitialspace='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5cd59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TITRE-H and TITRE-P labels to transformers NER Pipeline\n",
    "for i in range(len(gold_reference)):\n",
    "    if '<TITRE-H>' in gold_reference['ner_xml'][i]:\n",
    "        gold_reference['ner_xml'][i] = gold_reference['ner_xml'][i].replace('TITRE-H','TITREH')\n",
    "    if '<TITRE-P>' in gold_reference['ner_xml'][i]:\n",
    "        gold_reference['ner_xml'][i] = gold_reference['ner_xml'][i].replace('TITRE-P','TITREP')\n",
    "gold_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_util import train_dev_test_split, unwrap # Local imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CONSTANTS\n",
    "MIN_TRAINSET_SIZE = 30\n",
    "\n",
    " # Split 72/8/20% w. stratified sampling on directories names\n",
    "train, dev, test = train_dev_test_split(gold_reference.to_numpy())\n",
    "\n",
    "# Iteratively split the trainset in half to create smaller trainsets\n",
    "exp1_trainsets = [train]\n",
    "t_len = len(train)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        current = exp1_trainsets[-1]\n",
    "        _, groups = unwrap(current)\n",
    "        smaller, rest = train_test_split(\n",
    "            current,\n",
    "            train_size=0.5,\n",
    "            shuffle=True,\n",
    "            random_state=config.SPLIT_SEED,\n",
    "            stratify=groups,\n",
    "        )\n",
    "        t_len = len(rest)\n",
    "        if t_len < MIN_TRAINSET_SIZE:\n",
    "            break\n",
    "        exp1_trainsets.append(smaller)\n",
    "\n",
    "    except ValueError:\n",
    "        # Stop now if we encounter the error \"The least populated class in y has only 1 member\".\n",
    "        break\n",
    "\n",
    "[len(s) for s in exp1_trainsets] # Should be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eedb65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Dev set should contain 676 examples\n",
    "assert len(dev) == 676\n",
    "\n",
    "# Test set should contain 1685 examples\n",
    "assert len(test) == 1685\n",
    "\n",
    "# Lenghts of exp1_trainsets should be fixed\n",
    "assert sorted([len(s) for s in exp1_trainsets] ) == sorted([6084, 3042, 1521, 760, 380, 190, 95, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import createStatsTab\n",
    "\n",
    "createStatsTab(train,dev,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on disk\n",
    "from dataset_util import save_dataset # Local import\n",
    "\n",
    "output_directory = OUT_BASE / f\"02-experiment_2_prepared_pero_ocr_dataset_{MODEL_NAME}\"\n",
    "output_directory.mkdir(exist_ok=True, parents=True) # Create if necessary\n",
    "   \n",
    "for train in exp1_trainsets:\n",
    "    datasets = [train, dev, test]\n",
    "    save_dataset(output_directory, datasets, [\"train\",\"dev\",\"test\"], suffix=len(train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
