{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294705ff-9e89-499f-a41f-9494362be5f9",
   "metadata": {
    "id": "2552858d-7386-4e9a-8b0e-c338b920f783"
   },
   "source": [
    "# 120 - M1 - Independent-Layered NER Model - Experiment #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9101d3bd",
   "metadata": {},
   "source": [
    "Experiment 2 deals with Pero OCR annotated entries with IO/IOB2 labels and CmBERT/Pretrained CmBERT models.\n",
    "* `121-camembert-multihead-io` : Fine-tuned CamemBERT-NER with IO labels\n",
    "* `122-camembert-multihead-iob2`: Fine-tuned CamemBERT-NER with IOB2 labels\n",
    "* `123-pretrained-camembert-multihead-io`: Fine-tuned Pretrained CamemBERT-NER with IO labels\n",
    "* `124-pretrained-camembert-multihead-iob2`: Fine-tuned Pretrained CamemBERT-NER with IOB2 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flD_9oT8LmDB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flD_9oT8LmDB",
    "outputId": "63a92e5f-d414-46cc-db86-b46981e42594"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers datasets spacy transformers[sentencepiece] seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cZvwNIzqBwDs",
   "metadata": {
    "id": "cZvwNIzqBwDs",
    "tags": []
   },
   "source": [
    "## Initialisation\n",
    "Set the BASE path.\n",
    "If run on Google Colab, will also mout Google Drive to the moutpoint given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LWJVak2mB6bI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWJVak2mB6bI",
    "outputId": "dbb54104-560b-480c-d4b0-74a0787e2024"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ENV_IS_GOOGLE_COLAB = True if 'google.colab' in str(get_ipython()) else False\n",
    "os.environ[\"ENV_IS_GOOGLE_COLAB\"] = str(ENV_IS_GOOGLE_COLAB)\n",
    "\n",
    "if ENV_IS_GOOGLE_COLAB:\n",
    "  from google.colab import drive\n",
    "  mountpoint = Path(\"/content/drive\")\n",
    "  drive.mount(str(mountpoint)) # Mount gdrive to BASE\n",
    "  base = mountpoint / \"MyDrive/article_icdar_2023\" # Adapt this to your situation\n",
    "  sys.path.append(str(base)) # Add BASE to Python Path\n",
    "  BASE = Path(base).resolve() # Make BASE absolute\n",
    "  DATASETS =  BASE / \"dataset_ICDAR\"\n",
    "  OUT_BASE = BASE / \"res_ICDAR/method_1\"\n",
    "else:\n",
    "  BASE = Path().resolve() # Directory of this approach\n",
    "  #Adapt this to your situation\n",
    "  DATASETS = Path('../dataset_ICDAR').resolve() #Where your data are located befor Dataset object creation\n",
    "  OUT_BASE = Path('../res_ICDAR/method_1').resolve() #Where you save the results of this notebook\n",
    "\n",
    "print(sys.path)\n",
    "print(BASE)\n",
    "print(DATASETS)\n",
    "print(OUT_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hxHdPTBlCCFO",
   "metadata": {
    "id": "hxHdPTBlCCFO"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a270f9-5f9e-449e-bbff-69136b383507",
   "metadata": {
    "id": "2552858d-7386-4e9a-8b0e-c338b920f783"
   },
   "outputs": [],
   "source": [
    "RUN_CAMEMBERT_IO = False\n",
    "RUN_CAMEMBERT_IOB2 = False\n",
    "#Can't run together because of convert_tokenizer_\n",
    "RUN_PTRN_CAMEMBERT_IO = False\n",
    "RUN_PTRN_CAMEMBERT_IOB2 = True\n",
    "\n",
    "# Number of times a model will be trained & evaluated on each a dataset\n",
    "N_RUNS = 5\n",
    "\n",
    "#Number of entities depth levels\n",
    "NUMBER_OF_LEVELS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd67ba1",
   "metadata": {},
   "source": [
    "**Don't forget to check tokenizer name in *model_util_io.py* and *model_util_iob2.py* files (same name as model) !**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b18a5bc-1abb-450d-90d3-6a7e56f773ed",
   "metadata": {
    "id": "6b18a5bc-1abb-450d-90d3-6a7e56f773ed"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91140aa5-b377-47c1-bd44-844cd9365ec3",
   "metadata": {
    "id": "91140aa5-b377-47c1-bd44-844cd9365ec3"
   },
   "outputs": [],
   "source": [
    "TRAINING_CONFIG = {\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 100,\n",
    "    \"max_steps\": 5000,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"greater_is_better\":True,\n",
    "    \"metric_for_best_model\": \"f1\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "\n",
    "#Print examples from datasets\n",
    "def loadExample(INPUT_DIR,set_length:int,i:int,subset:str):\n",
    "    set_ = load_from_disk(INPUT_DIR / f\"huggingface_{set_length}\")\n",
    "    data = {\"tokens\": set_[subset][i][\"tokens\"],\n",
    "            \"niv_1_tags\": set_[subset][i][f\"ner_tags_niv1\"],\n",
    "            \"niv_2_tags\": set_[subset][i][f\"ner_tags_niv2\"]}\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bacc1d2",
   "metadata": {},
   "source": [
    "## 121 - Train & eval : IO Pero OCR dataset with CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc776ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"camembert_ner\"\n",
    "MODEL = \"Jean-Baptiste/camembert-ner\"\n",
    "LABEL = \"io\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5421b5",
   "metadata": {},
   "source": [
    "### 121.1 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06332f9f-37d5-4a0c-9af6-1f2bda236789",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06332f9f-37d5-4a0c-9af6-1f2bda236789",
    "outputId": "5749eaf4-a3d1-40fd-b2d4-fd45a27eb16e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "\n",
    "# Expected datasets indexed by number of examples in the trainset\n",
    "#TRAINSETS_SIZES = [47,95,190,380,760,1521,3042,6084] #To train on the 7 datasets\n",
    "TRAINSETS_SIZES = [6084] #To train only on the biggest dataset\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "INPUT_DIR = OUT_BASE / f\"m1-experiment_2_prepared_dataset_pero_ocr_{LABEL}_{MODEL_NAME}\"\n",
    "METRICS_OUTPUT_DIR = OUT_BASE / \"m1-120-experiment_2_metrics\"\n",
    "INPUT_DIR, METRICS_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadExample(INPUT_DIR,TRAINSETS_SIZES[-1],22,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae05b9",
   "metadata": {},
   "source": [
    "### 121.2 Fine-tuning with IO labels - train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e821087-3623-4c14-a8fb-63dcc98dc1d4",
   "metadata": {
    "id": "2e821087-3623-4c14-a8fb-63dcc98dc1d4"
   },
   "outputs": [],
   "source": [
    "from config import logger\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "from multihead_utils.util_io import train_eval_loop, init_model\n",
    "import json\n",
    "    \n",
    "def train_bert(metrics_output_directory,model_output_directory,local_config,max_levels_number):\n",
    "    # Train & evaluate loop\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        output_dir = metrics_output_directory / f\"run_{run}\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for i in range(1,max_levels_number+1):\n",
    "        \n",
    "            for trainset_size in TRAINSETS_SIZES:\n",
    "                datasetdir = INPUT_DIR / f\"huggingface_{trainset_size}\"\n",
    "                logger.info(f\"Running on datasets in {datasetdir}\")\n",
    "                logger.info(f\"Metrics will be saved in {output_dir}\")\n",
    "\n",
    "                # Load data\n",
    "                train_dev_test = load_from_disk(datasetdir)\n",
    "                train_dev_test_ = train_dev_test.rename_column(\"labels_niv\"+str(i), \"labels\")\n",
    "                train_dev_test_ = train_dev_test_.rename_column(\"ner_tags_niv\"+str(i), \"ner_tags\")\n",
    "                \n",
    "                \n",
    "                conf =  local_config.copy()\n",
    "                conf[\"output_dir\"] = model_output_directory / f\"level-{str(i)}\"\n",
    "                \n",
    "                # Get the model components\n",
    "                model, tokenizer, training_args = init_model(MODEL, conf, run)\n",
    "                logger.info(f\"{model} #{run}, will save in {output_dir}\")\n",
    "                \n",
    "                #Update metrics outputdir\n",
    "                output_dir_metrics = output_dir / f\"level-{str(i)}\"\n",
    "                output_dir_metrics.mkdir(exist_ok=True)\n",
    "                \n",
    "                metrics = train_eval_loop(model,         # Implicit. Must be setbefore calling train_bert()\n",
    "                                          training_args, # Idem\n",
    "                                          tokenizer,\n",
    "                                          **train_dev_test_)\n",
    "\n",
    "                # Save the metrics\n",
    "                metrics_file = output_dir_metrics / f\"test_{trainset_size}.json\"\n",
    "                with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                    json.dump(metrics[0], o)\n",
    "\n",
    "                metrics_file = output_dir_metrics / f\"dev_{trainset_size}.json\"\n",
    "                with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                    json.dump(metrics[1], o)\n",
    "\n",
    "                train_dev_test_ = train_dev_test_.rename_column(\"labels\",\"labels_niv\"+str(i))\n",
    "                train_dev_test_ = train_dev_test_.rename_column(\"ner_tags\",\"ner_tags_niv\"+str(i))\n",
    "                \n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05387e39-dd69-491e-9517-57490356e5e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05387e39-dd69-491e-9517-57490356e5e9",
    "outputId": "a894e899-0646-4260-f12f-7468adfbb5b2"
   },
   "outputs": [],
   "source": [
    "if RUN_CAMEMBERT_IO:\n",
    "    from multihead_utils.util_io import init_model\n",
    "    import time\n",
    "    import datetime\n",
    "\n",
    "    h = time.time()\n",
    "    \n",
    "    # CONSTS\n",
    "    MODEL_METRICS_DIR = METRICS_OUTPUT_DIR / f\"121-camembert-ner-multihead-io\"\n",
    "    MODEL_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    MODEL_OUTPUT_MODEL_PATH = OUT_BASE / f\"tmp/121-camembert-ner-multihead-io\"\n",
    "    MODEL_METRICS_DIR, MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Run the main loop\n",
    "    train_bert(MODEL_METRICS_DIR,\n",
    "                   MODEL_OUTPUT_MODEL_PATH,\n",
    "                   TRAINING_CONFIG,\n",
    "                   NUMBER_OF_LEVELS\n",
    "                  )\n",
    "        \n",
    "    print(f\"{MODEL} fine-tuning with IO labels on level {str(i)} is over.\")\n",
    "    runtime = (time.time()- h)/N_RUNS\n",
    "    print(f\"Run-time is equal to {str(datetime.timedelta(seconds=runtime))}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(f\"Skipped finetuning {MODEL} for IO labels\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2d0dec6",
   "metadata": {},
   "source": [
    "Best L1 model : \n",
    "Best L2 model : \n",
    "Run time : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aecdd24",
   "metadata": {
    "id": "09de0446-e62f-46d5-ae73-34112f3c420d"
   },
   "source": [
    "## 122 - Train & eval : IOB2 Pero OCR dataset with CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82010c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"camembert_ner\"\n",
    "MODEL = \"Jean-Baptiste/camembert-ner\"\n",
    "LABEL = \"iob2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f31bb2",
   "metadata": {},
   "source": [
    "### 122.1 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b53152",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06332f9f-37d5-4a0c-9af6-1f2bda236789",
    "outputId": "5749eaf4-a3d1-40fd-b2d4-fd45a27eb16e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "\n",
    "# Expected datasets indexed by number of examples in the trainset\n",
    "#TRAINSETS_SIZES = [47,95,190,380,760,1521,3042,6084] #To train on the 7 datasets\n",
    "TRAINSETS_SIZES = [6084] #To train only on the biggest dataset\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "INPUT_DIR = OUT_BASE / f\"m1-experiment_2_prepared_dataset_pero_ocr_{LABEL}_{MODEL_NAME}\"\n",
    "METRICS_OUTPUT_DIR = OUT_BASE / \"m1-120-experiment_2_metrics\"\n",
    "INPUT_DIR, METRICS_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadExample(INPUT_DIR,TRAINSETS_SIZES[-1],22,\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d241d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import logger\n",
    "from datasets import load_from_disk\n",
    "from multihead_utils.util_iob2 import train_eval_loop, init_model\n",
    "import json\n",
    "    \n",
    "def train_bert(metrics_output_directory,model_output_directory,local_config,max_levels_number):\n",
    "    # Train & evaluate loop\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        output_dir = metrics_output_directory / f\"run_{run}\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for i in range(1,max_levels_number+1):\n",
    "        \n",
    "            for trainset_size in TRAINSETS_SIZES:\n",
    "                datasetdir = INPUT_DIR / f\"huggingface_{trainset_size}\"\n",
    "                logger.info(f\"Running on datasets in {datasetdir}\")\n",
    "                logger.info(f\"Metrics will be saved in {output_dir}\")\n",
    "\n",
    "                # Load data\n",
    "                train_dev_test = load_from_disk(datasetdir)\n",
    "                train_dev_test_ = train_dev_test.rename_column(\"labels_niv\"+str(i), \"labels\")\n",
    "                train_dev_test_ = train_dev_test_.rename_column(\"ner_tags_niv\"+str(i), \"ner_tags\")\n",
    "                \n",
    "                \n",
    "                conf =  local_config.copy()\n",
    "                conf[\"output_dir\"] = model_output_directory / f\"level-{str(i)}\"\n",
    "                \n",
    "                # Get the model components\n",
    "                model, tokenizer, training_args = init_model(MODEL, conf, run)\n",
    "                logger.info(f\"{model} #{run}, will save in {output_dir}\")\n",
    "                \n",
    "                #Update metrics outputdir\n",
    "                output_dir_metrics = output_dir / f\"level-{str(i)}\"\n",
    "                output_dir_metrics.mkdir(exist_ok=True)\n",
    "                \n",
    "                metrics = train_eval_loop(model,         # Implicit. Must be setbefore calling train_bert()\n",
    "                                          training_args, # Idem\n",
    "                                          tokenizer,\n",
    "                                          **train_dev_test_)\n",
    "\n",
    "                # Save the metrics\n",
    "                metrics_file = output_dir_metrics / f\"test_{trainset_size}.json\"\n",
    "                with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                    json.dump(metrics[0], o)\n",
    "\n",
    "                metrics_file = output_dir_metrics / f\"dev_{trainset_size}.json\"\n",
    "                with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                    json.dump(metrics[1], o)\n",
    "\n",
    "                train_dev_test_ = train_dev_test_.rename_column(\"labels\",\"labels_niv\"+str(i))\n",
    "                train_dev_test_ = train_dev_test_.rename_column(\"ner_tags\",\"ner_tags_niv\"+str(i))\n",
    "                \n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbbc2b5",
   "metadata": {},
   "source": [
    "### 122.2 Fine-tuning with IOB2 labels - train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c303ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05387e39-dd69-491e-9517-57490356e5e9",
    "outputId": "a894e899-0646-4260-f12f-7468adfbb5b2"
   },
   "outputs": [],
   "source": [
    "if RUN_CAMEMBERT_IOB2:\n",
    "    import time\n",
    "    import datetime\n",
    "\n",
    "    h = time.time()\n",
    "    \n",
    "    # CONSTS\n",
    "    MODEL_METRICS_DIR = METRICS_OUTPUT_DIR / f\"122-camembert-ner-multihead-iob2\"\n",
    "    MODEL_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    MODEL_OUTPUT_MODEL_PATH = OUT_BASE / f\"tmp/122-camembert-ner-multihead-iob2\"\n",
    "    MODEL_METRICS_DIR, MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Run the main loop\n",
    "    train_bert(MODEL_METRICS_DIR,\n",
    "                   MODEL_OUTPUT_MODEL_PATH,\n",
    "                   TRAINING_CONFIG,\n",
    "                   NUMBER_OF_LEVELS\n",
    "                  )\n",
    "        \n",
    "    print(f\"{MODEL} fine-tuning with IOB2 labels is over.\")\n",
    "    runtime = (time.time()- h)/N_RUNS\n",
    "    print(f\"Run-time is equal to {str(datetime.timedelta(seconds=runtime))}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(f\"Skipped finetuning {MODEL} for IOB2 labels\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f108799",
   "metadata": {},
   "source": [
    "Best level-1 layer : \n",
    "Best layer 2 layer : \n",
    "Mean run time : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae16e90",
   "metadata": {},
   "source": [
    "## 123 - Train & eval : IO Pero OCR dataset with Pretrain CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98634e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"pretrained_camembert_ner\"\n",
    "MODEL = \"HueyNemud/das22-10-camembert_pretrained\"\n",
    "LABEL = \"io\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dbf6f1",
   "metadata": {},
   "source": [
    "### 123.1 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae66f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "\n",
    "# Expected datasets indexed by number of examples in the trainset\n",
    "#TRAINSETS_SIZES = [47,95,190,380,760,1521,3042,6084] #To train on the 7 datasets\n",
    "TRAINSETS_SIZES = [6084] #To train only on the biggest dataset\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "INPUT_DIR = OUT_BASE / f\"m1-experiment_2_prepared_dataset_pero_ocr_{LABEL}_{MODEL_NAME}\"\n",
    "METRICS_OUTPUT_DIR = OUT_BASE / \"m1-120-experiment_2_metrics\"\n",
    "INPUT_DIR, METRICS_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad39e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadExample(INPUT_DIR,TRAINSETS_SIZES[-1],22,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60000c0b",
   "metadata": {},
   "source": [
    "### 123.2 Fine-tuning with IO labels - train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f56b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import logger\n",
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "from multihead_utils.util_io import train_eval_loop, init_model\n",
    "import json\n",
    "    \n",
    "def train_bert(metrics_output_directory,model_output_directory,local_config,max_levels_number):\n",
    "    # Train & evaluate loop\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        output_dir = metrics_output_directory / f\"run_{run}\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for i in range(1,max_levels_number+1):\n",
    "        \n",
    "            for trainset_size in TRAINSETS_SIZES:\n",
    "                datasetdir = INPUT_DIR / f\"huggingface_{trainset_size}\"\n",
    "                logger.info(f\"Running on datasets in {datasetdir}\")\n",
    "                logger.info(f\"Metrics will be saved in {output_dir}\")\n",
    "\n",
    "                # Load data\n",
    "                train_dev_test = load_from_disk(datasetdir)\n",
    "                train_dev_test_ = train_dev_test.rename_column(\"labels_niv\"+str(i), \"labels\")\n",
    "                train_dev_test_ = train_dev_test_.rename_column(\"ner_tags_niv\"+str(i), \"ner_tags\")\n",
    "                \n",
    "                \n",
    "                conf =  local_config.copy()\n",
    "                conf[\"output_dir\"] = model_output_directory / f\"level-{str(i)}\"\n",
    "                \n",
    "                # Get the model components\n",
    "                model, tokenizer, training_args = init_model(MODEL, conf, run)\n",
    "                logger.info(f\"{model} #{run}, will save in {output_dir}\")\n",
    "                \n",
    "                #Update metrics outputdir\n",
    "                output_dir_metrics = output_dir / f\"level-{str(i)}\"\n",
    "                output_dir_metrics.mkdir(exist_ok=True)\n",
    "                \n",
    "                metrics = train_eval_loop(model,         # Implicit. Must be setbefore calling train_bert()\n",
    "                                          training_args, # Idem\n",
    "                                          tokenizer,\n",
    "                                          **train_dev_test_)\n",
    "\n",
    "                # Save the metrics\n",
    "                metrics_file = output_dir_metrics / f\"test_{trainset_size}.json\"\n",
    "                with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                    json.dump(metrics[0], o)\n",
    "\n",
    "                metrics_file = output_dir_metrics / f\"dev_{trainset_size}.json\"\n",
    "                with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                    json.dump(metrics[1], o)\n",
    "\n",
    "                train_dev_test_ = train_dev_test_.rename_column(\"labels\",\"labels_niv\"+str(i))\n",
    "                train_dev_test_ = train_dev_test_.rename_column(\"ner_tags\",\"ner_tags_niv\"+str(i))\n",
    "                \n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b2b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_PTRN_CAMEMBERT_IO:\n",
    "    import time\n",
    "    import datetime\n",
    "\n",
    "    h = time.time()\n",
    "    \n",
    "    # CONSTS\n",
    "    MODEL_METRICS_DIR = METRICS_OUTPUT_DIR / f\"123-pretrained-camembert-ner-multihead-io\"\n",
    "    MODEL_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    MODEL_OUTPUT_MODEL_PATH = OUT_BASE / f\"tmp/123-pretrained-camembert-ner-multihead-io\"\n",
    "    MODEL_METRICS_DIR, MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Run the main loop\n",
    "    train_bert(MODEL_METRICS_DIR,\n",
    "                   MODEL_OUTPUT_MODEL_PATH,\n",
    "                   TRAINING_CONFIG,\n",
    "                   NUMBER_OF_LEVELS\n",
    "                  )\n",
    "        \n",
    "    print(f\"Model fine-tuning with IO labels is over.\")\n",
    "    runtime = (time.time()- h)/N_RUNS\n",
    "    print(f\"Run-time is equal to {str(datetime.timedelta(seconds=runtime))}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(f\"Skipped finetuning {MODEL} for IO labels\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af240578",
   "metadata": {},
   "source": [
    "Best level-1 layer : \n",
    "Best layer 2 layer : \n",
    "Mean run time : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d6e0df",
   "metadata": {},
   "source": [
    "## 124 - Train & eval : IOB2 Pero OCR dataset with Pretrain CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"pretrained_camembert_ner\"\n",
    "MODEL = \"HueyNemud/das22-10-camembert_pretrained\"\n",
    "LABEL = \"iob2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69263587",
   "metadata": {},
   "source": [
    "### 124.1 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13105ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "\n",
    "# Expected datasets indexed by number of examples in the trainset\n",
    "#TRAINSETS_SIZES = [47,95,190,380,760,1521,3042,6084] #To train on the 7 datasets\n",
    "TRAINSETS_SIZES = [6084] #To train only on the biggest dataset\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "INPUT_DIR = OUT_BASE / f\"m1-experiment_2_prepared_dataset_pero_ocr_{LABEL}_{MODEL_NAME}\"\n",
    "METRICS_OUTPUT_DIR = OUT_BASE / \"m1-120-experiment_2_metrics\"\n",
    "INPUT_DIR, METRICS_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564040f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadExample(INPUT_DIR,TRAINSETS_SIZES[-1],22,\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import logger\n",
    "from datasets import load_from_disk\n",
    "from multihead_utils.util_iob2 import train_eval_loop, init_model\n",
    "import json\n",
    "\n",
    "def train_bert(metrics_output_directory,model_output_directory,local_config,max_levels_number):\n",
    "    # Train & evaluate loop\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        output_dir = metrics_output_directory / f\"run_{run}\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for i in range(1,max_levels_number+1):\n",
    "        \n",
    "            for trainset_size in TRAINSETS_SIZES:\n",
    "                datasetdir = INPUT_DIR / f\"huggingface_{trainset_size}\"\n",
    "                logger.info(f\"Running on datasets in {datasetdir}\")\n",
    "                logger.info(f\"Metrics will be saved in {output_dir}\")\n",
    "\n",
    "                # Load data\n",
    "                train_dev_test = load_from_disk(datasetdir)\n",
    "                train_dev_test_ = train_dev_test.rename_column(\"labels_niv\"+str(i), \"labels\")\n",
    "                train_dev_test_ = train_dev_test_.rename_column(\"ner_tags_niv\"+str(i), \"ner_tags\")\n",
    "                \n",
    "                \n",
    "                conf =  local_config.copy()\n",
    "                conf[\"output_dir\"] = model_output_directory / f\"level-{str(i)}\"\n",
    "                \n",
    "                # Get the model components\n",
    "                model, tokenizer, training_args = init_model(MODEL, conf, run)\n",
    "                logger.info(f\"{model} #{run}, will save in {output_dir}\")\n",
    "                \n",
    "                #Update metrics outputdir\n",
    "                output_dir_metrics = output_dir / f\"level-{str(i)}\"\n",
    "                output_dir_metrics.mkdir(exist_ok=True)\n",
    "                \n",
    "                metrics = train_eval_loop(model,         # Implicit. Must be setbefore calling train_bert()\n",
    "                                          training_args, # Idem\n",
    "                                          tokenizer,\n",
    "                                          **train_dev_test_)\n",
    "\n",
    "                # Save the metrics\n",
    "                metrics_file = output_dir_metrics / f\"test_{trainset_size}.json\"\n",
    "                with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                    json.dump(metrics[0], o)\n",
    "\n",
    "                metrics_file = output_dir_metrics / f\"dev_{trainset_size}.json\"\n",
    "                with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                    json.dump(metrics[1], o)\n",
    "\n",
    "                train_dev_test_ = train_dev_test_.rename_column(\"labels\",\"labels_niv\"+str(i))\n",
    "                train_dev_test_ = train_dev_test_.rename_column(\"ner_tags\",\"ner_tags_niv\"+str(i))\n",
    "                \n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc2c4d",
   "metadata": {},
   "source": [
    "### 124.2 Fine-tuning with IOB2 labels - train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22622842",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_PTRN_CAMEMBERT_IOB2:\n",
    "    import time\n",
    "    import datetime\n",
    "\n",
    "    h = time.time()\n",
    "    \n",
    "    # CONSTS\n",
    "    MODEL_METRICS_DIR = METRICS_OUTPUT_DIR / f\"124-pretrained-camembert-multihead-iob2\"\n",
    "    MODEL_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    MODEL_OUTPUT_MODEL_PATH = OUT_BASE / f\"tmp/124-pretrained-camembert-multihead-iob2\"\n",
    "    MODEL_METRICS_DIR, MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Run the main loop\n",
    "    train_bert(MODEL_METRICS_DIR,\n",
    "                   MODEL_OUTPUT_MODEL_PATH,\n",
    "                   TRAINING_CONFIG,\n",
    "                   NUMBER_OF_LEVELS\n",
    "                  )\n",
    "        \n",
    "    print(f\"{MODEL} fine-tuning with IOB2 labels is over.\")\n",
    "    runtime = (time.time()- h)/N_RUNS\n",
    "    print(f\"Run-time is equal to {str(datetime.timedelta(seconds=runtime))}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(f\"Skipped finetuning {MODEL} for IOB2 labels\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16bbb757",
   "metadata": {},
   "source": [
    "Best level-1 layer : \n",
    "Best layer 2 layer : \n",
    "Mean run time : "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "20-experiment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
