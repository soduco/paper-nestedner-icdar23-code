{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8189029c-0b96-45a0-8d89-6c49ffdda9b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 100 - Independant NER layers - Datasets generation\n",
    "\n",
    "Outputs:\n",
    "Train, dev & test datasets for multilayers NER experiment 1 (with ref dataset) and experiment 2 (with pero OCR dataset)\n",
    "\n",
    "<b>Experiment 1 : Groundtruth dataset</b>\n",
    "* `m1-experiment_1_prepared_dataset_ref_io_camembert_ner`\n",
    "* `m1-experiment_1_prepared_dataset_ref_io_pretrained_camembert_ner`\n",
    "* `m1-experiment_1_prepared_dataset_ref_iob2_camembert_ner`\n",
    "* `m1-experiment_1_prepared_dataset_ref_iob2_pretrained_camembert_ner`\n",
    "\n",
    "<b>Experiment 2 : Pero OCR dataset</b>\n",
    "* `m1-experiment_2_prepared_dataset_pero_ocr_io_camembert_ner`\n",
    "* `m1-experiment_2_prepared_dataset_pero_ocr_io_pretrained_camembert_ner`\n",
    "* `m1-experiment_2_prepared_dataset_pero_ocr_iob2_camembert_ner`\n",
    "* `m1-experiment_2_prepared_dataset_pero_ocr_iob2_pretrained_camembert_ner`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77ed69-9fa6-4462-9b1b-00ecb3f17392",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff04646-29cc-4cb7-a931-45b18016ed64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ENV_IS_GOOGLE_COLAB = True if 'google.colab' in str(get_ipython()) else False\n",
    "os.environ[\"ENV_IS_GOOGLE_COLAB\"] = str(ENV_IS_GOOGLE_COLAB)\n",
    "\n",
    "if ENV_IS_GOOGLE_COLAB:\n",
    "  from google.colab import drive\n",
    "  mountpoint = Path(\"/content/drive\")\n",
    "  drive.mount(str(mountpoint)) # Mount gdrive to BASE\n",
    "  base = mountpoint / \"MyDrive/article_icdar_2023\" # Adapt this to your situation\n",
    "  sys.path.append(str(base)) # Add BASE to Python Path\n",
    "  BASE = Path(base).resolve() # Make BASE absolute\n",
    "  DATASETS =  BASE / \"dataset_ICDAR\"\n",
    "  OUT_BASE = BASE / \"res_ICDAR/method_1\"\n",
    "else:\n",
    "  BASE = Path().resolve() # Directory of this approach\n",
    "  #Adapt this to your situation\n",
    "  DATASETS = Path('../dataset_ICDAR').resolve() #Where your data are located befor Dataset object creation\n",
    "  OUT_BASE = Path('../res_ICDAR/method_1').resolve() #Where you save the results of this notebook\n",
    "\n",
    "print(sys.path)\n",
    "print(BASE)\n",
    "print(DATASETS)\n",
    "print(OUT_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ab2a8-d126-4a36-87c1-5bd934a2f5c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb983af",
   "metadata": {},
   "source": [
    "**Tokenizer load in *util_io.py* and *util_iob2.py* has to be the same that the model to fine-tune.<br>\n",
    "Please check *_convert_tokenizer* var in those files and adapt MODEL_NAME variable in this notebook before creating datasets.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6813783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on disk\n",
    "from multihead_utils.util_io import _convert_tokenizer\n",
    "print(\"Tokenizer called in util_io.py\")\n",
    "_convert_tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multihead_utils.util_iob2 import _convert_tokenizer\n",
    "print(\"Tokenizer called in util_iob2.py\")\n",
    "_convert_tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16874c6b-e422-4e07-b31c-856841a71300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GLOBAL CONSTANTS\n",
    "import config\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "config.SPLIT_SEED = 42 # Random seed used in train/dev/test. Do not change it if you want to recreate the paper results.\n",
    "config.DEBUG = False # If true, text versions of the spacy datasets will be saved along with the .spacy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb71a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"pretrained_camembert_ner\" #Used for folder name, no space, different from real model name\n",
    "#camembert_ner OR\n",
    "#pretrained_camembert_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71010115-8bac-43d9-9638-15f56f029493",
   "metadata": {},
   "source": [
    "## 101. Experiment #1 : Reference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de597797-0541-404e-b721-8667077eb7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "GOLD_REF = DATASETS / \"41-ner_ref_from_pero/gold.csv\"\n",
    "assert GOLD_REF.exists()\n",
    "\n",
    "with open(GOLD_REF,'r',encoding='utf8') as f:\n",
    "    lines = f.readlines()\n",
    "    res = []\n",
    "    for line in lines:\n",
    "        l = line.split('\", \"')\n",
    "        res.append([l[0][1:],l[1][:-2]])\n",
    "gold_reference = pd.DataFrame(res,columns=[\"ner_xml\",\"book\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b1fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TITRE-H and TITRE-P labels to transformers NER Pipeline\n",
    "for i in range(len(gold_reference)):\n",
    "    if '<TITRE-H>' in gold_reference['ner_xml'][i]:\n",
    "        gold_reference['ner_xml'][i] = gold_reference['ner_xml'][i].replace('TITRE-H','TITREH')\n",
    "    if '<TITRE-P>' in gold_reference['ner_xml'][i]:\n",
    "        gold_reference['ner_xml'][i] = gold_reference['ner_xml'][i].replace('TITRE-P','TITREP')\n",
    "\n",
    "gold_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5bdd42-8847-48e5-833f-8cbb8106e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multihead_utils.multihead_dataset_util import train_dev_test_split, unwrap # Local imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CONSTANTS\n",
    "MIN_TRAINSET_SIZE = 30\n",
    "\n",
    " # Split 72/8/20% w. stratified sampling on directories names\n",
    "train, dev, test = train_dev_test_split(gold_reference.to_numpy())\n",
    "print(\"Dev : \"+str(len(dev)))\n",
    "print(\"Test : \"+str(len(test)))\n",
    "\n",
    "# Iteratively split the trainset in half to create smaller trainsets\n",
    "exp1_trainsets = [train]\n",
    "t_len = len(train)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        current = exp1_trainsets[-1]\n",
    "        _, groups = unwrap(current)\n",
    "        smaller, rest = train_test_split(\n",
    "            current,\n",
    "            train_size=0.5,\n",
    "            shuffle=True,\n",
    "            random_state=config.SPLIT_SEED,\n",
    "            stratify=groups,\n",
    "        )\n",
    "        t_len = len(rest)\n",
    "        if t_len < MIN_TRAINSET_SIZE:\n",
    "            break\n",
    "        exp1_trainsets.append(smaller)\n",
    "\n",
    "    except ValueError:\n",
    "        # Stop now if we encounter the error \"The least populated class in y has only 1 member\".\n",
    "        break\n",
    "\n",
    "[len(s) for s in exp1_trainsets] # Should be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f4fd33-72da-4eb8-bec2-c90aa3798c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Dev set should contain 676 examples\n",
    "assert len(dev) == 676\n",
    "\n",
    "# Test set should contain 1685 examples\n",
    "assert len(test) == 1685\n",
    "\n",
    "# Lenghts of exp1_trainsets should be fixed\n",
    "assert sorted([len(s) for s in exp1_trainsets] ) == sorted([6084, 3042, 1521, 760, 380, 190, 95, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a660001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multihead_utils.tools import createStatsTab\n",
    "\n",
    "createStatsTab(train,dev,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de90853",
   "metadata": {},
   "source": [
    "### IO Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd88d23-c062-4772-b53f-610927523397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on disk\n",
    "from multihead_utils.util_io import save_dataset # Local import\n",
    "\n",
    "output_directory = OUT_BASE / f\"m1-experiment_1_prepared_dataset_ref_io_{MODEL_NAME}\"\n",
    "output_directory.mkdir(exist_ok=True, parents=True) # Create if necessary\n",
    "   \n",
    "for train in exp1_trainsets:\n",
    "    datasets = [train, dev, test]\n",
    "    save_dataset(output_directory, datasets, [\"train\",\"dev\",\"test\"], suffix=len(train))\n",
    "output_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396465e",
   "metadata": {},
   "source": [
    "### IOB2 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883fda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on disk\n",
    "from multihead_utils.util_iob2 import save_dataset # Local import\n",
    "\n",
    "output_directory = OUT_BASE / f\"m1-experiment_1_prepared_dataset_ref_iob2_{MODEL_NAME}\"\n",
    "output_directory.mkdir(exist_ok=True, parents=True) # Create if necessary\n",
    "   \n",
    "for train in exp1_trainsets:\n",
    "    datasets = [train, dev, test]\n",
    "    save_dataset(output_directory, datasets, [\"train\",\"dev\",\"test\"], suffix=len(train))\n",
    "output_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59a0f5",
   "metadata": {},
   "source": [
    "## 102. Experiment 2 : Pero OCR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39459f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "GOLD_REF = DATASETS / \"31-ner_align_pero/gold.csv\"\n",
    "\n",
    "gold_reference = pd.read_csv(GOLD_REF, header=None, names=[\"ner_xml\",\"book\"],skipinitialspace='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4560741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TITRE-H and TITRE-P labels to transformers NER Pipeline\n",
    "for i in range(len(gold_reference)):\n",
    "    if '<TITRE-H>' in gold_reference['ner_xml'][i]:\n",
    "        gold_reference['ner_xml'][i] = gold_reference['ner_xml'][i].replace('TITRE-H','TITREH')\n",
    "    if '<TITRE-P>' in gold_reference['ner_xml'][i]:\n",
    "        gold_reference['ner_xml'][i] = gold_reference['ner_xml'][i].replace('TITRE-P','TITREP')\n",
    "gold_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multihead_utils.multihead_dataset_util import train_dev_test_split, unwrap # Local imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CONSTANTS\n",
    "MIN_TRAINSET_SIZE = 30\n",
    "\n",
    " # Split 72/8/20% w. stratified sampling on directories names\n",
    "train, dev, test = train_dev_test_split(gold_reference.to_numpy())\n",
    "print(\"Dev : \"+str(len(dev)))\n",
    "print(\"Test : \"+str(len(test)))\n",
    "\n",
    "# Iteratively split the trainset in half to create smaller trainsets\n",
    "exp1_trainsets = [train]\n",
    "t_len = len(train)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        current = exp1_trainsets[-1]\n",
    "        _, groups = unwrap(current)\n",
    "        smaller, rest = train_test_split(\n",
    "            current,\n",
    "            train_size=0.5,\n",
    "            shuffle=True,\n",
    "            random_state=config.SPLIT_SEED,\n",
    "            stratify=groups,\n",
    "        )\n",
    "        t_len = len(rest)\n",
    "        if t_len < MIN_TRAINSET_SIZE:\n",
    "            break\n",
    "        exp1_trainsets.append(smaller)\n",
    "\n",
    "    except ValueError:\n",
    "        # Stop now if we encounter the error \"The least populated class in y has only 1 member\".\n",
    "        break\n",
    "\n",
    "[len(s) for s in exp1_trainsets] # Should be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289cded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Dev set should contain 676 examples\n",
    "assert len(dev) == 676\n",
    "\n",
    "# Test set should contain 1685 examples\n",
    "assert len(test) == 1685\n",
    "\n",
    "# Lenghts of exp1_trainsets should be fixed\n",
    "assert sorted([len(s) for s in exp1_trainsets] ) == sorted([6084, 3042, 1521, 760, 380, 190, 95, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e8cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multihead_utils.tools import createStatsTab\n",
    "\n",
    "createStatsTab(train,dev,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d012a97",
   "metadata": {},
   "source": [
    "### IO Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3198886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on disk\n",
    "from multihead_utils.util_io import save_dataset # Local import\n",
    "\n",
    "output_directory = OUT_BASE / f\"m1-experiment_2_prepared_dataset_pero_ocr_io_{MODEL_NAME}\"\n",
    "output_directory.mkdir(exist_ok=True, parents=True) # Create if necessary\n",
    "   \n",
    "for train in exp1_trainsets:\n",
    "    datasets = [train, dev, test]\n",
    "    save_dataset(output_directory, datasets, [\"train\",\"dev\",\"test\"], suffix=len(train))\n",
    "output_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2146e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "\n",
    "dataset_root = f\"huggingface_6084\"\n",
    "datasetdir = output_directory / dataset_root\n",
    "train_dev_test = load_from_disk(datasetdir)\n",
    "\n",
    "print(train_dev_test[\"train\"][3][\"tokens\"])\n",
    "print(train_dev_test[\"train\"][3][\"ner_tags_niv1\"])\n",
    "print(train_dev_test[\"train\"][3][\"ner_tags_niv2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca7195e",
   "metadata": {},
   "source": [
    "### IOB2 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ee1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on disk\n",
    "from multihead_utils.util_iob2 import save_dataset # Local import\n",
    "\n",
    "output_directory = OUT_BASE / f\"m1-experiment_2_prepared_dataset_pero_ocr_iob2_{MODEL_NAME}\"\n",
    "output_directory.mkdir(exist_ok=True, parents=True) # Create if necessary\n",
    "   \n",
    "for train in exp1_trainsets:\n",
    "    datasets = [train, dev, test]\n",
    "    save_dataset(output_directory, datasets, [\"train\",\"dev\",\"test\"], suffix=len(train))\n",
    "output_directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
