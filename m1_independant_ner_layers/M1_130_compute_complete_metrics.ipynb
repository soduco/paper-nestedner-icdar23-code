{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60788d09",
   "metadata": {},
   "source": [
    "# M1 - 130 - Compute metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c59565",
   "metadata": {},
   "source": [
    "Using by levels scores computed during the fine-tuning :\n",
    "* compute evaluation on joint-labels\n",
    "* compute evaluation on all entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30acc9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ENV_IS_GOOGLE_COLAB = True if 'google.colab' in str(get_ipython()) else False\n",
    "os.environ[\"ENV_IS_GOOGLE_COLAB\"] = str(ENV_IS_GOOGLE_COLAB)\n",
    "\n",
    "if ENV_IS_GOOGLE_COLAB:\n",
    "  from google.colab import drive\n",
    "  mountpoint = Path(\"/content/drive\")\n",
    "  drive.mount(str(mountpoint)) # Mount gdrive to BASE\n",
    "  base = mountpoint / \"MyDrive/article_icdar_2023\" # Adapt this to your situation\n",
    "  sys.path.append(str(base)) # Add BASE to Python Path\n",
    "  BASE = Path(base).resolve() # Make BASE absolute\n",
    "  DATASETS =  BASE / \"dataset_ICDAR\"\n",
    "  OUT_BASE = BASE / \"res_ICDAR/method_1\"\n",
    "else:\n",
    "  BASE = Path().resolve() # Directory of this approach\n",
    "  #Adapt this to your situation\n",
    "  DATASETS = Path('../dataset_ICDAR').resolve() #Where your data are located befor Dataset object creation\n",
    "  OUT_BASE = Path('../res_ICDAR/method_1').resolve() #Where you save the results of this notebook\n",
    "\n",
    "print(sys.path)\n",
    "print(BASE)\n",
    "print(DATASETS)\n",
    "print(OUT_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46960c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "ls = sorted(glob.glob(f\"{OUT_BASE}/*_metrics/*\"))\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadb954",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65945292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please choose index in ls list to compute complete metrics on initial results\n",
    "i = 0\n",
    "dir_ = ls[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'iob2' in dir_:\n",
    "    FORMAT = \"IOB2\"\n",
    "else:\n",
    "    FORMAT = \"IO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b54cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "\n",
    "TRAINSETS_SIZES = [6084] #To train only on the biggest dataset\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "METRICS_OUTPUT_DIR = dir_\n",
    "METRICS_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09adbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "RUNS = sorted(glob.glob(f\"{METRICS_OUTPUT_DIR}/run_*\"))\n",
    "L1_test = []\n",
    "L2_test = []\n",
    "for r in RUNS:\n",
    "    l1 = glob.glob(f\"{r}\" + \"/level-1/test_*\")\n",
    "    L1_test.append(l1[0])\n",
    "    l2 = glob.glob(f\"{r}\" + \"/level-2/test_*\")\n",
    "    L2_test.append(l2[0])\n",
    "L1_test,L2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f8b1e9",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "Align L1 and L2 predictions and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanLists(res):\n",
    "    entries = []\n",
    "    for entry in res:\n",
    "        entry = entry.split(\", \")\n",
    "        ftag = []\n",
    "        for tag in entry:\n",
    "            tag = tag.replace(\"'\",'')\n",
    "            ftag.append(tag)\n",
    "        entries.append(ftag)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86adf969",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {} #results[run][set][entree]   set : predictionsl1 labelsl1 predictionsl2 labelsl2\n",
    "\n",
    "#for each run\n",
    "for i in range(len(L1_test)):\n",
    "    # Opening JSON file\n",
    "    f1 = open(L1_test[i])\n",
    "    f2 = open(L2_test[i])\n",
    "    # returns JSON object as\n",
    "    # a dictionary\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "    \n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    \n",
    "    predictions1 = data1['eval_predictions'][2:-2]\n",
    "    predictions1 = predictions1.split('], [')\n",
    "    labels1 = data1['eval_labels'][2:-2]\n",
    "    labels1 = labels1.split('], [')\n",
    "    predictions2 = data2['eval_predictions'][2:-2]\n",
    "    predictions2 = predictions2.split('], [')\n",
    "    labels2 = data2['eval_labels'][2:-2]\n",
    "    labels2 = labels2.split('], [')\n",
    "    \n",
    "    fpredictions1 = cleanLists(predictions1)\n",
    "    fpredictions2 = cleanLists(predictions2)\n",
    "    flabels1 = cleanLists(labels1)\n",
    "    flabels2 = cleanLists(labels2)\n",
    "    \n",
    "    results[f\"run_{i+1}\"] = {\"gold_l1\":flabels1,\"gold_l2\":flabels2,\"predictions_l1\":fpredictions1,\"predictions_l2\":fpredictions2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd56547",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff20df",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"run_1\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd37ec",
   "metadata": {},
   "source": [
    "## Compute joint-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createJointLabels(list1,list2):\n",
    "    entries = []\n",
    "    for i,(entryl1,entryl2) in enumerate(zip(list1,list2)):\n",
    "        assert len(entryl1) == len(entryl2)\n",
    "        new_tags = []\n",
    "        for j in range(len(entryl1)):\n",
    "            tag = entryl1[j] + '+' + entryl2[j]\n",
    "            tag = tag.replace('+I-','+i_')\n",
    "            tag = tag.replace('I-','I-i_')\n",
    "            tag = tag.replace('+B-','+b_')\n",
    "            tag = tag.replace('B-','I-b_')\n",
    "            new_tags.append(tag)\n",
    "        entries.append(new_tags)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 20\n",
    "jl_gold = createJointLabels(results[\"run_1\"][\"gold_l1\"],results[\"run_1\"][\"gold_l2\"])\n",
    "jl_preds = createJointLabels(results[\"run_1\"][\"predictions_l1\"],results[\"run_1\"][\"predictions_l2\"])\n",
    "print(jl_gold[i])\n",
    "print(jl_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad112ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createL1L2Labels(list1,list2):\n",
    "    entries = []\n",
    "    for i,(entryl1,entryl2) in enumerate(zip(list1,list2)):\n",
    "        assert len(entryl1) == len(entryl2)\n",
    "        new_tags = []\n",
    "        for j in range(len(entryl1)):\n",
    "            if 'O' in entryl2[j]:\n",
    "                prefixe_l2 = ''\n",
    "                tag_l2 = 'O'\n",
    "            else:\n",
    "                prefixe_l2, tag_l2 = entryl2[j].split('-')\n",
    "                \n",
    "            if 'O' in entryl1[j]:\n",
    "                prefixe_l1 = ''\n",
    "                tag_l1 = 'O'\n",
    "            else:\n",
    "                prefixe_l1, tag_l1 = entryl1[j].split('-')\n",
    "                \n",
    "            if prefixe_l2 == '' and prefixe_l1 != '':\n",
    "                new_tags.append(prefixe_l1 + '-' + tag_l2 + '+' + tag_l1)\n",
    "            elif prefixe_l2 == '' and prefixe_l1 == '':\n",
    "                new_tags.append('O+O')\n",
    "            else:\n",
    "                new_tags.append(prefixe_l2 + '-' + tag_l2 + '+' + tag_l1)\n",
    "                \n",
    "        entries.append(new_tags)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1l2_gold = createL1L2Labels(results[\"run_1\"][\"gold_l1\"],results[\"run_1\"][\"gold_l2\"])\n",
    "l1l2_preds = createL1L2Labels(results[\"run_1\"][\"predictions_l1\"],results[\"run_1\"][\"predictions_l2\"])\n",
    "print(l1l2_gold[i])\n",
    "print(l1l2_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdbfdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createL1L2Labels(jlentries):\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    l1_p = []\n",
    "    l2_p = []\n",
    "    res = []\n",
    "    for tags in jlentries:\n",
    "        new_tags = []\n",
    "        #print(tags)\n",
    "        for elem in tags:\n",
    "            if elem[0] == 'I' and '+O' not in elem:\n",
    "                parts = elem.split('+')\n",
    "                e1 = parts[0][4:]\n",
    "                p1 = parts[0][2:3].upper()\n",
    "                e2 = parts[1][2:]\n",
    "                p2 = parts[1][:1].upper()\n",
    "                tag = p2 + '-' + e2 + '+' + e1\n",
    "            elif elem[0] == 'I' and elem[-1] == 'O':\n",
    "                parts = elem.split('+')\n",
    "                e1 = parts[0][4:]\n",
    "                p1 = parts[0][2:3].upper()\n",
    "                e2 = 'O'\n",
    "                p2 = parts[1][:1].upper()\n",
    "                tag = p1 + '-' + e2 + '+' + e1\n",
    "            elif elem[0] == 'O' and len(elem) > 3:\n",
    "                parts = elem.split('+')\n",
    "                e1 = 'O'\n",
    "                p1 = ''\n",
    "                e2 = parts[1][2:]\n",
    "                p2 = parts[1][:1].upper()\n",
    "                tag = p2 + '-' + e2 + '+' + e1\n",
    "                #print(e1,p1,e2,p2)\n",
    "            else:\n",
    "                e1 = 'O'\n",
    "                p1 = ''\n",
    "                e2 = 'O'\n",
    "                p2 = ''\n",
    "                tag = e2 + '+' + e1\n",
    "            new_tags.append(tag)\n",
    "        res.append(new_tags)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09cf530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFlatLabels(jl_gold,jl_preds,FORMAT):\n",
    "    \n",
    "    LABELS_ID_TO_DAS = []\n",
    "    if FORMAT == 'IO':\n",
    "        LABELS_ID = {\n",
    "            \"O+O\" : 0,\n",
    "            \"I-i_PER+O\" : 1,\n",
    "            \"I-i_PER+i_TITREH\" : 2,\n",
    "            \"I-i_ACT+O\" : 3,\n",
    "            \"I-i_DESC+O\" : 4,\n",
    "            \"I-i_DESC+i_ACT\" : 5,\n",
    "            \"I-i_DESC+i_TITREP\" : 6,\n",
    "            \"I-i_SPAT+O\" : 7,\n",
    "            \"I-i_SPAT+i_LOC\" : 8,\n",
    "            \"I-i_SPAT+i_CARDINAL\" : 9,\n",
    "            \"I-i_SPAT+i_FT\" : 10,\n",
    "            \"I-i_TITRE+O\" : 11\n",
    "        }\n",
    "        LABELS_ID_TO_DAS =[\"O\",\"I-PER\",\"I-TITRE\",\"I-ACT\",\"O\",\"I-ACT\",\"I-TITRE\",\"O\",\"I-LOC\",\"I-CARDINAL\",\"I-FT\",\"I-TITRE\"]\n",
    "        \n",
    "    elif FORMAT == \"IOB2\":\n",
    "        LABELS_ID = {\n",
    "            \"O+O\" : 0,\n",
    "            \"I-b_PER+O\" : 1,\n",
    "            \"I-i_PER+O\" : 2,\n",
    "            \"I-b_PER+b_TITREH\" : 3,\n",
    "            \"I-i_PER+b_TITREH\" : 4,\n",
    "            \"I-i_PER+i_TITREH\" : 5,\n",
    "            \"I-b_ACT+O\" : 6,\n",
    "            \"I-i_ACT+O\" : 7,\n",
    "            \"I-b_DESC+O\" : 8,\n",
    "            \"I-i_DESC+O\" : 9,\n",
    "            \"I-b_DESC+b_ACT\" : 10,\n",
    "            \"I-i_DESC+b_ACT\" : 11,\n",
    "            \"I-i_DESC+i_ACT\" : 12,\n",
    "            \"I-b_DESC+b_TITREP\" : 13,\n",
    "            \"I-i_DESC+b_TITREP\" : 14,\n",
    "            \"I-i_DESC+i_TITREP\" : 15,\n",
    "            \"I-b_SPAT+O\" : 16,\n",
    "            \"I-i_SPAT+O\" : 17,\n",
    "            \"I-b_SPAT+b_LOC\" : 18,\n",
    "            \"I-i_SPAT+b_LOC\" : 19,\n",
    "            \"I-i_SPAT+i_LOC\" : 20,\n",
    "            \"I-b_SPAT+b_CARDINAL\" : 21,\n",
    "            \"I-i_SPAT+b_CARDINAL\" : 22,\n",
    "            \"I-i_SPAT+i_CARDINAL\" : 23,\n",
    "            \"I-b_SPAT+b_FT\" : 24,\n",
    "            \"I-i_SPAT+b_FT\" : 25,\n",
    "            \"I-i_SPAT+i_FT\" : 26,\n",
    "            \"I-b_TITRE+O\" : 27,\n",
    "            \"I-i_TITRE+O\" : 28\n",
    "        }\n",
    "        LABELS_ID_TO_DAS = ['O','I-PER','I-PER','I-TITRE','I-TITRE','I-TITRE','I-ACT','I-ACT','O','O','I-ACT','I-ACT','I-ACT','I-TITRE','I-TITRE','I-TITRE','O','O','I-LOC','I-LOC','I-LOC','I-CARDINAL','I-CARDINAL','I-CARDINAL','I-FT','I-FT','I-FT','I-TITRE','I-TITRE']\n",
    "    \n",
    "    das_labels = []\n",
    "    for entry in jl_gold:\n",
    "        das_entry = []\n",
    "        for elem in entry:\n",
    "            das_label = LABELS_ID_TO_DAS[LABELS_ID[elem]]\n",
    "            das_entry.append(das_label)\n",
    "        das_labels.append(das_entry)\n",
    "    \n",
    "    das_predictions = []\n",
    "    for entry in jl_preds:\n",
    "        das_entry = []\n",
    "        for elem in entry:\n",
    "            try:\n",
    "                das_label = LABELS_ID_TO_DAS[LABELS_ID[elem]]\n",
    "                das_entry.append(das_label)\n",
    "            except:\n",
    "                das_label = 'I-NO'\n",
    "                das_entry.append(das_label)\n",
    "        das_predictions.append(das_entry)\n",
    "        \n",
    "    return das_labels, das_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff23e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from multihead_utils.tools import unique\n",
    "\n",
    "for i in range(0,len(L1_test)):\n",
    "    r = i+1\n",
    "    ################ L1\n",
    "    metric_l1 = load_metric(\"seqeval\")\n",
    "    results_l1 = metric_l1.compute(predictions=results[f\"run_{r}\"][\"predictions_l1\"], references=results[f\"run_{r}\"][\"gold_l1\"])\n",
    "    \n",
    "    ################ L2\n",
    "    metric_l2 = load_metric(\"seqeval\")\n",
    "    results_l2 = metric_l2.compute(predictions=results[f\"run_{r}\"][\"predictions_l2\"], references=results[f\"run_{r}\"][\"gold_l2\"])\n",
    "    \n",
    "    ################ Joint-labels metrics (including prefixes)\n",
    "    jl_predictions = createJointLabels(results[f\"run_{r}\"][\"predictions_l1\"],results[f\"run_{r}\"][\"predictions_l2\"]) #Gold\n",
    "    jl_labels = createJointLabels(results[f\"run_{r}\"][\"gold_l1\"],results[f\"run_{r}\"][\"gold_l2\"]) #Predictions\n",
    "    \n",
    "    metric_jl = load_metric(\"seqeval\")\n",
    "    results_jl = metric_jl.compute(predictions=jl_predictions, references=jl_labels)\n",
    "    \n",
    "    #Save the joint-labels\n",
    "    #jointlabelslist = unique(jl_predictions)\n",
    "    \n",
    "    ################ L1+l2\n",
    "    if FORMAT == 'IOB2':\n",
    "        l1l2_predictions = createL1L2Labels(jl_predictions)\n",
    "        l1l2_labels = createL1L2Labels(jl_labels)\n",
    "    else:\n",
    "        l1l2_predictions = jl_predictions\n",
    "        l1l2_labels = jl_labels\n",
    "    \n",
    "    metric_l1l2 = load_metric(\"seqeval\")\n",
    "    results_l1l2 = metric_l1l2.compute(predictions=l1l2_predictions, references=l1l2_labels)\n",
    "    \n",
    "    ################ DAS\n",
    "    das_labels, das_predictions = createFlatLabels(jl_gold,jl_preds,FORMAT)\n",
    "    \n",
    "    metric_das = load_metric(\"seqeval\")\n",
    "    results_das = metric_das.compute(predictions=das_predictions, references=das_labels)\n",
    "    \n",
    "    ################# Global metrics\n",
    "    all_preds = results[f\"run_{r}\"][\"predictions_l1\"] + results[f\"run_{r}\"][\"predictions_l2\"]\n",
    "    all_labels = results[f\"run_{r}\"][\"gold_l1\"] + results[f\"run_{r}\"][\"gold_l2\"]\n",
    "\n",
    "    metric_all = load_metric(\"seqeval\")\n",
    "    results_all = metric_all.compute(predictions=all_preds, references=all_labels)\n",
    "    \n",
    "    scores = {\n",
    "        #Global results (L1 and L2)\n",
    "        \"eval_precision-all\": results_all[\"overall_precision\"],\n",
    "        \"eval_recall-all\": results_all[\"overall_recall\"],\n",
    "        \"eval_f1-all\": results_all[\"overall_f1\"],\n",
    "        \"eval_accuracy-all\": results_all[\"overall_accuracy\"],\n",
    "        #L1\n",
    "        \"eval_precision-l1\": results_l1[\"overall_precision\"],\n",
    "        \"eval_recall-l1\": results_l1[\"overall_recall\"],\n",
    "        \"eval_f1-l1\": results_l1[\"overall_f1\"],\n",
    "        \"eval_accuracy-l1\": results_l1[\"overall_accuracy\"],\n",
    "        #L2\n",
    "        \"eval_precision-l2\": results_l2[\"overall_precision\"],\n",
    "        \"eval_recall-l2\": results_l2[\"overall_recall\"],\n",
    "        \"eval_f1-l2\": results_l2[\"overall_f1\"],\n",
    "        \"eval_accuracy-l2\": results_l2[\"overall_accuracy\"],\n",
    "        #Joint-labels (includign prefixes)\n",
    "        \"eval_precision\": results_jl[\"overall_precision\"],\n",
    "        \"eval_recall\": results_jl[\"overall_recall\"],\n",
    "        \"eval_f1\": results_jl[\"overall_f1\"],\n",
    "        \"eval_accuracy\": results_jl[\"overall_accuracy\"],\n",
    "        #L1+L2\n",
    "        \"eval_precision-l1l2\": results_l1l2[\"overall_precision\"],\n",
    "        \"eval_recall-l1l2\": results_l1l2[\"overall_recall\"],\n",
    "        \"eval_f1-l1l2\": results_l1l2[\"overall_f1\"],\n",
    "        \"eval_accuracy-l1l2\": results_l1l2[\"overall_accuracy\"],\n",
    "        #DAS\n",
    "        \"eval_precision-das\": results_das[\"overall_precision\"],\n",
    "        \"eval_recall-das\": results_das[\"overall_recall\"],\n",
    "        \"eval_f1-das\": results_das[\"overall_f1\"],\n",
    "        \"eval_accuracy-das\": results_das[\"overall_accuracy\"],\n",
    "        #\"joint-labels-list\":f\"{jointlabelslist}\",\n",
    "        #By class\n",
    "        \"eval_PER\": results_all['PER'],\n",
    "        \"eval_ACT\": results_all['ACT'],\n",
    "        \"eval_ACT_L1\": results_l1['ACT'],\n",
    "        \"eval_ACT_L2\": results_l2['ACT'],\n",
    "        \"eval_DESC\": results_all['DESC'],\n",
    "        \"eval_TITREH\": results_all['TITREH'],\n",
    "        \"eval_TITREP\": results_all['TITREP'],\n",
    "        \"eval_SPAT\": results_all['SPAT'],\n",
    "        \"eval_LOC\": results_all['LOC'],\n",
    "        \"eval_CARDINAL\": results_all['CARDINAL'],\n",
    "        \"eval_FT\": results_all['FT']\n",
    "    }\n",
    "    \n",
    "    #if 'TITRE' in list(results_all.keys()):\n",
    "        #scores[\"TITRE\"] = f\"{results_all['eval_TITRE']}\"\n",
    "    print(scores)\n",
    "    if not os.path.isdir(f\"{METRICS_OUTPUT_DIR}/run_{r}\"):\n",
    "        os.mkdir(f\"{METRICS_OUTPUT_DIR}/run_{r}\")\n",
    "\n",
    "    with open(f\"{METRICS_OUTPUT_DIR}/run_{r}/test_{str(TRAINSETS_SIZES[0])}.json\", 'w') as fp:\n",
    "        json.dump(scores, fp, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914253ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
