{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294705ff-9e89-499f-a41f-9494362be5f9",
   "metadata": {
    "id": "2552858d-7386-4e9a-8b0e-c338b920f783"
   },
   "source": [
    "# M2 - 220 - Experiment #2 - Joint-Labelling with noisy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flD_9oT8LmDB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flD_9oT8LmDB",
    "outputId": "63a92e5f-d414-46cc-db86-b46981e42594"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers datasets spacy transformers[sentencepiece] seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cZvwNIzqBwDs",
   "metadata": {
    "id": "cZvwNIzqBwDs",
    "tags": []
   },
   "source": [
    "## Initialisation\n",
    "Set the BASE path.\n",
    "If run on Google Colab, will also mout Google Drive to the moutpoint given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LWJVak2mB6bI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWJVak2mB6bI",
    "outputId": "dbb54104-560b-480c-d4b0-74a0787e2024"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ENV_IS_GOOGLE_COLAB = True if 'google.colab' in str(get_ipython()) else False\n",
    "os.environ[\"ENV_IS_GOOGLE_COLAB\"] = str(ENV_IS_GOOGLE_COLAB)\n",
    "\n",
    "if ENV_IS_GOOGLE_COLAB:\n",
    "  from google.colab import drive\n",
    "  mountpoint = Path(\"/content/drive\")\n",
    "  drive.mount(str(mountpoint)) # Mount gdrive to BASE\n",
    "  base = mountpoint / \"MyDrive/article_icdar_2023\" # Adapt this to your situation\n",
    "  sys.path.append(str(base)) # Add BASE to Python Path\n",
    "  BASE = Path(base).resolve() # Make BASE absolute\n",
    "  DATASETS =  BASE / \"dataset_ICDAR\"\n",
    "  OUT_BASE = BASE / \"res_ICDAR/method_2\"\n",
    "else:\n",
    "  BASE = Path().resolve() # Directory of this approach\n",
    "  #Adapt this to your situation\n",
    "  DATASETS = Path('../dataset_ICDAR').resolve() #Where your data are located befor Dataset object creation\n",
    "  OUT_BASE = Path('../res_ICDAR/method_2').resolve() #Where you save the results of this notebook\n",
    "\n",
    "print(sys.path)\n",
    "print(BASE)\n",
    "print(DATASETS)\n",
    "print(OUT_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10706f1",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68582532",
   "metadata": {},
   "source": [
    "**Don't forget to check tokenizer name in *model_util_IO.py* and *model_util_IOB2.py* files (same name as model) !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a270f9-5f9e-449e-bbff-69136b383507",
   "metadata": {
    "id": "2552858d-7386-4e9a-8b0e-c338b920f783"
   },
   "outputs": [],
   "source": [
    "RUN_CAMEMBERT_IO = False\n",
    "RUN_CAMEMBERT_IOB2 = False\n",
    "#Can't run together because of convert_tokenizer_\n",
    "RUN_PTRN_CAMEMBERT_IO = True\n",
    "RUN_PTRN_CAMEMBERT_IOB2 = True\n",
    "\n",
    "# Number of times a model will be trained & evaluated on each a dataset\n",
    "N_RUNS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hxHdPTBlCCFO",
   "metadata": {
    "id": "hxHdPTBlCCFO"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91140aa5-b377-47c1-bd44-844cd9365ec3",
   "metadata": {
    "id": "91140aa5-b377-47c1-bd44-844cd9365ec3"
   },
   "outputs": [],
   "source": [
    "# COMMON CONSTANTS\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 100,\n",
    "    \"max_steps\": 5000,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"greater_is_better\":True,\n",
    "    \"metric_for_best_model\": \"f1\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba5de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "\n",
    "#Print examples from datasets\n",
    "def loadExample(INPUT_DIR,set_length:int,i:int,subset:str):\n",
    "    set_ = load_from_disk(INPUT_DIR / f\"huggingface_{set_length}\")\n",
    "    data = {\"tokens\": set_[subset][i][\"tokens\"],\n",
    "            \"labels\": set_[subset][i][\"ner_tags\"]}\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a5f08",
   "metadata": {},
   "source": [
    "# 221 - Train & eval : IO Pero OCR dataset with CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"camembert_ner\"\n",
    "MODEL = \"Jean-Baptiste/camembert-ner\"\n",
    "LABEL = \"io\"\n",
    "FOLDER = \"221-camembert-ner-joint-labelling-io\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e89ac0",
   "metadata": {},
   "source": [
    "### 221.1 Load IO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06332f9f-37d5-4a0c-9af6-1f2bda236789",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06332f9f-37d5-4a0c-9af6-1f2bda236789",
    "outputId": "5749eaf4-a3d1-40fd-b2d4-fd45a27eb16e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "\n",
    "# Expected datasets indexed by number of examples in the trainset\n",
    "#TRAINSETS_SIZES = [47,95,190,380,760,1521,3042,6084] #To train on the 7 datasets\n",
    "TRAINSETS_SIZES = [6084] #To train only on the biggest dataset\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "INPUT_DIR = OUT_BASE / f\"m2-experiment_2_prepared_dataset_pero_ocr_{LABEL}_{MODEL_NAME}\"\n",
    "METRICS_OUTPUT_DIR = OUT_BASE / \"m2-220-experiment_2_metrics\"\n",
    "INPUT_DIR, METRICS_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf6e330",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadExample(INPUT_DIR,TRAINSETS_SIZES[-1],10,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bacc1d2",
   "metadata": {},
   "source": [
    "### 221.2 Fine-tuning with IO labels - train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e821087-3623-4c14-a8fb-63dcc98dc1d4",
   "metadata": {
    "id": "2e821087-3623-4c14-a8fb-63dcc98dc1d4"
   },
   "outputs": [],
   "source": [
    "from config import logger\n",
    "from datasets import load_from_disk\n",
    "import json\n",
    "from camembert_utils.util_IO import init_model, train_eval_loop, _convert_tokenizer\n",
    "\n",
    "def train_bert(metrics_output_directory):\n",
    "    # Train & evaluate loop\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        output_dir = metrics_output_directory / f\"run_{run}\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for trainset_size in TRAINSETS_SIZES:\n",
    "            datasetdir = INPUT_DIR / f\"huggingface_{trainset_size}\"\n",
    "            logger.info(f\"Running on datasets in {datasetdir}\")\n",
    "            logger.info(f\"Metrics will be saved in {output_dir}\")\n",
    "            \n",
    "            model, tokenizer, training_args = init_model(MODEL, local_config)\n",
    "            logger.info(f\"{model} #{run}, will save in {output_dir}\")\n",
    "\n",
    "            train_dev_test = load_from_disk(datasetdir)\n",
    "            train = train_dev_test[\"train\"]\n",
    "            dev = train_dev_test[\"dev\"]\n",
    "            test = train_dev_test[\"test\"]\n",
    "            metrics = train_eval_loop(model,         # Implicit. Must be setbefore calling train_bert()\n",
    "                                      training_args, # Idem\n",
    "                                      tokenizer,\n",
    "                                      train,dev,test)\n",
    "\n",
    "            # Save the dev and test metrics\n",
    "            metrics_file = output_dir / f\"test_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[0], o)\n",
    "\n",
    "            metrics_file = output_dir / f\"dev_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[1], o)\n",
    "                \n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05387e39-dd69-491e-9517-57490356e5e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05387e39-dd69-491e-9517-57490356e5e9",
    "outputId": "a894e899-0646-4260-f12f-7468adfbb5b2"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "if RUN_CAMEMBERT_IO:\n",
    "    assert _convert_tokenizer.name_or_path == MODEL\n",
    "    \n",
    "    # MODEL CONSTS\n",
    "    MODEL_METRICS_DIR = METRICS_OUTPUT_DIR / f\"{FOLDER}\"\n",
    "    MODEL_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    MODEL_OUTPUT_MODEL_PATH = OUT_BASE / f\"tmp/{FOLDER}\"\n",
    "    MODEL_METRICS_DIR, MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Set config output dir\n",
    "    local_config = TRAINING_CONFIG.copy() \n",
    "    local_config[\"output_dir\"]=MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Run the main loop\n",
    "    h = time.time()\n",
    "    train_bert(MODEL_METRICS_DIR)\n",
    "    runtime = (time.time()- h)/N_RUNS\n",
    "    print(f\"Run-time is equal to {str(datetime.timedelta(seconds=runtime))}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipped finetuning model for IO labels\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90368bf0",
   "metadata": {},
   "source": [
    "Best model :\n",
    "Mean run time : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ddb911",
   "metadata": {},
   "source": [
    "# 222 - Train & eval : IOB2 Pero OCR dataset with CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"camembert_ner\"\n",
    "MODEL = \"Jean-Baptiste/camembert-ner\"\n",
    "LABEL = \"iob2\"\n",
    "FOLDER = \"222-camembert-ner-joint-labelling-iob2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a3f98",
   "metadata": {},
   "source": [
    "### 222.1 Load IOB2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b53152",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06332f9f-37d5-4a0c-9af6-1f2bda236789",
    "outputId": "5749eaf4-a3d1-40fd-b2d4-fd45a27eb16e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "\n",
    "# Expected datasets indexed by number of examples in the trainset\n",
    "#TRAINSETS_SIZES = [47,95,190,380,760,1521,3042,6084] #To train on the 7 datasets\n",
    "TRAINSETS_SIZES = [6084] #To train only on the biggest dataset\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "INPUT_DIR = OUT_BASE / f\"m2-experiment_2_prepared_dataset_pero_ocr_{LABEL}_{MODEL_NAME}\"\n",
    "METRICS_OUTPUT_DIR = OUT_BASE / \"m2-220-experiment_2_metrics\"\n",
    "INPUT_DIR, METRICS_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f78a783",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadExample(INPUT_DIR,TRAINSETS_SIZES[-1],10,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4febe76",
   "metadata": {},
   "source": [
    "### 222.2 Fine-tuning with IOB2 labels - train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d241d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import logger\n",
    "from datasets import load_from_disk\n",
    "import json\n",
    "from camembert_utils.util_IOB2 import init_model, train_eval_loop, _convert_tokenizer\n",
    "\n",
    "def train_bert(metrics_output_directory):\n",
    "    # Train & evaluate loop\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        output_dir = metrics_output_directory / f\"run_{run}\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for trainset_size in TRAINSETS_SIZES:\n",
    "            datasetdir = INPUT_DIR / f\"huggingface_{trainset_size}\"\n",
    "            logger.info(f\"Running on datasets in {datasetdir}\")\n",
    "            logger.info(f\"Metrics will be saved in {output_dir}\")\n",
    "            \n",
    "            model, tokenizer, training_args = init_model(MODEL, local_config,run)\n",
    "            logger.info(f\"{model} #{run}, will save in {output_dir}\")\n",
    "            \n",
    "            train_dev_test = load_from_disk(datasetdir)\n",
    "            train = train_dev_test[\"train\"]\n",
    "            dev = train_dev_test[\"dev\"]\n",
    "            test = train_dev_test[\"test\"]\n",
    "            metrics = train_eval_loop(model,         # Implicit. Must be setbefore calling train_bert()\n",
    "                                      training_args, # Idem\n",
    "                                      tokenizer,\n",
    "                                      train,dev,test)\n",
    "\n",
    "            # Save the dev and test metrics\n",
    "            metrics_file = output_dir / f\"test_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[0], o)\n",
    "\n",
    "            metrics_file = output_dir / f\"dev_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[1], o)\n",
    "                \n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c303ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05387e39-dd69-491e-9517-57490356e5e9",
    "outputId": "a894e899-0646-4260-f12f-7468adfbb5b2"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "if RUN_CAMEMBERT_IOB2:\n",
    "    print(_convert_tokenizer.name_or_path)\n",
    "    assert _convert_tokenizer.name_or_path == MODEL\n",
    "    \n",
    "    # MODEL CONSTS\n",
    "    MODEL_METRICS_DIR = METRICS_OUTPUT_DIR / f\"{FOLDER}\"\n",
    "    MODEL_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    MODEL_OUTPUT_MODEL_PATH = OUT_BASE / f\"tmp/{FOLDER}\"\n",
    "    MODEL_METRICS_DIR, MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Set config output dir\n",
    "    local_config = TRAINING_CONFIG.copy() \n",
    "    local_config[\"output_dir\"]=MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Run the main loop\n",
    "    h = time.time()\n",
    "    train_bert(MODEL_METRICS_DIR)\n",
    "    runtime = (time.time()- h)/N_RUNS\n",
    "    print(f\"Run-time is equal to {str(datetime.timedelta(seconds=runtime))}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipped finetuning model for IOB2 labels\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "381b352e",
   "metadata": {},
   "source": [
    "Best model : \n",
    "Mean run time : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea1ad3",
   "metadata": {},
   "source": [
    "# 223 - Train & eval : IO Pero OCR dataset with Pretrained CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"pretrained_camembert_ner\"\n",
    "MODEL = \"HueyNemud/das22-10-camembert_pretrained\"\n",
    "LABEL = \"io\"\n",
    "FOLDER = \"223-pretrained-camembert-ner-joint-labelling-io\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da580f08",
   "metadata": {},
   "source": [
    "### 223.1 Load IO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d060aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "\n",
    "# Expected datasets indexed by number of examples in the trainset\n",
    "#TRAINSETS_SIZES = [47,95,190,380,760,1521,3042,6084] #To train on the 7 datasets\n",
    "TRAINSETS_SIZES = [6084] #To train only on the biggest dataset\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "INPUT_DIR = OUT_BASE / f\"m2-experiment_2_prepared_dataset_pero_ocr_{LABEL}_{MODEL_NAME}\"\n",
    "METRICS_OUTPUT_DIR = OUT_BASE / \"m2-220-experiment_2_metrics\"\n",
    "INPUT_DIR, METRICS_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a56fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadExample(INPUT_DIR,TRAINSETS_SIZES[-1],10,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c497c0",
   "metadata": {},
   "source": [
    "### 223.2 Fine-tuning with IO labels - train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff74397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import logger\n",
    "from datasets import load_from_disk\n",
    "import json\n",
    "from camembert_utils.util_IO import init_model, train_eval_loop, _convert_tokenizer\n",
    "\n",
    "def train_bert(metrics_output_directory):\n",
    "    # Train & evaluate loop\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        output_dir = metrics_output_directory / f\"run_{run}\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for trainset_size in TRAINSETS_SIZES:\n",
    "            datasetdir = INPUT_DIR / f\"huggingface_{trainset_size}\"\n",
    "            logger.info(f\"Running on datasets in {datasetdir}\")\n",
    "            logger.info(f\"Metrics will be saved in {output_dir}\")\n",
    "            \n",
    "            model, tokenizer, training_args = init_model(MODEL, local_config,run)\n",
    "            logger.info(f\"{model} #{run}, will save in {output_dir}\")\n",
    "\n",
    "            train_dev_test = load_from_disk(datasetdir)\n",
    "            train = train_dev_test[\"train\"]\n",
    "            dev = train_dev_test[\"dev\"]\n",
    "            test = train_dev_test[\"test\"]\n",
    "            metrics = train_eval_loop(model,         # Implicit. Must be setbefore calling train_bert()\n",
    "                                      training_args, # Idem\n",
    "                                      tokenizer,\n",
    "                                      train,dev,test)\n",
    "\n",
    "            # Save the dev and test metrics\n",
    "            metrics_file = output_dir / f\"test_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[0], o)\n",
    "\n",
    "            metrics_file = output_dir / f\"dev_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[1], o)\n",
    "                \n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f000bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "if RUN_PTRN_CAMEMBERT_IO:\n",
    "    print(_convert_tokenizer.name_or_path)\n",
    "    assert _convert_tokenizer.name_or_path == MODEL\n",
    "    \n",
    "    # MODEL CONSTS\n",
    "    MODEL_METRICS_DIR = METRICS_OUTPUT_DIR / f\"{FOLDER}\"\n",
    "    MODEL_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    MODEL_OUTPUT_MODEL_PATH = OUT_BASE / f\"tmp/{FOLDER}\"\n",
    "    MODEL_METRICS_DIR, MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "\n",
    "    # Set config output dir\n",
    "    local_config = TRAINING_CONFIG.copy() \n",
    "    local_config[\"output_dir\"]=MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Run the main loop\n",
    "    h = time.time()\n",
    "    train_bert(MODEL_METRICS_DIR)\n",
    "    runtime = (time.time()- h)/N_RUNS\n",
    "    print(f\"Run-time is equal to {str(datetime.timedelta(seconds=runtime))}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipped finetuning model for IO labels\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bb8ef90",
   "metadata": {},
   "source": [
    "Best model : \n",
    "Run time : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb0c3e",
   "metadata": {},
   "source": [
    "# 224 - Train & eval : IOB2 Pero dataset with Pretrained CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72dec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"pretrained_camembert_ner\"\n",
    "MODEL = \"HueyNemud/das22-10-camembert_pretrained\"\n",
    "LABEL = \"iob2\"\n",
    "FOLDER = \"224-pretrained-camembert-ner-joint-labelling-iob2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655c7c8",
   "metadata": {},
   "source": [
    "### 224.1 Load IOB2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "\n",
    "# Expected datasets indexed by number of examples in the trainset\n",
    "#TRAINSETS_SIZES = [47,95,190,380,760,1521,3042,6084] #To train on the 7 datasets\n",
    "TRAINSETS_SIZES = [6084] #To train only on the biggest dataset\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "INPUT_DIR = OUT_BASE / f\"m2-experiment_2_prepared_dataset_pero_ocr_{LABEL}_{MODEL_NAME}\"\n",
    "METRICS_OUTPUT_DIR = OUT_BASE / \"m2-220-experiment_2_metrics\"\n",
    "INPUT_DIR, METRICS_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadExample(INPUT_DIR,TRAINSETS_SIZES[-1],10,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0172b73a",
   "metadata": {},
   "source": [
    "### 224.2 Fine-tuning with IOB2 labels - train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b41bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import logger\n",
    "from datasets import load_from_disk\n",
    "import json\n",
    "from camembert_utils.util_IOB2 import init_model, train_eval_loop, _convert_tokenizer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def train_bert(metrics_output_directory):\n",
    "    # Train & evaluate loop\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        output_dir = metrics_output_directory / f\"run_{run}\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        #Fine-tuning on the biggest dataset\n",
    "        for trainset_size in TRAINSETS_SIZES:\n",
    "            datasetdir = INPUT_DIR / f\"huggingface_{trainset_size}\"\n",
    "            logger.info(f\"Running on datasets in {datasetdir}\")\n",
    "            logger.info(f\"Metrics will be saved in {output_dir}\")\n",
    "            \n",
    "            model, tokenizer, training_args = init_model(MODEL, local_config,run)\n",
    "            logger.info(f\"{model} #{run}, will save in {output_dir}\")\n",
    "\n",
    "            train_dev_test = load_from_disk(datasetdir)\n",
    "            \n",
    "            train = train_dev_test[\"train\"]\n",
    "            dev = train_dev_test[\"dev\"]\n",
    "            test = train_dev_test[\"test\"]\n",
    "            metrics = train_eval_loop(model,         # Implicit. Must be setbefore calling train_bert()\n",
    "                                      training_args, # Idem\n",
    "                                      tokenizer,\n",
    "                                      train,dev,test)\n",
    "\n",
    "            # Save the dev and test metrics\n",
    "            metrics_file = output_dir / f\"test_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[0], o)\n",
    "\n",
    "            metrics_file = output_dir / f\"dev_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[1], o)\n",
    "                \n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "if RUN_PTRN_CAMEMBERT_IOB2:\n",
    "    print(_convert_tokenizer.name_or_path)\n",
    "    assert _convert_tokenizer.name_or_path == MODEL\n",
    "    \n",
    "    # MODEL CONSTS\n",
    "    MODEL_METRICS_DIR = METRICS_OUTPUT_DIR / f\"{FOLDER}\"\n",
    "    MODEL_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    MODEL_OUTPUT_MODEL_PATH = OUT_BASE / f\"tmp/{FOLDER}\"\n",
    "    MODEL_METRICS_DIR, MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Set config output dir\n",
    "    local_config = TRAINING_CONFIG.copy() \n",
    "    local_config[\"output_dir\"]=MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Run the main loop\n",
    "    h = time.time()\n",
    "    train_bert(MODEL_METRICS_DIR)\n",
    "    runtime = (time.time()- h)/N_RUNS\n",
    "    print(f\"Run-time is equal to {str(datetime.timedelta(seconds=runtime))}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipped finetuning model for IOB2 labels\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a1e4b87",
   "metadata": {},
   "source": [
    "Brest model : \n",
    "Runtime : "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "20-experiment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
