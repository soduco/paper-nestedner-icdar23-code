{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 140 - Experiment 1: Figures and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6Oayttuiff2"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ENV_IS_GOOGLE_COLAB = True if 'google.colab' in str(get_ipython()) else False\n",
    "os.environ[\"ENV_IS_GOOGLE_COLAB\"] = str(ENV_IS_GOOGLE_COLAB)\n",
    "\n",
    "if ENV_IS_GOOGLE_COLAB:\n",
    "  from google.colab import drive\n",
    "  mountpoint = Path(\"/content/drive\")\n",
    "  drive.mount(str(mountpoint)) # Mount gdrive to BASE\n",
    "  base = mountpoint / \"MyDrive/article_icdar_2023\" # Adapt this to your situation\n",
    "  sys.path.append(str(base)) # Add BASE to Python Path\n",
    "  BASE = Path(base).resolve() # Make BASE absolute\n",
    "  DATASETS =  BASE / \"dataset_ICDAR\"\n",
    "  OUT_BASE = BASE / \"res_ICDAR/method_1\"\n",
    "else:\n",
    "  BASE = Path().resolve() # Directory of this approach\n",
    "  #Adapt this to your situation\n",
    "  DATASETS = Path('../dataset_ICDAR').resolve() #Where your data are located befor Dataset object creation\n",
    "  OUT_BASE = Path('../res_ICDAR/method_1').resolve() #Where you save the results of this notebook\n",
    "\n",
    "print(sys.path)\n",
    "print(BASE)\n",
    "print(DATASETS)\n",
    "print(OUT_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6Oayttuiff2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "def compile_metrics(path): \n",
    "    rundirs = [f for f in os.listdir(path) if \"run_\" in f]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for run_dir in rundirs:\n",
    "        run_path = path / run_dir\n",
    "        nrun = re.search(\"\\d+\",run_dir)[0]\n",
    "        \n",
    "        files = [f for f in os.listdir(run_path) if \"test_\" in f and \"level\" not in f]\n",
    "        sizes = [int(re.search(\"\\d+\",f)[0]) for f in files]\n",
    "        \n",
    "        for file, size in zip(files,sizes):\n",
    "            file_path = run_path / file\n",
    "            dftmp = pd.read_json(file_path, typ='series')\n",
    "            dftmp = pd.DataFrame([dftmp])\n",
    "            \n",
    "            dftmp[\"trainsize\"] = size \n",
    "            dftmp[\"run\"] = nrun\n",
    "            dftmp[\"trainsize_p\"] = round(100 * size / 6084, 1)\n",
    "            df = pd.concat([df, dftmp])\n",
    "\n",
    "    return df.groupby([\"run\",\"trainsize\"]).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6Oayttuiff2"
   },
   "source": [
    "# 141 - Experiment 1: figures and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6Oayttuiff2"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# PATHS\n",
    "METRICS_DIR_E1 = OUT_BASE / \"m1-110-experiment_1_metrics\"\n",
    "assert METRICS_DIR_E1\n",
    "METRICS_DIR_E1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6Oayttuiff2"
   },
   "outputs": [],
   "source": [
    "# Load Camembert IO metrics from metrics jsons\n",
    "camembert_io_ref = compile_metrics(METRICS_DIR_E1 / \"111-camembert-ner-multihead-io\")\n",
    "camembert_iob2_ref = compile_metrics(METRICS_DIR_E1 / \"112-camembert-ner-multihead-iob2\")\n",
    "prtn_camembert_io_ref = compile_metrics(METRICS_DIR_E1 / \"113-pretrained-camembert-ner-multihead-io\")\n",
    "prtn_camembert_iob2_ref = compile_metrics(METRICS_DIR_E1 / \"114-pretrained-camembert-multihead-iob2\")\n",
    "metrics_raw_ref = pd.concat([camembert_io_ref,camembert_iob2_ref,prtn_camembert_io_ref,prtn_camembert_iob2_ref], keys=[\"CmBERT IO\", \"CmBERT IOB2\", \"Ptrn CmBERT IO\", \"Ptrn CmBERT IOB2\"])\n",
    "metrics_raw_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store p/r/f1 as percentages\n",
    "eval_ = [\"precision-l1\",\"recall-l1\",\"f1-l1\",\"precision-l2\",\"recall-l2\",\"f1-l2\",\"precision\",\"recall\",\"f1\",\"precision-jl\",\"recall_jl\",\"f1_jl\"]\n",
    "metrics_ref = metrics_raw_ref.copy()\n",
    "metrics_ref[eval_] = metrics_raw_ref[eval_].multiply(100., axis=1)\n",
    "metrics_ref.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the average table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEwY8jAYB8yd"
   },
   "outputs": [],
   "source": [
    "# Average over runs\n",
    "averaged_ref = metrics_ref.groupby(level=0).apply(lambda grp: grp.groupby(by=\"trainsize\").mean())\n",
    "averaged_ref.set_index([\"trainsize_p\"], append=True,inplace=True)\n",
    "\n",
    "# Keep just the necessary columns\n",
    "averaged_ref=averaged_ref[[\"f1-l1\",\"f1-l2\",\"f1\",\"f1_jl\"]]\n",
    "\n",
    "# Set pretty names\n",
    "averaged_ref.index.names = ['Model','Trainset Size',\"%\"]\n",
    "averaged_ref.rename(columns={\"f1-l1\":\"Level 1\",\n",
    "    \"f1-l2\":\"Level 2\",\n",
    "    \"f1\":\"Global\",\n",
    "    \"f1_jl\":\"P+L1+P+L2\"\n",
    "                         }, errors=\"raise\", inplace=True)\n",
    "averaged_ref.rename(mapper={\"camembert_io_ref\": \"CmBERT IO\",\"camembert_iob2_ref\": \"CmBERT IOB2\",\"prtn_camembert_io_ref\": \"Ptrn CmBERT IO\",\"prtn_camembert_iob2_ref\": \"Ptrn CmBERT IOB2\"}, errors=\"ignore\", inplace=True, axis=0)\n",
    "averaged_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811
    },
    "id": "02lAmQ9sn8Es",
    "outputId": "eb6500fa-8eee-4475-f084-4e7ca34ed39d"
   },
   "outputs": [],
   "source": [
    "# Filter and transpose to obtain the latex table\n",
    "latex_table_ref = averaged_ref.stack().unstack(level=[1,2])\n",
    "\n",
    "# Swap model name and metrics to get a nice table\n",
    "latex_table_ref = latex_table_ref.swaplevel(0,1).sort_index(level=0)\n",
    "\n",
    "caption = \"F1 score measured on the fine-tuned models CmBERT, CmBERT+ptrn on reference dataset with Independent Flat NER layers approach (M1).\"\n",
    "print(latex_table_ref.to_latex(float_format=\"%.1f\", multirow=True, caption=caption))\n",
    "latex_table_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 142 - Experiment 2: figures and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# PATHS\n",
    "METRICS_DIR_E2 = OUT_BASE / \"m1-120-experiment_2_metrics\"\n",
    "assert METRICS_DIR_E2\n",
    "METRICS_DIR_E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Camembert IO metrics from metrics jsons\n",
    "camembert_io_pero = compile_metrics(METRICS_DIR_E2 / \"121-camembert-ner-multihead-io\")\n",
    "camembert_iob2_pero = compile_metrics(METRICS_DIR_E2 / \"122-camembert-ner-multihead-iob2\")\n",
    "prtn_camembert_io_pero = compile_metrics(METRICS_DIR_E2 / \"123-pretrained-camembert-ner-multihead-io\")\n",
    "prtn_camembert_iob2_pero = compile_metrics(METRICS_DIR_E2 / \"124-pretrained-camembert-multihead-iob2\")\n",
    "metrics_raw_pero = pd.concat([camembert_io_pero,camembert_iob2_pero,prtn_camembert_io_pero,prtn_camembert_iob2_pero], keys=[\"CmBERT IO\", \"CmBERT IOB2\", \"Ptrn CmBERT IO\", \"Ptrn CmBERT IOB2\"])\n",
    "metrics_raw_pero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store p/r/f1 as percentages\n",
    "eval_ = [\"precision-l1\",\"recall-l1\",\"f1-l1\",\"precision-l2\",\"recall-l2\",\"f1-l2\",\"precision\",\"recall\",\"f1\",\"precision-jl\",\"recall_jl\",\"f1_jl\"]\n",
    "metrics_pero = metrics_raw_pero.copy()\n",
    "metrics_pero[eval_] = metrics_raw_pero[eval_].multiply(100., axis=1)\n",
    "metrics_pero.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNQ7QaOVBoPV"
   },
   "source": [
    "### Build the averaged table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average over runs\n",
    "averaged_pero = metrics_pero.groupby(level=0).apply(lambda grp: grp.groupby(by=\"trainsize\").mean())\n",
    "averaged_pero.set_index([\"trainsize_p\"], append=True,inplace=True)\n",
    "\n",
    "# Keep just the necessary columns\n",
    "averaged_pero=averaged_pero[[\"f1-l1\",\"f1-l2\",\"f1\",\"f1_jl\"]]\n",
    "\n",
    "# Set pretty names\n",
    "averaged_pero.index.names = ['Model','Trainset Size',\"%\"]\n",
    "averaged_pero.rename(columns={\n",
    "    \"f1-l1\":\"Level 1\",\n",
    "    \"f1-l2\":\"Level 2\",\n",
    "    \"f1\":\"Global\",\n",
    "    \"f1_jl\":\"P+L1+P+L2\"}, errors=\"raise\", inplace=True)\n",
    "averaged_pero.rename(mapper={\"camembert_io_ref\": \"CmBERT IO\",\"camembert_iob2_ref\": \"CmBERT IOB2\",\"prtn_camembert_io_ref\": \"Ptrn CmBERT IO\",\"prtn_camembert_iob2_ref\": \"Ptrn CmBERT IOB2\"}, errors=\"ignore\", inplace=True, axis=0)\n",
    "averaged_pero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtJTnFAyjLte"
   },
   "source": [
    "### Create the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and transpose to obtain the latex table\n",
    "latex_table_pero = averaged_pero.stack().unstack(level=[1,2])\n",
    "\n",
    "# Swap model name and metrics to get a nice table\n",
    "latex_table_pero = latex_table_pero.swaplevel(0,1).sort_index(level=0)\n",
    "\n",
    "caption = \"F1 score measured on the fine-tuned models CmBERT, CmBERT+ptrn on noisy dataset with Independent Flat NER layers approach (M1).\"\n",
    "print(latex_table_pero.to_latex(float_format=\"%.1f\", multirow=True, caption=caption))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 143 - Experiments 1 and 2 results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged = pd.concat([averaged_ref,averaged_pero],keys=[\"Reference\",\"Pero OCR\"])\n",
    "averaged = averaged.reset_index(level=[2,3], drop=True)\n",
    "averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = \"F1 score measured on the fine-tuned models CmBERTand CmBERT+ptrn on reference dataset and noisy dataset with Independent Flat NER layers approach (M1).\"\n",
    "print(averaged.to_latex(float_format=\"%.1f\", multirow=True, caption=caption))\n",
    "averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DAS22-experiment1-figures.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
