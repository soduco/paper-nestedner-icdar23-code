{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda9d76b",
   "metadata": {},
   "source": [
    "# 150 : Qualitative analysis using M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53ce29-cd5f-4295-ac93-2677c2d07034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ENV_IS_GOOGLE_COLAB = True if 'google.colab' in str(get_ipython()) else False\n",
    "os.environ[\"ENV_IS_GOOGLE_COLAB\"] = str(ENV_IS_GOOGLE_COLAB)\n",
    "\n",
    "if ENV_IS_GOOGLE_COLAB:\n",
    "  from google.colab import drive\n",
    "  mountpoint = Path(\"/content/drive\")\n",
    "  drive.mount(str(mountpoint)) # Mount gdrive to BASE\n",
    "  base = mountpoint / \"MyDrive/article_icdar_2023\" # Adapt this to your situation\n",
    "  sys.path.append(str(base)) # Add BASE to Python Path\n",
    "  BASE = Path(base).resolve() # Make BASE absolute\n",
    "  DATASETS =  BASE / \"dataset_ICDAR\"\n",
    "  OUT_BASE = BASE / \"res_ICDAR/method_1\"\n",
    "else:\n",
    "  BASE = Path().resolve() # Directory of this approach\n",
    "  #Adapt this to your situation\n",
    "  DATASETS = Path('../dataset_ICDAR').resolve() #Where your data are located befor Dataset object creation\n",
    "  OUT_BASE = Path('../res_ICDAR/method_1').resolve() #Where you save the results of this notebook\n",
    "\n",
    "print(sys.path)\n",
    "print(BASE)\n",
    "print(DATASETS)\n",
    "print(OUT_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2021c2a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9b68e",
   "metadata": {},
   "source": [
    "Choose a fine-tuned model to perform qualitative analysis. Load models from the HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['nlpso/m1_ind_layers_ref_cmbert_io',\n",
    " 'nlpso/m1_ind_layers_ref_cmbert_iob2',\n",
    " 'nlpso/m1_ind_layers_ref_ptrn_cmbert_io',\n",
    " 'nlpso/m1_ind_layers_ref_ptrn_cmbert_iob2',\n",
    " 'nlpso/m1_ind_layers_ocr_cmbert_io',\n",
    " 'nlpso/m1_ind_layers_ocr_cmbert_iob2',\n",
    " 'nlpso/m1_ind_layers_ocr_ptrn_cmbert_io',\n",
    " 'nlpso/m1_ind_layers_ocr_ptrn_cmbert_iob2',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79735318",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = widgets.RadioButtons(\n",
    "            options=models,\n",
    "            layout={'width': 'max-content'}\n",
    "        )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model you want to run\n",
    "MODEL_PATH_L1 = model.value + '_level_1'\n",
    "MODEL_PATH_L2 = model.value + '_level_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f72061",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = model.value\n",
    "FORMAT = 'IO'\n",
    "\n",
    "if 'ref' in model.value:\n",
    "    SET = \"ref\"\n",
    "elif 'ocr' in model.value:\n",
    "    SET = \"ocr\"\n",
    "\n",
    "if 'ptrn' in model.value:\n",
    "    MODEL_TYPE = 'pretrained_camembert_ner'\n",
    "else:\n",
    "    MODEL_TYPE = 'camembert_ner'\n",
    "    \n",
    "if 'm1_ind_layers_ref_cmbert_io' in MODEL:\n",
    "    DATASET = 'nlpso/m1_qualitative_analysis_ref_cmbert_io'\n",
    "elif 'm1_ind_layers_ref_cmbert_iob2' in MODEL:\n",
    "    DATASET = 'nlpso/m1_qualitative_analysis_ref_cmbert_iob2'\n",
    "elif 'm1_ind_layers_ref_ptrn_cmbert_io' in MODEL:\n",
    "    DATASET = 'nlpso/m1_qualitative_analysis_ref_ptrn_cmbert_io'\n",
    "elif 'm1_ind_layers_ref_ptrn_cmbert_iob2' in MODEL:\n",
    "    DATASET = 'nlpso/m1_qualitative_analysis_ref_ptrn_cmbert_iob2'\n",
    "elif 'm1_ind_layers_ocr_cmbert_io' in MODEL:\n",
    "    DATASET = 'nlpso/m1_qualitative_analysis_ocr_cmbert_io'\n",
    "elif 'm1_ind_layers_ocr_cmbert_iob2' in MODEL:\n",
    "    DATASET = 'nlpso/m1_qualitative_analysis_ocr_cmbert_iob2'\n",
    "elif 'm1_ind_layers_ocr_ptrn_cmbert_io' in MODEL:\n",
    "    DATASET = 'nlpso/m1_qualitative_analysis_ocr_ptrn_cmbert_io'\n",
    "elif 'm1_ind_layers_ocr_ptrn_cmbert_iob2' in MODEL:\n",
    "    DATASET = 'nlpso/m1_qualitative_analysis_ocr_ptrn_cmbert_iob2'\n",
    "\n",
    "print(f\"MODEL : {MODEL}\")\n",
    "print(f\"MODEL TYPE : {MODEL_TYPE}\")\n",
    "print(f\"FORMAT : {FORMAT}\")\n",
    "print(f\"SET : {SET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb597f",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299fc2c1",
   "metadata": {},
   "source": [
    "### Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa81965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "from datasets import load_dataset\n",
    "\n",
    "TRAINSETS_SIZES = [6084]\n",
    "train_dev_test = load_dataset(DATASET)\n",
    "test = train_dev_test[\"test\"]\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3741d14e",
   "metadata": {},
   "source": [
    "### Non-structured entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983f5a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = f\"{DATASETS}/qualitative_analysis/test_entries_{SET}.txt\"\n",
    "with open(PATH, 'r',encoding='utf8') as ex:\n",
    "    lines = ex.read()\n",
    "    lines = lines.split('\\n')\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba547f",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b00a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline\n",
    "\n",
    "LIMIT = 100\n",
    "\n",
    "#Level-1 layer\n",
    "tokenizer_l1 = AutoTokenizer.from_pretrained(MODEL_PATH_L1)\n",
    "model_l1 = AutoModelForTokenClassification.from_pretrained(MODEL_PATH_L1)\n",
    "nlp_l1 = TokenClassificationPipeline(model=model_l1, tokenizer=tokenizer_l1, aggregation_strategy=None, ignore_labels=[''])\n",
    "\n",
    "#Level-2 layer\n",
    "tokenizer_l2 = AutoTokenizer.from_pretrained(MODEL_PATH_L2)\n",
    "model_l2 = AutoModelForTokenClassification.from_pretrained(MODEL_PATH_L2)\n",
    "nlp_l2 = TokenClassificationPipeline(model=model_l2, tokenizer=tokenizer_l2, aggregation_strategy=None, ignore_labels=[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e356ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xmlize_util import get_NER_tags, xmlize_multilevel, group_entities\n",
    "\n",
    "k = 0\n",
    "stats = []\n",
    "errors_count = 0\n",
    "errors = []\n",
    "for i in range(len(lines)):\n",
    "    s = lines[i]\n",
    "    res = []\n",
    "    res.append(nlp_l1(s))\n",
    "    res.append(nlp_l2(s))\n",
    "    \n",
    "    #Create joint-labels at token scale\n",
    "    preds_tokens = []\n",
    "    preds_tags = []\n",
    "    for j in range(len(res[0])):\n",
    "        l1 = res[0]\n",
    "        l2 = res[1]\n",
    "        preds_tokens.append(l1[j]['word'])\n",
    "        if l1[j]['entity'] != 'O' and l2[j]['entity'] != 'O':\n",
    "            preds_tags.append('I-' + l1[j]['entity'][0].lower() + '_' + l1[j]['entity'][2:] + '+' + l2[j]['entity'][0].lower() + '_' + l2[j]['entity'][2:])\n",
    "        elif l1[j]['entity'] != 'O' and l2[j]['entity'] == 'O':\n",
    "            preds_tags.append('I-' + l1[j]['entity'][0].lower() + '_' + l1[j]['entity'][2:] + '+' + l2[j]['entity'])\n",
    "        elif l1[j]['entity'] == 'O' and l2[j]['entity'] != 'O':\n",
    "            preds_tags.append('I-' + l1[j]['entity'] + '+' + l2[j]['entity'][0].lower() + '_' + l2[j]['entity'][2:])\n",
    "        else:\n",
    "            preds_tags.append('O+O')\n",
    "            \n",
    "    test_tags = []\n",
    "    for h in range(len(test[i][\"tokens\"])):\n",
    "        if test[i][\"ner_tags_niv1\"][h] != 'O' and test[i][\"ner_tags_niv2\"][h] != 'O':\n",
    "            test_tags.append('I-' + test[i][\"ner_tags_niv1\"][h][0].lower() + '_' + test[i][\"ner_tags_niv1\"][h][2:] + '+' + test[i][\"ner_tags_niv2\"][h][0].lower() + '_' + test[i][\"ner_tags_niv2\"][h][2:])\n",
    "        elif test[i][\"ner_tags_niv1\"][h] != 'O' and test[i][\"ner_tags_niv2\"][h] == 'O':\n",
    "            test_tags.append('I-' + test[i][\"ner_tags_niv1\"][h][0].lower() + '_' + test[i][\"ner_tags_niv1\"][h][2:] + '+O')\n",
    "        else:\n",
    "            test_tags.append('O+O')\n",
    "    \n",
    "    #Create XML output\n",
    "    aggregate = [group_entities(res[0],tokenizer_l1),group_entities(res[1],tokenizer_l1)]\n",
    "    levels, num_levels = get_NER_tags(s,aggregate)\n",
    "    \n",
    "    if len(levels['niv_1']) > 0:\n",
    "        xml = xmlize_multilevel(levels,num_levels)\n",
    "        print(xml)\n",
    "        print('')\n",
    "    \n",
    "    try:\n",
    "        assert len(test[i][\"tokens\"]) == len(preds_tokens)\n",
    "        assert len(test_tags) == len(preds_tags)\n",
    "        \n",
    "        stats.append([k,s,xml,test[i][\"tokens\"],test_tags,preds_tokens,preds_tags])\n",
    "    except:\n",
    "        errors_count += 1\n",
    "        print(\"NUM \" + str(k))\n",
    "        print(s)\n",
    "        print(xml)\n",
    "        \n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd552df",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"index\",\"entry\",\"entry_xml\",\"spans_gold\",\"tags_gold\",\"spans_preds\",\"tags_preds\"]\n",
    "df = pd.DataFrame(stats,columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd9c68",
   "metadata": {},
   "source": [
    "## F1-Score ranking\n",
    "F1-Score is calculated for each entry using seqeval library :\n",
    "* entities are rebuild using joint-labels with seqeval lib\n",
    "* f1-score is calculted for each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f979bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "scores = []\n",
    "count = 0\n",
    "for i in range(len(df)):\n",
    "    y_preds = df.iloc[i][\"tags_preds\"]\n",
    "    y_true = df.iloc[i][\"tags_gold\"]\n",
    "    try:\n",
    "        f1 = f1_score([y_true],[y_preds])\n",
    "        scores.append(f1)\n",
    "    except:\n",
    "        count += 1\n",
    "        print(df.iloc[i])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1 = df.copy()\n",
    "df_f1[\"f1\"] = scores\n",
    "df_f1 = df_f1.sort_values(by=['f1']).reset_index()\n",
    "del df_f1[\"level_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75854eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b17082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "\n",
    "df_f1.hist(column='f1',bins=40,sharey=True, sharex=True)\n",
    "pl.suptitle('Entery-scale F1-Score distribution over test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6b85b",
   "metadata": {},
   "source": [
    "### 15-Top worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccccc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_f1[0:14])):\n",
    "    print(f\"INDEX {i}\")\n",
    "    print(df_f1.iloc[i][\"entry\"])\n",
    "    print(rdf.iloc[i][\"entry_xml\"])\n",
    "    print(f\"F1-Score : {df_f1.iloc[i]['f1']}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b631e",
   "metadata": {},
   "source": [
    "### 15-Top best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc515768",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = df_f1.sort_values(by='f1', ascending=False)\n",
    "for i in range(len(rdf[0:14])):\n",
    "    print(rdf.iloc[i][\"entry\"])\n",
    "    print(rdf.iloc[i][\"entry_xml\"])\n",
    "    print(f\"F1-Score : {rdf.iloc[i]['f1']}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1581a2",
   "metadata": {},
   "source": [
    "## Sub-word global analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_preds = []\n",
    "flat_labels = []\n",
    "for i in range(len(df)):\n",
    "    flat_preds += df[\"tags_preds\"][i]\n",
    "    flat_labels += df[\"tags_gold\"][i]\n",
    "flat_preds = pd.Series(flat_preds, name=\"Predictions\")\n",
    "flat_labels = pd.Series(flat_labels, name=\"Gold\")\n",
    "\n",
    "global_confusion = pd.crosstab(flat_labels, flat_preds,normalize='index').multiply(100., axis=1)\n",
    "\n",
    "col = []\n",
    "for c in global_confusion.columns:\n",
    "    c = c.replace('I-','')\n",
    "    c = c.replace('i_','')\n",
    "    c = c.replace('B-','')\n",
    "    c = c.replace('b_','')\n",
    "    col.append(c)\n",
    "global_confusion.columns = col\n",
    "\n",
    "ind = []\n",
    "for c in global_confusion.index:\n",
    "    c = c.replace('I-','')\n",
    "    c = c.replace('i_','')\n",
    "    c = c.replace('B-','')\n",
    "    c = c.replace('b_','')\n",
    "    ind.append(c)\n",
    "global_confusion.index = ind\n",
    "global_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605f65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Init figure\n",
    "plt.figure(figsize=(60, 16))\n",
    "\n",
    "#Create heatmap\n",
    "snsfig = sns.heatmap(\n",
    "    global_confusion, \n",
    "    annot = True, #Display labels\n",
    "    cmap=sns.color_palette(\"rocket_r\", as_cmap=True), #Color\n",
    "    fmt=\".1f\",\n",
    "    cbar=False,\n",
    "    annot_kws={\"fontsize\":40}\n",
    ")\n",
    "\n",
    "#Rename label axis and set their style\n",
    "plt.xlabel('Predictions',weight = 'bold',fontsize=35)\n",
    "plt.ylabel('Gold',weight = 'bold',fontsize=35)\n",
    "\n",
    "#Set x labels position to top\n",
    "snsfig.xaxis.tick_top()\n",
    "snsfig.xaxis.set_label_position('top')\n",
    "\n",
    "#Rotate y ticks horizontaly\n",
    "plt.yticks(rotation=0) \n",
    "\n",
    "#Change ticks size\n",
    "snsfig.set_xticklabels(snsfig.get_xmajorticklabels(), fontsize = 32)\n",
    "snsfig.set_yticklabels(snsfig.get_ymajorticklabels(), fontsize = 32)\n",
    "\n",
    "print(\"Confusion matrix of reference and predicted tokens types.\")\n",
    "print(\"Values are normalized by row (percentage of each reference classe and its resultants predictions)\")\n",
    "print(\"Last row represent percentage of each class in gold.\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Save figure\n",
    "fig = snsfig.get_figure()\n",
    "fig.tight_layout()\n",
    "#fig.savefig(f\"./tokenscaleanalysis-{FORMAT}-{SET}.pdf\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9342a",
   "metadata": {},
   "source": [
    "## Entry scale analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987574e1",
   "metadata": {},
   "source": [
    "Please choose one entry giving its index to perform an entry scale analysis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad6d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca834a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.iloc[i][\"entry\"])\n",
    "print(df.iloc[i][\"entry_xml\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8275b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "y_preds = df.iloc[i][\"tags_preds\"]\n",
    "y_true = df.iloc[i][\"tags_gold\"]\n",
    "print(classification_report([y_true],[y_preds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c619eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_true = pd.Series(df[\"tags_gold\"].iloc[i], name='Gold')\n",
    "y_pred = pd.Series(df[\"tags_preds\"].iloc[i], name='Predictions')\n",
    "entry_confusion = pd.crosstab(y_true, y_pred)\n",
    "\n",
    "col = []\n",
    "for c in entry_confusion.columns:\n",
    "    c = c.replace('I-','')\n",
    "    c = c.replace('i_','')\n",
    "    c = c.replace('B-','')\n",
    "    c = c.replace('b_','')\n",
    "    col.append(c)\n",
    "entry_confusion.columns = col\n",
    "\n",
    "ind = []\n",
    "for c in entry_confusion.index:\n",
    "    c = c.replace('I-','')\n",
    "    c = c.replace('i_','')\n",
    "    c = c.replace('B-','')\n",
    "    c = c.replace('b_','')\n",
    "    ind.append(c)\n",
    "entry_confusion.index = ind\n",
    "entry_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init figure\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "#Create heatmap\n",
    "snsfig = sns.heatmap(\n",
    "    entry_confusion, \n",
    "    annot = True, #Display labels\n",
    "    cmap=sns.color_palette(\"rocket_r\", as_cmap=True), #Color\n",
    "    fmt=\"g\",\n",
    "    cbar=False,\n",
    "    annot_kws={\"fontsize\":30}\n",
    ")\n",
    "\n",
    "#Rename label axis and set their style\n",
    "plt.xlabel('Predictions',weight = 'bold',fontsize=25) # x-axis label with fontsize 15\n",
    "plt.ylabel('Gold',weight = 'bold',fontsize=25) # y-axis label with fontsize 15\n",
    "\n",
    "#Set x labels position to top\n",
    "snsfig.xaxis.tick_top()\n",
    "snsfig.xaxis.set_label_position('top')\n",
    "\n",
    "#Rotate y ticks horizontaly\n",
    "plt.yticks(rotation=0) \n",
    "\n",
    "#Change ticks size\n",
    "snsfig.set_xticklabels(snsfig.get_xmajorticklabels(), fontsize = 20)\n",
    "snsfig.set_yticklabels(snsfig.get_ymajorticklabels(), fontsize = 20)\n",
    "\n",
    "print(\"Confusion matrix of reference and predicted tokens types.\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Save figure\n",
    "fig = snsfig.get_figure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
