{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60788d09",
   "metadata": {},
   "source": [
    "# M1 - 130 - Compute metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c59565",
   "metadata": {},
   "source": [
    "Using by levels scores computed during the fine-tuning :\n",
    "* compute evaluation on joint-labels\n",
    "* compute evaluation on all entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1732580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #Num√©ro GPU\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e9b2add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.6\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "30acc9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/lrde/home2/stual/stage_DAS/m1_independant_ner_layers', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages', '/lrde/home2/stual/.cache/huggingface/modules']\n",
      "/lrde/home2/stual/stage_DAS/m1_independant_ner_layers\n",
      "/work/stual/dataset_ICDAR\n",
      "/work/stual/res_ICDAR/method_1\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ENV_IS_GOOGLE_COLAB = True if 'google.colab' in str(get_ipython()) else False\n",
    "os.environ[\"ENV_IS_GOOGLE_COLAB\"] = str(ENV_IS_GOOGLE_COLAB)\n",
    "\n",
    "if ENV_IS_GOOGLE_COLAB:\n",
    "  from google.colab import drive\n",
    "  mountpoint = Path(\"/content/drive\")\n",
    "  drive.mount(str(mountpoint)) # Mount gdrive to BASE\n",
    "  base = mountpoint / \"MyDrive/article_icdar_2023\" # Adapt this to your situation\n",
    "  sys.path.append(str(base)) # Add BASE to Python Path\n",
    "  BASE = Path(base).resolve() # Make BASE absolute\n",
    "  DATASETS =  BASE / \"dataset\"\n",
    "else:\n",
    "  BASE = Path(os.path.dirname(os.path.realpath(\"__file__\"))).resolve() # If not on GColab, BASE will be the directory of this notebook\n",
    "  DATASETS = Path('/work/stual/dataset_ICDAR')\n",
    "  OUT_BASE = Path('/work/stual/res_ICDAR/method_1')\n",
    "\n",
    "print(sys.path)\n",
    "print(BASE)\n",
    "print(DATASETS)\n",
    "print(OUT_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "46960c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/work/stual/res_ICDAR/method_1/m1-110-experiment_1_metrics/111-camembert-ner-multihead-io',\n",
       " '/work/stual/res_ICDAR/method_1/m1-110-experiment_1_metrics/112-camembert-ner-multihead-iob2',\n",
       " '/work/stual/res_ICDAR/method_1/m1-110-experiment_1_metrics/113-pretrained-camembert-ner-multihead-io',\n",
       " '/work/stual/res_ICDAR/method_1/m1-110-experiment_1_metrics/114-pretrained-camembert-multihead-iob2',\n",
       " '/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/121-camembert-ner-multihead-io',\n",
       " '/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/122-camembert-ner-multihead-iob2',\n",
       " '/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/123-pretrained-camembert-ner-multihead-io',\n",
       " '/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/124-pretrained-camembert-multihead-iob2']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "ls = sorted(glob.glob(f\"{OUT_BASE}/*_metrics/*\"))\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadb954",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82b54cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/124-pretrained-camembert-multihead-iob2')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "\n",
    "# Expected datasets indexed by number of examples in the trainset\n",
    "#TRAINSETS_SIZES = [47,95,190,380,760,1521,3042,6084] #To train on the 7 datasets\n",
    "TRAINSETS_SIZES = [6084] #To train only on the biggest dataset\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "FORMAT = \"IOB2\"\n",
    "METRICS_OUTPUT_DIR = OUT_BASE / \"m1-120-experiment_2_metrics/124-pretrained-camembert-multihead-iob2\"\n",
    "METRICS_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09adbb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/124-pretrained-camembert-multihead-iob2/run_1/level-1/test_6084.json',\n",
       "  '/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/124-pretrained-camembert-multihead-iob2/run_2/level-1/test_6084.json',\n",
       "  '/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/124-pretrained-camembert-multihead-iob2/run_3/level-1/test_6084.json',\n",
       "  '/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/124-pretrained-camembert-multihead-iob2/run_4/level-1/test_6084.json',\n",
       "  '/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/124-pretrained-camembert-multihead-iob2/run_5/level-1/test_6084.json'],\n",
       " ['/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/124-pretrained-camembert-multihead-iob2/run_1/level-2/test_6084.json',\n",
       "  '/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/124-pretrained-camembert-multihead-iob2/run_2/level-2/test_6084.json',\n",
       "  '/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/124-pretrained-camembert-multihead-iob2/run_3/level-2/test_6084.json',\n",
       "  '/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/124-pretrained-camembert-multihead-iob2/run_4/level-2/test_6084.json',\n",
       "  '/work/stual/res_ICDAR/method_1/m1-120-experiment_2_metrics/124-pretrained-camembert-multihead-iob2/run_5/level-2/test_6084.json'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "RUNS = sorted(glob.glob(f\"{METRICS_OUTPUT_DIR}/run_*\"))\n",
    "L1_test = []\n",
    "L2_test = []\n",
    "for r in RUNS:\n",
    "    l1 = glob.glob(f\"{r}\" + \"/level-1/test_*\")\n",
    "    L1_test.append(l1[0])\n",
    "    l2 = glob.glob(f\"{r}\" + \"/level-2/test_*\")\n",
    "    L2_test.append(l2[0])\n",
    "L1_test,L2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f8b1e9",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "Align L1 and L2 predictions and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c65e59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanLists(res):\n",
    "    entries = []\n",
    "    for entry in res:\n",
    "        entry = entry.split(\", \")\n",
    "        ftag = []\n",
    "        for tag in entry:\n",
    "            tag = tag.replace(\"'\",'')\n",
    "            ftag.append(tag)\n",
    "        entries.append(ftag)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "86adf969",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {} #results[run][set][entree]   set : predictionsl1 labelsl1 predictionsl2 labelsl2\n",
    "\n",
    "#for each run\n",
    "for i in range(len(L1_test)):\n",
    "    # Opening JSON file\n",
    "    f1 = open(L1_test[i])\n",
    "    f2 = open(L2_test[i])\n",
    "    # returns JSON object as\n",
    "    # a dictionary\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "    \n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    \n",
    "    predictions1 = data1['eval_predictions'][2:-2]\n",
    "    predictions1 = predictions1.split('], [')\n",
    "    labels1 = data1['eval_labels'][2:-2]\n",
    "    labels1 = labels1.split('], [')\n",
    "    predictions2 = data2['eval_predictions'][2:-2]\n",
    "    predictions2 = predictions2.split('], [')\n",
    "    labels2 = data2['eval_labels'][2:-2]\n",
    "    labels2 = labels2.split('], [')\n",
    "    \n",
    "    fpredictions1 = cleanLists(predictions1)\n",
    "    fpredictions2 = cleanLists(predictions2)\n",
    "    flabels1 = cleanLists(labels1)\n",
    "    flabels2 = cleanLists(labels2)\n",
    "    \n",
    "    results[f\"run_{i+1}\"] = {\"gold_l1\":flabels1,\"gold_l2\":flabels2,\"predictions_l1\":fpredictions1,\"predictions_l2\":fpredictions2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3dd56547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['run_1', 'run_2', 'run_3', 'run_4', 'run_5'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42ff20df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['gold_l1', 'gold_l2', 'predictions_l1', 'predictions_l2'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"run_1\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd37ec",
   "metadata": {},
   "source": [
    "## Compute joint-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f4e5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createJointLabels(list1,list2):\n",
    "    entries = []\n",
    "    for i,(entryl1,entryl2) in enumerate(zip(list1,list2)):\n",
    "        assert len(entryl1) == len(entryl2)\n",
    "        new_tags = []\n",
    "        for j in range(len(entryl1)):\n",
    "            tag = entryl1[j] + '+' + entryl2[j]\n",
    "            tag = tag.replace('+I-','+i_')\n",
    "            tag = tag.replace('I-','I-i_')\n",
    "            tag = tag.replace('+B-','+b_')\n",
    "            tag = tag.replace('B-','I-b_')\n",
    "            new_tags.append(tag)\n",
    "        entries.append(new_tags)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e846c2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-b_PER+O', 'I-i_PER+O', 'I-i_PER+O', 'I-i_PER+O', 'I-i_PER+O', 'O+O', 'I-b_ACT+O', 'I-i_ACT+O', 'I-i_ACT+O', 'I-i_ACT+O', 'I-i_ACT+O', 'I-i_ACT+O', 'O+O', 'I-b_SPAT+b_LOC', 'I-i_SPAT+O', 'I-i_SPAT+b_CARDINAL', 'O+O']\n",
      "['I-b_PER+O', 'I-i_PER+O', 'I-i_PER+O', 'I-i_PER+O', 'I-i_PER+O', 'O+O', 'I-b_ACT+O', 'I-i_ACT+O', 'I-i_ACT+O', 'I-i_ACT+O', 'I-i_ACT+O', 'I-i_ACT+O', 'O+O', 'I-b_SPAT+b_LOC', 'I-i_SPAT+O', 'I-i_SPAT+b_CARDINAL', 'O+O']\n"
     ]
    }
   ],
   "source": [
    "i = 20\n",
    "jl_gold = createJointLabels(results[\"run_1\"][\"gold_l1\"],results[\"run_1\"][\"gold_l2\"])\n",
    "jl_preds = createJointLabels(results[\"run_1\"][\"predictions_l1\"],results[\"run_1\"][\"predictions_l2\"])\n",
    "print(jl_gold[i])\n",
    "print(jl_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad112ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createL1L2Labels(list1,list2):\n",
    "    entries = []\n",
    "    for i,(entryl1,entryl2) in enumerate(zip(list1,list2)):\n",
    "        assert len(entryl1) == len(entryl2)\n",
    "        new_tags = []\n",
    "        for j in range(len(entryl1)):\n",
    "            if 'O' in entryl2[j]:\n",
    "                prefixe_l2 = ''\n",
    "                tag_l2 = 'O'\n",
    "            else:\n",
    "                prefixe_l2, tag_l2 = entryl2[j].split('-')\n",
    "                \n",
    "            if 'O' in entryl1[j]:\n",
    "                prefixe_l1 = ''\n",
    "                tag_l1 = 'O'\n",
    "            else:\n",
    "                prefixe_l1, tag_l1 = entryl1[j].split('-')\n",
    "                \n",
    "            if prefixe_l2 == '' and prefixe_l1 != '':\n",
    "                new_tags.append(prefixe_l1 + '-' + tag_l2 + '+' + tag_l1)\n",
    "            elif prefixe_l2 == '' and prefixe_l1 == '':\n",
    "                new_tags.append('O+O')\n",
    "            else:\n",
    "                new_tags.append(prefixe_l2 + '-' + tag_l2 + '+' + tag_l1)\n",
    "                \n",
    "        entries.append(new_tags)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0742bb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-O+PER', 'I-O+PER', 'I-O+PER', 'I-O+PER', 'I-O+PER', 'O+O', 'B-O+ACT', 'I-O+ACT', 'I-O+ACT', 'I-O+ACT', 'I-O+ACT', 'I-O+ACT', 'O+O', 'B-O+SPAT', 'I-O+SPAT', 'B-CARDINAL+SPAT', 'O+O']\n",
      "['B-O+PER', 'I-O+PER', 'I-O+PER', 'I-O+PER', 'I-O+PER', 'O+O', 'B-O+ACT', 'I-O+ACT', 'I-O+ACT', 'I-O+ACT', 'I-O+ACT', 'I-O+ACT', 'O+O', 'B-O+SPAT', 'I-O+SPAT', 'B-CARDINAL+SPAT', 'O+O']\n"
     ]
    }
   ],
   "source": [
    "l1l2_gold = createL1L2Labels(results[\"run_1\"][\"gold_l1\"],results[\"run_1\"][\"gold_l2\"])\n",
    "l1l2_preds = createL1L2Labels(results[\"run_1\"][\"predictions_l1\"],results[\"run_1\"][\"predictions_l2\"])\n",
    "print(l1l2_gold[i])\n",
    "print(l1l2_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "abdbfdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createL1L2Labels(jlentries):\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    l1_p = []\n",
    "    l2_p = []\n",
    "    res = []\n",
    "    for tags in jlentries:\n",
    "        new_tags = []\n",
    "        #print(tags)\n",
    "        for elem in tags:\n",
    "            if elem[0] == 'I' and '+O' not in elem:\n",
    "                parts = elem.split('+')\n",
    "                e1 = parts[0][4:]\n",
    "                p1 = parts[0][2:3].upper()\n",
    "                e2 = parts[1][2:]\n",
    "                p2 = parts[1][:1].upper()\n",
    "                tag = p2 + '-' + e2 + '+' + e1\n",
    "            elif elem[0] == 'I' and elem[-1] == 'O':\n",
    "                parts = elem.split('+')\n",
    "                e1 = parts[0][4:]\n",
    "                p1 = parts[0][2:3].upper()\n",
    "                e2 = 'O'\n",
    "                p2 = parts[1][:1].upper()\n",
    "                tag = p1 + '-' + e2 + '+' + e1\n",
    "            elif elem[0] == 'O' and len(elem) > 3:\n",
    "                parts = elem.split('+')\n",
    "                e1 = 'O'\n",
    "                p1 = ''\n",
    "                e2 = parts[1][2:]\n",
    "                p2 = parts[1][:1].upper()\n",
    "                tag = p2 + '-' + e2 + '+' + e1\n",
    "                #print(e1,p1,e2,p2)\n",
    "            else:\n",
    "                e1 = 'O'\n",
    "                p1 = ''\n",
    "                e2 = 'O'\n",
    "                p2 = ''\n",
    "                tag = e2 + '+' + e1\n",
    "            new_tags.append(tag)\n",
    "        res.append(new_tags)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e09cf530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFlatLabels(jl_gold,jl_preds,FORMAT):\n",
    "    \n",
    "    LABELS_ID_TO_DAS = []\n",
    "    if FORMAT == 'IO':\n",
    "        LABELS_ID = {\n",
    "            \"O+O\" : 0,\n",
    "            \"I-i_PER+O\" : 1,\n",
    "            \"I-i_PER+i_TITREH\" : 2,\n",
    "            \"I-i_ACT+O\" : 3,\n",
    "            \"I-i_DESC+O\" : 4,\n",
    "            \"I-i_DESC+i_ACT\" : 5,\n",
    "            \"I-i_DESC+i_TITREP\" : 6,\n",
    "            \"I-i_SPAT+O\" : 7,\n",
    "            \"I-i_SPAT+i_LOC\" : 8,\n",
    "            \"I-i_SPAT+i_CARDINAL\" : 9,\n",
    "            \"I-i_SPAT+i_FT\" : 10,\n",
    "            \"I-i_TITRE+O\" : 11\n",
    "        }\n",
    "        LABELS_ID_TO_DAS =[\"O\",\"I-PER\",\"I-TITRE\",\"I-ACT\",\"O\",\"I-ACT\",\"I-TITRE\",\"O\",\"I-LOC\",\"I-CARDINAL\",\"I-FT\",\"I-TITRE\"]\n",
    "        \n",
    "    elif FORMAT == \"IOB2\":\n",
    "        LABELS_ID = {\n",
    "            \"O+O\" : 0,\n",
    "            \"I-b_PER+O\" : 1,\n",
    "            \"I-i_PER+O\" : 2,\n",
    "            \"I-b_PER+b_TITREH\" : 3,\n",
    "            \"I-i_PER+b_TITREH\" : 4,\n",
    "            \"I-i_PER+i_TITREH\" : 5,\n",
    "            \"I-b_ACT+O\" : 6,\n",
    "            \"I-i_ACT+O\" : 7,\n",
    "            \"I-b_DESC+O\" : 8,\n",
    "            \"I-i_DESC+O\" : 9,\n",
    "            \"I-b_DESC+b_ACT\" : 10,\n",
    "            \"I-i_DESC+b_ACT\" : 11,\n",
    "            \"I-i_DESC+i_ACT\" : 12,\n",
    "            \"I-b_DESC+b_TITREP\" : 13,\n",
    "            \"I-i_DESC+b_TITREP\" : 14,\n",
    "            \"I-i_DESC+i_TITREP\" : 15,\n",
    "            \"I-b_SPAT+O\" : 16,\n",
    "            \"I-i_SPAT+O\" : 17,\n",
    "            \"I-b_SPAT+b_LOC\" : 18,\n",
    "            \"I-i_SPAT+b_LOC\" : 19,\n",
    "            \"I-i_SPAT+i_LOC\" : 20,\n",
    "            \"I-b_SPAT+b_CARDINAL\" : 21,\n",
    "            \"I-i_SPAT+b_CARDINAL\" : 22,\n",
    "            \"I-i_SPAT+i_CARDINAL\" : 23,\n",
    "            \"I-b_SPAT+b_FT\" : 24,\n",
    "            \"I-i_SPAT+b_FT\" : 25,\n",
    "            \"I-i_SPAT+i_FT\" : 26,\n",
    "            \"I-b_TITRE+O\" : 27,\n",
    "            \"I-i_TITRE+O\" : 28\n",
    "        }\n",
    "        LABELS_ID_TO_DAS = ['O','I-PER','I-PER','I-TITRE','I-TITRE','I-TITRE','I-ACT','I-ACT','O','O','I-ACT','I-ACT','I-ACT','I-TITRE','I-TITRE','I-TITRE','O','O','I-LOC','I-LOC','I-LOC','I-CARDINAL','I-CARDINAL','I-CARDINAL','I-FT','I-FT','I-FT','I-TITRE','I-TITRE']\n",
    "    \n",
    "    das_labels = []\n",
    "    for entry in jl_gold:\n",
    "        das_entry = []\n",
    "        for elem in entry:\n",
    "            das_label = LABELS_ID_TO_DAS[LABELS_ID[elem]]\n",
    "            das_entry.append(das_label)\n",
    "        das_labels.append(das_entry)\n",
    "    \n",
    "    das_predictions = []\n",
    "    for entry in jl_preds:\n",
    "        das_entry = []\n",
    "        for elem in entry:\n",
    "            try:\n",
    "                das_label = LABELS_ID_TO_DAS[LABELS_ID[elem]]\n",
    "                das_entry.append(das_label)\n",
    "            except:\n",
    "                das_label = 'I-NO'\n",
    "                das_entry.append(das_label)\n",
    "        das_predictions.append(das_entry)\n",
    "        \n",
    "    return das_labels, das_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cff23e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_TITREH seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_TITREH seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_CARDINAL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_CARDINAL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision-all': 0.9403165398091096, 'eval_recall-all': 0.9452271071168327, 'eval_f1-all': 0.9427654291078674, 'eval_accuracy-all': 0.9548206921751841, 'eval_precision-l1': 0.9297565179918121, 'eval_recall-l1': 0.9500220167327169, 'eval_f1-l1': 0.9397800283131874, 'eval_accuracy-l1': 0.9460771519548989, 'eval_precision-l2': 0.9537953795379538, 'eval_recall-l2': 0.9393282773564464, 'eval_f1-l2': 0.9465065502183406, 'eval_accuracy-l2': 0.9635642323954691, 'eval_precision': 0.9373561013046815, 'eval_recall': 0.9346661564951215, 'eval_f1': 0.9360091962831688, 'eval_accuracy': 0.9216996398183431, 'eval_precision-l1l2': 0.9340223530825622, 'eval_recall-l1l2': 0.9394415568717515, 'eval_f1-l1l2': 0.9367241171507774, 'eval_accuracy-l1l2': 0.9223782429399175, 'eval_precision-das': 0.9467835341994052, 'eval_recall-das': 0.9441236147963166, 'eval_f1-das': 0.9454517036573928, 'eval_accuracy-das': 0.933601294565955, 'eval_PER': {'precision': 0.947953216374269, 'recall': 0.9620178041543027, 'f1': 0.9549337260677467, 'number': 1685}, 'eval_ACT': {'precision': 0.9347408829174664, 'recall': 0.8903107861060329, 'f1': 0.9119850187265917, 'number': 1094}, 'eval_ACT_L1': {'precision': 0.9347408829174664, 'recall': 0.944713870029098, 'f1': 0.9397009165460685, 'number': 1031}, 'eval_ACT_L2': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 63}, 'eval_DESC': {'precision': 0.408, 'recall': 0.6455696202531646, 'f1': 0.5, 'number': 79}, 'eval_TITREH': {'precision': 0.825, 'recall': 0.7674418604651163, 'f1': 0.7951807228915662, 'number': 43}, 'eval_TITREP': {'precision': 0.4482758620689655, 'recall': 0.3939393939393939, 'f1': 0.41935483870967744, 'number': 33}, 'eval_SPAT': {'precision': 0.9461451247165533, 'recall': 0.9553520320549513, 'f1': 0.9507262888066078, 'number': 1747}, 'eval_LOC': {'precision': 0.9379461834157057, 'recall': 0.9552572706935123, 'f1': 0.946522582432807, 'number': 1788}, 'eval_CARDINAL': {'precision': 0.9861431870669746, 'recall': 0.9754426042261565, 'f1': 0.98076370944588, 'number': 1751}, 'eval_FT': {'precision': 0.42857142857142855, 'recall': 0.42857142857142855, 'f1': 0.42857142857142855, 'number': 14}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_TITREH seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_TITREH seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_TITREP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_CARDINAL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision-all': 0.9394048909769908, 'eval_recall-all': 0.9470488219577362, 'eval_f1-all': 0.9432113698215906, 'eval_accuracy-all': 0.9609542203894138, 'eval_precision-l1': 0.9348817017581941, 'eval_recall-l1': 0.9482606781153676, 'eval_f1-l1': 0.941523663788392, 'eval_accuracy-l1': 0.9567260009396044, 'eval_precision-l2': 0.9450460205739036, 'eval_recall-l2': 0.945557963163597, 'eval_f1-l2': 0.9453019225561873, 'eval_accuracy-l2': 0.9651824398392232, 'eval_precision': 0.938645038167939, 'eval_recall': 0.9409795293667496, 'eval_f1': 0.9398108340498711, 'eval_accuracy': 0.9355327034504358, 'eval_precision-l1l2': 0.9320214669051878, 'eval_recall-l1l2': 0.9446391877190862, 'eval_f1-l1l2': 0.9382879097130506, 'eval_accuracy-l1l2': 0.9361591063318891, 'eval_precision-das': 0.9467835341994052, 'eval_recall-das': 0.9441236147963166, 'eval_f1-das': 0.9454517036573928, 'eval_accuracy-das': 0.933601294565955, 'eval_PER': {'precision': 0.9580873671782763, 'recall': 0.9632047477744807, 'f1': 0.9606392423794022, 'number': 1685}, 'eval_ACT': {'precision': 0.9054680259499537, 'recall': 0.8930530164533821, 'f1': 0.8992176714219973, 'number': 1094}, 'eval_ACT_L1': {'precision': 0.9395121951219512, 'recall': 0.9340446168768186, 'f1': 0.9367704280155642, 'number': 1031}, 'eval_ACT_L2': {'precision': 0.25925925925925924, 'recall': 0.2222222222222222, 'f1': 0.2393162393162393, 'number': 63}, 'eval_DESC': {'precision': 0.424, 'recall': 0.6708860759493671, 'f1': 0.5196078431372549, 'number': 79}, 'eval_TITREH': {'precision': 0.7333333333333333, 'recall': 0.7674418604651163, 'f1': 0.7499999999999999, 'number': 43}, 'eval_TITREP': {'precision': 0.48, 'recall': 0.36363636363636365, 'f1': 0.41379310344827586, 'number': 33}, 'eval_SPAT': {'precision': 0.9461145774248441, 'recall': 0.954779622209502, 'f1': 0.9504273504273505, 'number': 1747}, 'eval_LOC': {'precision': 0.9412410763316859, 'recall': 0.9586129753914989, 'f1': 0.9498476032141867, 'number': 1788}, 'eval_CARDINAL': {'precision': 0.9867511520737328, 'recall': 0.9782981153626499, 'f1': 0.9825064525379983, 'number': 1751}, 'eval_FT': {'precision': 0.38461538461538464, 'recall': 0.35714285714285715, 'f1': 0.3703703703703704, 'number': 14}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_CARDINAL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_ACT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_CARDINAL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_FT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision-all': 0.9291919552540759, 'eval_recall-all': 0.9482632985183386, 'eval_f1-all': 0.9386307627577086, 'eval_accuracy-all': 0.9536461867724592, 'eval_precision-l1': 0.9284791038345541, 'eval_recall-l1': 0.9489211800968737, 'eval_f1-l1': 0.938588850174216, 'eval_accuracy-l1': 0.9549511927754868, 'eval_precision-l2': 0.9300717894177081, 'eval_recall-l2': 0.9474539544962081, 'eval_f1-l2': 0.9386824097678788, 'eval_accuracy-l2': 0.9523411807694315, 'eval_precision': 0.9281595613952169, 'eval_recall': 0.93925770040176, 'eval_f1': 0.933675652545999, 'eval_accuracy': 0.9205512345356789, 'eval_precision-l1l2': 0.9176678445229682, 'eval_recall-l1l2': 0.9417381844554575, 'eval_f1-l1l2': 0.9295472170852472, 'eval_accuracy-l1l2': 0.9213864383776166, 'eval_precision-das': 0.9467835341994052, 'eval_recall-das': 0.9441236147963166, 'eval_f1-das': 0.9454517036573928, 'eval_accuracy-das': 0.933601294565955, 'eval_PER': {'precision': 0.9662921348314607, 'recall': 0.96973293768546, 'f1': 0.9680094786729858, 'number': 1685}, 'eval_ACT': {'precision': 0.8269230769230769, 'recall': 0.9040219378427788, 'f1': 0.8637554585152838, 'number': 1094}, 'eval_ACT_L1': {'precision': 0.9101229895931883, 'recall': 0.933074684772066, 'f1': 0.9214559386973181, 'number': 1031}, 'eval_ACT_L2': {'precision': 0.19424460431654678, 'recall': 0.42857142857142855, 'f1': 0.26732673267326734, 'number': 63}, 'eval_DESC': {'precision': 0.42016806722689076, 'recall': 0.6329113924050633, 'f1': 0.5050505050505051, 'number': 79}, 'eval_TITREH': {'precision': 0.8717948717948718, 'recall': 0.7906976744186046, 'f1': 0.8292682926829267, 'number': 43}, 'eval_TITREP': {'precision': 0.6666666666666666, 'recall': 0.6060606060606061, 'f1': 0.6349206349206349, 'number': 33}, 'eval_SPAT': {'precision': 0.9401129943502825, 'recall': 0.9524899828277046, 'f1': 0.946261017912994, 'number': 1747}, 'eval_LOC': {'precision': 0.9397790055248619, 'recall': 0.9513422818791947, 'f1': 0.9455252918287937, 'number': 1788}, 'eval_CARDINAL': {'precision': 0.9878752886836027, 'recall': 0.9771559109080525, 'f1': 0.9824863623313236, 'number': 1751}, 'eval_FT': {'precision': 0.45454545454545453, 'recall': 0.35714285714285715, 'f1': 0.4, 'number': 14}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_TITREH seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_TITREH seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_CARDINAL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_CARDINAL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision-all': 0.940152339499456, 'eval_recall-all': 0.944376973524411, 'eval_f1-all': 0.9422599212359891, 'eval_accuracy-all': 0.9570392023803309, 'eval_precision-l1': 0.9301017095866696, 'eval_recall-l1': 0.9462791721708499, 'eval_f1-l1': 0.9381207028265852, 'eval_accuracy-l1': 0.9508273738059195, 'eval_precision-l2': 0.9528767123287671, 'eval_recall-l2': 0.9420368364030336, 'eval_f1-l2': 0.9474257695450831, 'eval_accuracy-l2': 0.9632510309547424, 'eval_precision': 0.9405826362849726, 'eval_recall': 0.9358140424717811, 'eval_f1': 0.9381922800287702, 'eval_accuracy': 0.9258756590280315, 'eval_precision-l1l2': 0.9339259971167708, 'eval_recall-l1l2': 0.9396833071437205, 'eval_f1-l1l2': 0.936795806471049, 'eval_accuracy-l1l2': 0.9265020619094848, 'eval_precision-das': 0.9467835341994052, 'eval_recall-das': 0.9441236147963166, 'eval_f1-das': 0.9454517036573928, 'eval_accuracy-das': 0.933601294565955, 'eval_PER': {'precision': 0.9621972829297105, 'recall': 0.9667655786350149, 'f1': 0.9644760213143871, 'number': 1685}, 'eval_ACT': {'precision': 0.9122641509433962, 'recall': 0.8839122486288848, 'f1': 0.8978644382544103, 'number': 1094}, 'eval_ACT_L1': {'precision': 0.9168260038240917, 'recall': 0.930164888457808, 'f1': 0.9234472797303803, 'number': 1031}, 'eval_ACT_L2': {'precision': 0.5714285714285714, 'recall': 0.12698412698412698, 'f1': 0.20779220779220778, 'number': 63}, 'eval_DESC': {'precision': 0.3865546218487395, 'recall': 0.5822784810126582, 'f1': 0.4646464646464647, 'number': 79}, 'eval_TITREH': {'precision': 0.6730769230769231, 'recall': 0.813953488372093, 'f1': 0.736842105263158, 'number': 43}, 'eval_TITREP': {'precision': 0.07692307692307693, 'recall': 0.030303030303030304, 'f1': 0.043478260869565216, 'number': 33}, 'eval_SPAT': {'precision': 0.9438457175269427, 'recall': 0.9524899828277046, 'f1': 0.9481481481481481, 'number': 1747}, 'eval_LOC': {'precision': 0.945303867403315, 'recall': 0.9569351230425056, 'f1': 0.9510839355197331, 'number': 1788}, 'eval_CARDINAL': {'precision': 0.9834001144819691, 'recall': 0.9811536264991434, 'f1': 0.982275586049171, 'number': 1751}, 'eval_FT': {'precision': 0.35714285714285715, 'recall': 0.35714285714285715, 'f1': 0.35714285714285715, 'number': 14}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_TITREH seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_TITREH seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_LOC seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_FT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_TITREP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_ACT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+i_CARDINAL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+b_CARDINAL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: O+O seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/lrde/home2/stual/.venv/python_3_9/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_precision-all': 0.9294412607449857, 'eval_recall-all': 0.9454700024289531, 'eval_f1-all': 0.9373871161950632, 'eval_accuracy-all': 0.9566216004593621, 'eval_precision-l1': 0.9173819742489271, 'eval_recall-l1': 0.9412153236459709, 'eval_f1-l1': 0.9291458378613344, 'eval_accuracy-l1': 0.9462337526752623, 'eval_precision-l2': 0.9445640473627557, 'eval_recall-l2': 0.9507042253521126, 'eval_f1-l2': 0.9476241900647948, 'eval_accuracy-l2': 0.9670094482434619, 'eval_precision': 0.9356251785544234, 'eval_recall': 0.93983164339009, 'eval_f1': 0.9377236936292054, 'eval_accuracy': 0.9278592681526335, 'eval_precision-l1l2': 0.922559320033054, 'eval_recall-l1l2': 0.9446391877190862, 'eval_f1-l1l2': 0.9334687052078356, 'eval_accuracy-l1l2': 0.9290076734352978, 'eval_precision-das': 0.9467835341994052, 'eval_recall-das': 0.9441236147963166, 'eval_f1-das': 0.9454517036573928, 'eval_accuracy-das': 0.933601294565955, 'eval_PER': {'precision': 0.9471520845566647, 'recall': 0.9572700296735905, 'f1': 0.9521841794569067, 'number': 1685}, 'eval_ACT': {'precision': 0.8474721508140531, 'recall': 0.9040219378427788, 'f1': 0.8748341441839894, 'number': 1094}, 'eval_ACT_L1': {'precision': 0.8793103448275862, 'recall': 0.9398642095053347, 'f1': 0.9085794655414909, 'number': 1031}, 'eval_ACT_L2': {'precision': 0.3076923076923077, 'recall': 0.31746031746031744, 'f1': 0.3125, 'number': 63}, 'eval_DESC': {'precision': 0.38144329896907214, 'recall': 0.46835443037974683, 'f1': 0.42045454545454536, 'number': 79}, 'eval_TITREH': {'precision': 0.9444444444444444, 'recall': 0.7906976744186046, 'f1': 0.8607594936708861, 'number': 43}, 'eval_TITREP': {'precision': 0.6333333333333333, 'recall': 0.5757575757575758, 'f1': 0.6031746031746033, 'number': 33}, 'eval_SPAT': {'precision': 0.9425156516789983, 'recall': 0.9479107040641099, 'f1': 0.9452054794520548, 'number': 1747}, 'eval_LOC': {'precision': 0.9411117226197028, 'recall': 0.9563758389261745, 'f1': 0.9486823855755895, 'number': 1788}, 'eval_CARDINAL': {'precision': 0.9856815578465064, 'recall': 0.9828669331810395, 'f1': 0.9842722333428653, 'number': 1751}, 'eval_FT': {'precision': 0.2727272727272727, 'recall': 0.42857142857142855, 'f1': 0.33333333333333326, 'number': 14}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "from multihead_utils.tools import unique\n",
    "\n",
    "for i in range(0,len(L1_test)):\n",
    "    r = i+1\n",
    "    ################ L1\n",
    "    metric_l1 = load_metric(\"seqeval\")\n",
    "    results_l1 = metric_l1.compute(predictions=results[f\"run_{r}\"][\"predictions_l1\"], references=results[f\"run_{r}\"][\"gold_l1\"])\n",
    "    \n",
    "    ################ L2\n",
    "    metric_l2 = load_metric(\"seqeval\")\n",
    "    results_l2 = metric_l2.compute(predictions=results[f\"run_{r}\"][\"predictions_l2\"], references=results[f\"run_{r}\"][\"gold_l2\"])\n",
    "    \n",
    "    ################ Joint-labels metrics (including prefixes)\n",
    "    jl_predictions = createJointLabels(results[f\"run_{r}\"][\"predictions_l1\"],results[f\"run_{r}\"][\"predictions_l2\"]) #Gold\n",
    "    jl_labels = createJointLabels(results[f\"run_{r}\"][\"gold_l1\"],results[f\"run_{r}\"][\"gold_l2\"]) #Predictions\n",
    "    \n",
    "    metric_jl = load_metric(\"seqeval\")\n",
    "    results_jl = metric_jl.compute(predictions=jl_predictions, references=jl_labels)\n",
    "    \n",
    "    #Save the joint-labels\n",
    "    #jointlabelslist = unique(jl_predictions)\n",
    "    \n",
    "    ################ L1+l2\n",
    "    if FORMAT == 'IOB2':\n",
    "        l1l2_predictions = createL1L2Labels(jl_predictions)\n",
    "        l1l2_labels = createL1L2Labels(jl_labels)\n",
    "    else:\n",
    "        l1l2_predictions = jl_predictions\n",
    "        l1l2_labels = jl_labels\n",
    "    \n",
    "    metric_l1l2 = load_metric(\"seqeval\")\n",
    "    results_l1l2 = metric_l1l2.compute(predictions=l1l2_predictions, references=l1l2_labels)\n",
    "    \n",
    "    ################ DAS\n",
    "    das_labels, das_predictions = createFlatLabels(jl_gold,jl_preds,FORMAT)\n",
    "    \n",
    "    metric_das = load_metric(\"seqeval\")\n",
    "    results_das = metric_das.compute(predictions=das_predictions, references=das_labels)\n",
    "    \n",
    "    ################# Global metrics\n",
    "    all_preds = results[f\"run_{r}\"][\"predictions_l1\"] + results[f\"run_{r}\"][\"predictions_l2\"]\n",
    "    all_labels = results[f\"run_{r}\"][\"gold_l1\"] + results[f\"run_{r}\"][\"gold_l2\"]\n",
    "\n",
    "    metric_all = load_metric(\"seqeval\")\n",
    "    results_all = metric_all.compute(predictions=all_preds, references=all_labels)\n",
    "    \n",
    "    scores = {\n",
    "        #Global results (L1 and L2)\n",
    "        \"eval_precision-all\": results_all[\"overall_precision\"],\n",
    "        \"eval_recall-all\": results_all[\"overall_recall\"],\n",
    "        \"eval_f1-all\": results_all[\"overall_f1\"],\n",
    "        \"eval_accuracy-all\": results_all[\"overall_accuracy\"],\n",
    "        #L1\n",
    "        \"eval_precision-l1\": results_l1[\"overall_precision\"],\n",
    "        \"eval_recall-l1\": results_l1[\"overall_recall\"],\n",
    "        \"eval_f1-l1\": results_l1[\"overall_f1\"],\n",
    "        \"eval_accuracy-l1\": results_l1[\"overall_accuracy\"],\n",
    "        #L2\n",
    "        \"eval_precision-l2\": results_l2[\"overall_precision\"],\n",
    "        \"eval_recall-l2\": results_l2[\"overall_recall\"],\n",
    "        \"eval_f1-l2\": results_l2[\"overall_f1\"],\n",
    "        \"eval_accuracy-l2\": results_l2[\"overall_accuracy\"],\n",
    "        #Joint-labels (includign prefixes)\n",
    "        \"eval_precision\": results_jl[\"overall_precision\"],\n",
    "        \"eval_recall\": results_jl[\"overall_recall\"],\n",
    "        \"eval_f1\": results_jl[\"overall_f1\"],\n",
    "        \"eval_accuracy\": results_jl[\"overall_accuracy\"],\n",
    "        #L1+L2\n",
    "        \"eval_precision-l1l2\": results_l1l2[\"overall_precision\"],\n",
    "        \"eval_recall-l1l2\": results_l1l2[\"overall_recall\"],\n",
    "        \"eval_f1-l1l2\": results_l1l2[\"overall_f1\"],\n",
    "        \"eval_accuracy-l1l2\": results_l1l2[\"overall_accuracy\"],\n",
    "        #DAS\n",
    "        \"eval_precision-das\": results_das[\"overall_precision\"],\n",
    "        \"eval_recall-das\": results_das[\"overall_recall\"],\n",
    "        \"eval_f1-das\": results_das[\"overall_f1\"],\n",
    "        \"eval_accuracy-das\": results_das[\"overall_accuracy\"],\n",
    "        #\"joint-labels-list\":f\"{jointlabelslist}\",\n",
    "        #By class\n",
    "        \"eval_PER\": results_all['PER'],\n",
    "        \"eval_ACT\": results_all['ACT'],\n",
    "        \"eval_ACT_L1\": results_l1['ACT'],\n",
    "        \"eval_ACT_L2\": results_l2['ACT'],\n",
    "        \"eval_DESC\": results_all['DESC'],\n",
    "        \"eval_TITREH\": results_all['TITREH'],\n",
    "        \"eval_TITREP\": results_all['TITREP'],\n",
    "        \"eval_SPAT\": results_all['SPAT'],\n",
    "        \"eval_LOC\": results_all['LOC'],\n",
    "        \"eval_CARDINAL\": results_all['CARDINAL'],\n",
    "        \"eval_FT\": results_all['FT']\n",
    "    }\n",
    "    \n",
    "    #if 'TITRE' in list(results_all.keys()):\n",
    "        #scores[\"TITRE\"] = f\"{results_all['eval_TITRE']}\"\n",
    "    print(scores)\n",
    "    if not os.path.isdir(f\"{METRICS_OUTPUT_DIR}/run_{r}\"):\n",
    "        os.mkdir(f\"{METRICS_OUTPUT_DIR}/run_{r}\")\n",
    "\n",
    "    with open(f\"{METRICS_OUTPUT_DIR}/run_{r}/test_{str(TRAINSETS_SIZES[0])}.json\", 'w') as fp:\n",
    "        json.dump(scores, fp, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914253ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
