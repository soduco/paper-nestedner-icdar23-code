{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294705ff-9e89-499f-a41f-9494362be5f9",
   "metadata": {
    "id": "2552858d-7386-4e9a-8b0e-c338b920f783"
   },
   "source": [
    "# 310 - Experiment #1 - Hierarchical NER with reference dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6536176a",
   "metadata": {},
   "source": [
    "Requirements : \n",
    "* Create datasets in `m2_joint-labelling_for_ner` : `M2_200-prepare_datasets_joint_labelling`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cZvwNIzqBwDs",
   "metadata": {
    "id": "cZvwNIzqBwDs",
    "tags": []
   },
   "source": [
    "## Initialisation\n",
    "Set the BASE path.\n",
    "If run on Google Colab, will also mout Google Drive to the moutpoint given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LWJVak2mB6bI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWJVak2mB6bI",
    "outputId": "dbb54104-560b-480c-d4b0-74a0787e2024"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ENV_IS_GOOGLE_COLAB = True if 'google.colab' in str(get_ipython()) else False\n",
    "os.environ[\"ENV_IS_GOOGLE_COLAB\"] = str(ENV_IS_GOOGLE_COLAB)\n",
    "\n",
    "if ENV_IS_GOOGLE_COLAB:\n",
    "  from google.colab import drive\n",
    "  mountpoint = Path(\"/content/drive\")\n",
    "  drive.mount(str(mountpoint)) # Mount gdrive to BASE\n",
    "  base = mountpoint / \"MyDrive/article_icdar_2023\" # Adapt this to your situation\n",
    "  sys.path.append(str(base)) # Add BASE to Python Path\n",
    "  BASE = Path(base).resolve() # Make BASE absolute\n",
    "  DATASETS =  BASE / \"dataset_ICDAR\"\n",
    "  OUT_BASE = BASE / \"res_ICDAR/method_3\"\n",
    "else:\n",
    "  BASE = Path().resolve() # Directory of this approach\n",
    "  #Adapt this to your situation\n",
    "  DATASETS = Path('../dataset_ICDAR').resolve() #Where your data are located befor Dataset object creation\n",
    "  DATA_BASE = Path('../res_ICDAR/method_2').resolve()\n",
    "  OUT_BASE = Path('../res_ICDAR/method_3').resolve() #Where you save the results of this notebook\n",
    "\n",
    "print(sys.path)\n",
    "print(BASE)\n",
    "print(DATASETS)\n",
    "print(OUT_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hxHdPTBlCCFO",
   "metadata": {
    "id": "hxHdPTBlCCFO"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d554600",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CAMEMBERT_IO = True\n",
    "RUN_CAMEMBERT_IOB2 = False\n",
    "#Can't run together because of convert_tokenizer_\n",
    "RUN_PTRN_CAMEMBERT_IO = False\n",
    "RUN_PTRN_CAMEMBERT_IOB2 = False\n",
    "\n",
    "# Number of times a model will be trained & evaluated on each a dataset\n",
    "N_RUNS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b18a5bc-1abb-450d-90d3-6a7e56f773ed",
   "metadata": {
    "id": "6b18a5bc-1abb-450d-90d3-6a7e56f773ed"
   },
   "source": [
    "## CamemBERT - Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91140aa5-b377-47c1-bd44-844cd9365ec3",
   "metadata": {
    "id": "91140aa5-b377-47c1-bd44-844cd9365ec3"
   },
   "outputs": [],
   "source": [
    "# COMMON CONSTANTS\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 100,\n",
    "    \"max_steps\": 5000,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"greater_is_better\":True,\n",
    "    \"metric_for_best_model\": \"f1\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defda8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "\n",
    "#Print examples from datasets\n",
    "def loadExample(INPUT_DIR,set_length:int,i:int,subset:str):\n",
    "    set_ = load_from_disk(INPUT_DIR / f\"huggingface_{set_length}\")\n",
    "    data = {\"tokens\": set_[subset][i][\"tokens\"],\n",
    "            \"labels\": set_[subset][i][\"ner_tags\"]}\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bacc1d2",
   "metadata": {},
   "source": [
    "## Hierarchical NER : Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f76a0e1",
   "metadata": {},
   "source": [
    "### Tree with IO Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc90872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchicalNER.trees import Ltree, Wtree\n",
    "Ltree.pretty_print(unicodelines=True, nodedist=2)\n",
    "print(Wtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c78841",
   "metadata": {},
   "source": [
    "### Tree with IOB2 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0689a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchicalNER.trees import Ltree_iob2, Wtree_iob2\n",
    "Ltree_iob2.pretty_print(unicodelines=True, nodedist=2)\n",
    "print(Wtree_iob2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e9c889",
   "metadata": {},
   "source": [
    "## 311 - Train & eval : IO Ref dataset with CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"camembert_ner\"\n",
    "MODEL = \"Jean-Baptiste/camembert-ner\"\n",
    "LABEL = \"io\"\n",
    "FOLDER = \"311-camembert-ner-hierarchical-loss-io\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882da225",
   "metadata": {},
   "source": [
    "### 311.1 Load IO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06332f9f-37d5-4a0c-9af6-1f2bda236789",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06332f9f-37d5-4a0c-9af6-1f2bda236789",
    "outputId": "5749eaf4-a3d1-40fd-b2d4-fd45a27eb16e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "\n",
    "# Expected datasets indexed by number of examples in the trainset\n",
    "#TRAINSETS_SIZES = [47,95,190,380,760,1521,3042,6084] #To train on the 7 datasets\n",
    "TRAINSETS_SIZES = [6084] #To train only on the biggest dataset\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "INPUT_DIR = DATA_BASE / f\"m2-experiment_1_prepared_dataset_ref_{LABEL}_{MODEL_NAME}\"\n",
    "METRICS_OUTPUT_DIR = OUT_BASE / \"m3-310-experiment_1_metrics\"\n",
    "INPUT_DIR, METRICS_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadExample(INPUT_DIR,TRAINSETS_SIZES[-1],10,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4aa5b",
   "metadata": {},
   "source": [
    "### 311.2 Fine-tuning with IO labels - train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e821087-3623-4c14-a8fb-63dcc98dc1d4",
   "metadata": {
    "id": "2e821087-3623-4c14-a8fb-63dcc98dc1d4"
   },
   "outputs": [],
   "source": [
    "from config import logger\n",
    "from datasets import load_from_disk\n",
    "import json\n",
    "from hierarchicalNER.util_IO import init_model, train_eval_loop\n",
    "\n",
    "def train_bert(metrics_output_directory):\n",
    "    # Train & evaluate loop\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        output_dir = metrics_output_directory / f\"run_{run}\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        #Fine-tuning on the biggest dataset\n",
    "        for trainset_size in TRAINSETS_SIZES:\n",
    "            datasetdir = INPUT_DIR / f\"huggingface_{trainset_size}\"\n",
    "            logger.info(f\"Running on datasets in {datasetdir}\")\n",
    "            logger.info(f\"Metrics will be saved in {output_dir}\")\n",
    "            \n",
    "            model, tokenizer, training_args = init_model(MODEL, local_config,run)\n",
    "            logger.info(f\"{model} #{run}, will save in {output_dir}\")\n",
    "            \n",
    "            train_dev_test = load_from_disk(datasetdir)\n",
    "            train = train_dev_test[\"train\"]\n",
    "            dev = train_dev_test[\"dev\"]\n",
    "            test = train_dev_test[\"test\"]\n",
    "            metrics = train_eval_loop(model,         # Implicit. Must be setbefore calling train_bert()\n",
    "                                      training_args, # Idem\n",
    "                                      tokenizer,\n",
    "                                      train,dev,test)\n",
    "\n",
    "            # Save the dev and test metrics\n",
    "            metrics_file = output_dir / f\"test_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[0], o)\n",
    "\n",
    "            metrics_file = output_dir / f\"dev_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[1], o)\n",
    "                \n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6131ba8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05387e39-dd69-491e-9517-57490356e5e9",
    "outputId": "a894e899-0646-4260-f12f-7468adfbb5b2"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from hierarchicalNER.util_IO import _convert_tokenizer\n",
    "\n",
    "if RUN_CAMEMBERT_IO:\n",
    "    assert _convert_tokenizer.name_or_path == MODEL\n",
    "    \n",
    "    # MODEL CONSTS\n",
    "    MODEL_METRICS_DIR = METRICS_OUTPUT_DIR / f\"{FOLDER}\"\n",
    "    MODEL_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    MODEL_OUTPUT_MODEL_PATH = OUT_BASE / f\"tmp/{FOLDER}\"\n",
    "    MODEL_METRICS_DIR, MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Set config output dir\n",
    "    local_config = TRAINING_CONFIG.copy() \n",
    "    local_config[\"output_dir\"]=MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Run the main loop\n",
    "    h = time.time()\n",
    "    train_bert(MODEL_METRICS_DIR)\n",
    "    runtime = (time.time()- h)/N_RUNS\n",
    "    print(f\"Run-time is equal to {str(datetime.timedelta(seconds=runtime))}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipped finetuning model for IO labels\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29431951",
   "metadata": {},
   "source": [
    "Best model : \n",
    "Run time : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a46aea",
   "metadata": {},
   "source": [
    "## 312 - Train & eval : IOB2 Ref dataset with CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"camembert_ner\"\n",
    "MODEL = \"Jean-Baptiste/camembert-ner\"\n",
    "LABEL = \"iob2\"\n",
    "FOLDER = \"312-camembert-ner-hierarchical-loss-iob2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17156e4a",
   "metadata": {},
   "source": [
    "### 312.1 Load IOB2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120918dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "\n",
    "# Expected datasets indexed by number of examples in the trainset\n",
    "#TRAINSETS_SIZES = [47,95,190,380,760,1521,3042,6084] #To train on the 7 datasets\n",
    "TRAINSETS_SIZES = [6084] #To train only on the biggest dataset\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "INPUT_DIR = DATA_BASE / f\"m2-experiment_1_prepared_dataset_ref_{LABEL}_{MODEL_NAME}\"\n",
    "METRICS_OUTPUT_DIR = OUT_BASE / \"m3-310-experiment_1_metrics\"\n",
    "INPUT_DIR, METRICS_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ae2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadExample(INPUT_DIR,TRAINSETS_SIZES[-1],10,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa6008",
   "metadata": {},
   "source": [
    "### 312.2 Fine-tuning with IOB2 labels - train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5465fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import logger\n",
    "from datasets import load_from_disk\n",
    "from hierarchicalNER.util_IOB2 import init_model, train_eval_loop, _convert_tokenizer\n",
    "import json\n",
    "\n",
    "def train_bert(metrics_output_directory):\n",
    "    # Train & evaluate loop\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        output_dir = metrics_output_directory / f\"run_{run}\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        #Fine-tuning on the biggest dataset\n",
    "        for trainset_size in TRAINSETS_SIZES:\n",
    "            datasetdir = INPUT_DIR / f\"huggingface_{trainset_size}\"\n",
    "            logger.info(f\"Running on datasets in {datasetdir}\")\n",
    "            logger.info(f\"Metrics will be saved in {output_dir}\")\n",
    "            \n",
    "            model, tokenizer, training_args = init_model(MODEL, local_config,run)\n",
    "            logger.info(f\"{model} #{run}, will save in {output_dir}\")\n",
    "            \n",
    "            train_dev_test = load_from_disk(datasetdir)\n",
    "            train = train_dev_test[\"train\"]\n",
    "            dev = train_dev_test[\"dev\"]\n",
    "            test = train_dev_test[\"test\"]\n",
    "            metrics = train_eval_loop(model,         # Implicit. Must be setbefore calling train_bert()\n",
    "                                      training_args, # Idem\n",
    "                                      tokenizer,\n",
    "                                      train,dev,test)\n",
    "\n",
    "            # Save the dev and test metrics\n",
    "            metrics_file = output_dir / f\"test_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[0], o)\n",
    "\n",
    "            metrics_file = output_dir / f\"dev_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[1], o)\n",
    "                \n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792bd540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "if RUN_CAMEMBERT_IOB2:\n",
    "    assert _convert_tokenizer.name_or_path == MODEL\n",
    "    \n",
    "    # MODEL CONSTS\n",
    "    MODEL_METRICS_DIR = METRICS_OUTPUT_DIR / f\"{FOLDER}\"\n",
    "    MODEL_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    MODEL_OUTPUT_MODEL_PATH = OUT_BASE / f\"tmp/{FOLDER}\"\n",
    "    MODEL_METRICS_DIR, MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Set config output dir\n",
    "    local_config = TRAINING_CONFIG.copy() \n",
    "    local_config[\"output_dir\"]=MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Run the main loop\n",
    "    h = time.time()\n",
    "    train_bert(MODEL_METRICS_DIR)\n",
    "    runtime = (time.time()- h)/N_RUNS\n",
    "    print(f\"Run-time is equal to {str(datetime.timedelta(seconds=runtime))}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipped finetuning model for IOB2 labels\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48370946",
   "metadata": {},
   "source": [
    "Best model : \n",
    "Run time :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47472b8",
   "metadata": {},
   "source": [
    "## 313 - Train & eval : IO Ref dataset with Pretrained CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"pretrained_camembert_ner\"\n",
    "MODEL = \"HueyNemud/das22-10-camembert_pretrained\"\n",
    "LABEL = \"io\"\n",
    "FOLDER = \"313-pretrained-camembert-ner-hierarchical-loss-io\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30025437",
   "metadata": {},
   "source": [
    "### 313.1 Load IO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b0fef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06332f9f-37d5-4a0c-9af6-1f2bda236789",
    "outputId": "5749eaf4-a3d1-40fd-b2d4-fd45a27eb16e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "\n",
    "# Expected datasets indexed by number of examples in the trainset\n",
    "#TRAINSETS_SIZES = [47,95,190,380,760,1521,3042,6084] #To train on the 7 datasets\n",
    "TRAINSETS_SIZES = [6084] #To train only on the biggest dataset\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "INPUT_DIR = DATA_BASE / f\"m2-experiment_1_prepared_dataset_ref_{LABEL}_{MODEL_NAME}\"\n",
    "METRICS_OUTPUT_DIR = OUT_BASE / \"m3-310-experiment_1_metrics\"\n",
    "INPUT_DIR, METRICS_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadExample(INPUT_DIR,TRAINSETS_SIZES[-1],10,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ce6d9",
   "metadata": {},
   "source": [
    "### 313.2 Fine-tuning with IO labels - train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558110b",
   "metadata": {
    "id": "2e821087-3623-4c14-a8fb-63dcc98dc1d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from config import logger\n",
    "from datasets import load_from_disk\n",
    "from hierarchicalNER.util_IO import init_model, train_eval_loop, _convert_tokenizer\n",
    "\n",
    "def train_bert(metrics_output_directory):\n",
    "    # Train & evaluate loop\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        output_dir = metrics_output_directory / f\"run_{run}\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        #Fine-tuning on the biggest dataset\n",
    "        for trainset_size in TRAINSETS_SIZES:\n",
    "            datasetdir = INPUT_DIR / f\"huggingface_{trainset_size}\"\n",
    "            logger.info(f\"Running on datasets in {datasetdir}\")\n",
    "            logger.info(f\"Metrics will be saved in {output_dir}\")\n",
    "            \n",
    "            model, tokenizer, training_args = init_model(MODEL, local_config,run)\n",
    "            logger.info(f\"{model} #{run}, will save in {output_dir}\")\n",
    "\n",
    "            train_dev_test = load_from_disk(datasetdir)\n",
    "            train = train_dev_test[\"train\"]\n",
    "            dev = train_dev_test[\"dev\"]\n",
    "            test = train_dev_test[\"test\"]\n",
    "            metrics = train_eval_loop(model,         # Implicit. Must be setbefore calling train_bert()\n",
    "                                      training_args, # Idem\n",
    "                                      tokenizer,\n",
    "                                      train,dev,test)\n",
    "\n",
    "            # Save the dev and test metrics\n",
    "            metrics_file = output_dir / f\"test_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[0], o)\n",
    "\n",
    "            metrics_file = output_dir / f\"dev_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[1], o)\n",
    "                \n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "if RUN_PTRN_CAMEMBERT_IO:\n",
    "    print(_convert_tokenizer.name_or_path)\n",
    "    assert _convert_tokenizer.name_or_path == MODEL\n",
    "        \n",
    "    # MODEL CONSTS\n",
    "    MODEL_METRICS_DIR = METRICS_OUTPUT_DIR / f\"{FOLDER}\"\n",
    "    MODEL_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    MODEL_OUTPUT_MODEL_PATH = OUT_BASE / f\"tmp/{FOLDER}\"\n",
    "    MODEL_METRICS_DIR, MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Set config output dir\n",
    "    local_config = TRAINING_CONFIG.copy() \n",
    "    local_config[\"output_dir\"]=MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Run the main loop\n",
    "    h = time.time()\n",
    "    train_bert(MODEL_METRICS_DIR)\n",
    "    runtime = (time.time()- h)/N_RUNS\n",
    "    print(f\"Run-time is equal to {str(datetime.timedelta(seconds=runtime))}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipped finetuning pretrained model for IO labels\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "252d09e4",
   "metadata": {},
   "source": [
    "Best model : \n",
    "Run time : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae315b",
   "metadata": {},
   "source": [
    "## 314 - Train & eval : IOB2 Ref dataset with Pretrained CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b01d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"pretrained_camembert_ner\"\n",
    "MODEL = \"HueyNemud/das22-10-camembert_pretrained\"\n",
    "LABEL = \"iob2\"\n",
    "FOLDER = \"314-pretrained-camembert-ner-hierarchical-loss-iob2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c019414f",
   "metadata": {},
   "source": [
    "### 314.1 Load IOB2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130daa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from config import logger\n",
    "\n",
    "# Expected datasets indexed by number of examples in the trainset\n",
    "#TRAINSETS_SIZES = [47,95,190,380,760,1521,3042,6084] #To train on the 7 datasets\n",
    "TRAINSETS_SIZES = [6084] #To train only on the biggest dataset\n",
    "\n",
    "# INPUT / OUTPUT DIRS\n",
    "INPUT_DIR = DATA_BASE / f\"m2-experiment_1_prepared_dataset_ref_{LABEL}_{MODEL_NAME}\"\n",
    "METRICS_OUTPUT_DIR = OUT_BASE / \"m3-310-experiment_1_metrics\"\n",
    "INPUT_DIR, METRICS_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadExample(INPUT_DIR,TRAINSETS_SIZES[-1],10,\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a30951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import logger\n",
    "from datasets import load_from_disk\n",
    "from hierarchicalNER.util_IOB2 import init_model, train_eval_loop, _convert_tokenizer\n",
    "import json\n",
    "\n",
    "def train_bert(metrics_output_directory):\n",
    "    # Train & evaluate loop\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        output_dir = metrics_output_directory / f\"run_{run}\"\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        #Fine-tuning on the biggest dataset\n",
    "        for trainset_size in TRAINSETS_SIZES:\n",
    "            datasetdir = INPUT_DIR / f\"huggingface_{trainset_size}\"\n",
    "            logger.info(f\"Running on datasets in {datasetdir}\")\n",
    "            logger.info(f\"Metrics will be saved in {output_dir}\")\n",
    "            \n",
    "            model, tokenizer, training_args = init_model(MODEL, local_config,run)\n",
    "            logger.info(f\"{model} #{run}, will save in {output_dir}\")\n",
    "\n",
    "            train_dev_test = load_from_disk(datasetdir)\n",
    "            \n",
    "            train = train_dev_test[\"train\"]\n",
    "            dev = train_dev_test[\"dev\"]\n",
    "            test = train_dev_test[\"test\"]\n",
    "            metrics = train_eval_loop(model,         # Implicit. Must be setbefore calling train_bert()\n",
    "                                      training_args, # Idem\n",
    "                                      tokenizer,\n",
    "                                      train,dev,test)\n",
    "\n",
    "            # Save the dev and test metrics\n",
    "            metrics_file = output_dir / f\"test_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[0], o)\n",
    "\n",
    "            metrics_file = output_dir / f\"dev_{trainset_size}.json\"\n",
    "            with open(metrics_file, \"w\", encoding=\"utf-8\") as o:\n",
    "                json.dump(metrics[1], o)\n",
    "            \n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9096720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "if RUN_PTRN_CAMEMBERT_IOB2:\n",
    "    print(_convert_tokenizer.name_or_path)\n",
    "    assert _convert_tokenizer.name_or_path == MODEL\n",
    "    \n",
    "    # MODEL CONSTS\n",
    "    MODEL_METRICS_DIR = METRICS_OUTPUT_DIR / f\"{FOLDER}\"\n",
    "    MODEL_METRICS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    MODEL_OUTPUT_MODEL_PATH = OUT_BASE / f\"tmp/{FOLDER}\"\n",
    "    MODEL_METRICS_DIR, MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Set config output dir\n",
    "    local_config = TRAINING_CONFIG.copy() \n",
    "    local_config[\"output_dir\"]=MODEL_OUTPUT_MODEL_PATH\n",
    "\n",
    "    # Run the main loop\n",
    "    h = time.time()\n",
    "    train_bert(MODEL_METRICS_DIR)\n",
    "    runtime = (time.time()- h)/N_RUNS\n",
    "    print(f\"Run-time is equal to {str(datetime.timedelta(seconds=runtime))}\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipped finetuning pretrained model for IOB2 labels\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "175ca00a",
   "metadata": {},
   "source": [
    "Best model :\n",
    "Run time : "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "20-experiment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
