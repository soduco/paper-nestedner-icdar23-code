{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2 : 230 - Figures and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation scores on level-1 entities segmentation and classification with joint-labels method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6Oayttuiff2"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ENV_IS_GOOGLE_COLAB = True if 'google.colab' in str(get_ipython()) else False\n",
    "os.environ[\"ENV_IS_GOOGLE_COLAB\"] = str(ENV_IS_GOOGLE_COLAB)\n",
    "\n",
    "if ENV_IS_GOOGLE_COLAB:\n",
    "  from google.colab import drive\n",
    "  mountpoint = Path(\"/content/drive\")\n",
    "  drive.mount(str(mountpoint)) # Mount gdrive to BASE\n",
    "  base = mountpoint / \"MyDrive/article_icdar_2023\" # Adapt this to your situation\n",
    "  sys.path.append(str(base)) # Add BASE to Python Path\n",
    "  BASE = Path(base).resolve() # Make BASE absolute\n",
    "  DATASETS =  BASE / \"dataset_ICDAR\"\n",
    "  OUT_BASE = BASE / \"res_ICDAR/method_2\"\n",
    "else:\n",
    "  BASE = Path().resolve() # Directory of this approach\n",
    "  #Adapt this to your situation\n",
    "  DATASETS = Path('../dataset_ICDAR').resolve() #Where your data are located befor Dataset object creation\n",
    "  OUT_BASE = Path('../res_ICDAR/method_2').resolve() #Where you save the results of this notebook\n",
    "\n",
    "print(sys.path)\n",
    "print(BASE)\n",
    "print(DATASETS)\n",
    "print(OUT_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TRAINSET_SIZE = 6084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6Oayttuiff2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "def compile_metrics(path): \n",
    "    rundirs = [f for f in os.listdir(path)]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for run_dir in rundirs:\n",
    "        if 'run' in run_dir:\n",
    "            run_path = path / run_dir\n",
    "            nrun = re.search(\"\\d+\",run_dir)[0]\n",
    "\n",
    "            files = [f for f in os.listdir(run_path) if \"test_\" in f and 'json' in f]\n",
    "            sizes = [int(re.search(\"\\d+\",f)[0]) for f in files]\n",
    "\n",
    "            for file, size in zip(files,sizes):\n",
    "                file_path = run_path / file\n",
    "                dftmp = pd.read_json(file_path, typ='series')\n",
    "                dftmp = pd.DataFrame([dftmp])\n",
    "\n",
    "                dftmp[\"trainsize\"] = size \n",
    "                dftmp[\"run\"] = nrun\n",
    "                dftmp[\"trainsize_p\"] = round(100 * size / MAX_TRAINSET_SIZE, 1)\n",
    "                df = pd.concat([df, dftmp])\n",
    "\n",
    "    return df.groupby([\"run\",\"trainsize\"]).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6Oayttuiff2"
   },
   "source": [
    "# 231 - Experiment 1: tables on all-entities metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR_REF = OUT_BASE / \"method_2/m2-210-experiment_1_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6Oayttuiff2"
   },
   "outputs": [],
   "source": [
    "# Load models metrics from metrics jsons\n",
    "res = []\n",
    "keys = []\n",
    "\n",
    "if os.path.exists(METRICS_DIR_REF / \"211-camembert-ner-joint-labelling-io/run_2\"):\n",
    "    camembert_ner_io_ref = compile_metrics(METRICS_DIR_REF / \"211-camembert-ner-joint-labelling-io\")\n",
    "    camembert_ner_io_ref[\"eval_precision-l1l2\"] = camembert_ner_io_ref[\"eval_precision\"]\n",
    "    camembert_ner_io_ref[\"eval_recall-l1l2\"] = camembert_ner_io_ref[\"eval_recall\"]\n",
    "    camembert_ner_io_ref[\"eval_f1-l1l2\"] = camembert_ner_io_ref[\"eval_f1\"]\n",
    "    camembert_ner_io_ref[\"eval_accuracy-l1l2\"] = camembert_ner_io_ref[\"eval_accuracy\"]\n",
    "    res.append(camembert_ner_io_ref)\n",
    "    keys.append(\"CmBERT IO\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_REF / \"212-camembert-ner-joint-labelling-iob2/run_2\"):\n",
    "    camembert_ner_iob2_ref = compile_metrics(METRICS_DIR_REF / \"212-camembert-ner-joint-labelling-iob2\")\n",
    "    res.append(camembert_ner_iob2_ref)\n",
    "    keys.append(\"CmBERT IOB2\")\n",
    "\n",
    "if os.path.exists(METRICS_DIR_REF / \"213-pretrained-camembert-ner-joint-labelling-io/run_2\"):\n",
    "    ptrn_camembert_ner_io_ref = compile_metrics(METRICS_DIR_REF / \"213-pretrained-camembert-ner-joint-labelling-io\")\n",
    "    ptrn_camembert_ner_io_ref[\"eval_precision-l1l2\"] = ptrn_camembert_ner_io_ref[\"eval_precision\"]\n",
    "    ptrn_camembert_ner_io_ref[\"eval_recall-l1l2\"] = ptrn_camembert_ner_io_ref[\"eval_recall\"]\n",
    "    ptrn_camembert_ner_io_ref[\"eval_f1-l1l2\"] = ptrn_camembert_ner_io_ref[\"eval_f1\"]\n",
    "    ptrn_camembert_ner_io_ref[\"eval_accuracy-l1l2\"] = ptrn_camembert_ner_io_ref[\"eval_accuracy\"]\n",
    "    res.append(ptrn_camembert_ner_io_ref)\n",
    "    keys.append(\"Ptrn CmBERT IO\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_REF / \"214-pretrained-camembert-ner-joint-labelling-iob2/run_2\"):\n",
    "    ptrn_camembert_ner_iob2_ref = compile_metrics(METRICS_DIR_REF / \"214-pretrained-camembert-ner-joint-labelling-iob2\")\n",
    "    res.append(ptrn_camembert_ner_iob2_ref)\n",
    "    keys.append(\"Ptrn CmBERT IOB2\")\n",
    "    \n",
    "# Create the full table\n",
    "print(keys)\n",
    "metrics_raw_ref = pd.concat(res, keys=keys)\n",
    "metrics_raw_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_raw_ref.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNQ7QaOVBoPV"
   },
   "source": [
    "## 231.1 Build the averaged table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U15qGMLuBu8g"
   },
   "outputs": [],
   "source": [
    "# Store p/r/f1 as percentages\n",
    "eval_ = [\"eval_f1\",\"eval_f1-l1\",\"eval_f1-all\",\"eval_f1-l2\",'eval_f1-l1l2','eval_f1-das']\n",
    "metrics_ref = metrics_raw_ref.copy()\n",
    "metrics_ref[eval_] = metrics_raw_ref[eval_].multiply(100., axis=1)\n",
    "metrics_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEwY8jAYB8yd"
   },
   "outputs": [],
   "source": [
    "# Average over runs\n",
    "averaged_ref = metrics_ref.groupby(level=0).apply(lambda grp: grp.groupby(by=\"trainsize\").mean())\n",
    "averaged_ref.set_index([\"trainsize_p\"], append=True,inplace=True)\n",
    "\n",
    "# Keep just the necessary columns\n",
    "averaged_ref=averaged_ref[[\"eval_f1\",'eval_f1-l1l2',\"eval_f1-all\",\"eval_f1-l1\",\"eval_f1-l2\",'eval_f1-das']]\n",
    "\n",
    "# Set pretty names\n",
    "averaged_ref.index.names = ['Model','Trainset Size',\"%\"]\n",
    "averaged_ref.rename(columns={\"eval_f1\":\"P+L1+P+L2 (train)\",\n",
    "                             \"eval_f1-l1l2\":\"L1+L2\",\n",
    "                             'eval_f1-all':\"All\",\n",
    "                            \"eval_f1-l1\":\"Level 1\",\n",
    "                            \"eval_f1-l2\":\"Level 2\",\n",
    "                            'eval_f1-das':\"DAS alignement\"\n",
    "                         }, errors=\"raise\", inplace=True)\n",
    "averaged_ref.rename(mapper={\"camembert_io_ref\": \"CmBERT IO\",\"camembert_iob2_ref\": \"CmBERT IOB2\",\"prtn_camembert_io_ref\": \"Ptrn CmBERT IO\",\"prtn_camembert_iob2_ref\": \"Ptrn CmBERT IOB2\"}, errors=\"ignore\", inplace=True, axis=0)\n",
    "averaged_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtJTnFAyjLte"
   },
   "source": [
    "## 231.2 Create the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811
    },
    "id": "02lAmQ9sn8Es",
    "outputId": "eb6500fa-8eee-4475-f084-4e7ca34ed39d"
   },
   "outputs": [],
   "source": [
    "latex_table_ref = averaged_ref.copy()\n",
    "\n",
    "caption = \"F1 score measured on the fine-tuned models CmBERT, CmBERT+ptrn on reference dataset with Independent Flat NER layers approach (M1).\"\n",
    "print(latex_table_ref.to_latex(float_format=\"%.1f\", multirow=True, caption=caption))\n",
    "latex_table_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 232 - Experiment 2: tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_DIR_PERO = OUT_BASE / \"method_2/m2-220-experiment_2_metrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models metrics from metrics jsons\n",
    "res = []\n",
    "keys = []\n",
    "\n",
    "if os.path.exists(METRICS_DIR_PERO / \"221-camembert-ner-joint-labelling-io/run_2\"):\n",
    "    camembert_ner_io_pero = compile_metrics(METRICS_DIR_PERO / \"221-camembert-ner-joint-labelling-io\")\n",
    "    camembert_ner_io_pero[\"eval_precision-l1l2\"] = camembert_ner_io_pero[\"eval_precision\"]\n",
    "    camembert_ner_io_pero[\"eval_recall-l1l2\"] = camembert_ner_io_pero[\"eval_recall\"]\n",
    "    camembert_ner_io_pero[\"eval_f1-l1l2\"] = camembert_ner_io_pero[\"eval_f1\"]\n",
    "    camembert_ner_io_pero[\"eval_accuracy-l1l2\"] = camembert_ner_io_pero[\"eval_accuracy\"]\n",
    "    res.append(camembert_ner_io_pero)\n",
    "    keys.append(\"CmBERT IO\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_PERO / \"222-camembert-ner-joint-labelling-iob2/run_2\"):\n",
    "    camembert_ner_iob2_pero = compile_metrics(METRICS_DIR_PERO / \"222-camembert-ner-joint-labelling-iob2\")\n",
    "    res.append(camembert_ner_iob2_pero)\n",
    "    keys.append(\"CmBERT IOB2\")\n",
    "\n",
    "if os.path.exists(METRICS_DIR_PERO / \"223-pretrained-camembert-ner-joint-labelling-io/run_2\"):\n",
    "    ptrn_camembert_ner_io_pero = compile_metrics(METRICS_DIR_PERO / \"223-pretrained-camembert-ner-joint-labelling-io\")\n",
    "    ptrn_camembert_ner_io_pero[\"eval_precision-l1l2\"] = ptrn_camembert_ner_io_pero[\"eval_precision\"]\n",
    "    ptrn_camembert_ner_io_pero[\"eval_recall-l1l2\"] = ptrn_camembert_ner_io_pero[\"eval_recall\"]\n",
    "    ptrn_camembert_ner_io_pero[\"eval_f1-l1l2\"] = ptrn_camembert_ner_io_pero[\"eval_f1\"]\n",
    "    ptrn_camembert_ner_io_pero[\"eval_accuracy-l1l2\"] = ptrn_camembert_ner_io_pero[\"eval_accuracy\"]\n",
    "    res.append(ptrn_camembert_ner_io_pero)\n",
    "    keys.append(\"Ptrn CmBERT IO\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR_PERO / \"224-pretrained-camembert-ner-joint-labelling-iob2/run_2\"):\n",
    "    ptrn_camembert_ner_iob2_pero = compile_metrics(METRICS_DIR_PERO / \"224-pretrained-camembert-ner-joint-labelling-iob2\")\n",
    "    res.append(ptrn_camembert_ner_iob2_pero)\n",
    "    keys.append(\"Ptrn CmBERT IOB2\")\n",
    "    \n",
    "# Create the full table\n",
    "print(keys)\n",
    "metrics_raw_pero = pd.concat(res, keys=keys)\n",
    "metrics_raw_pero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_raw_pero.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store p/r/f1 as percentages\n",
    "eval_ = [\"eval_f1\",\"eval_f1-l1\",\"eval_f1-all\",\"eval_f1-l2\",'eval_f1-l1l2','eval_f1-das']\n",
    "metrics_pero = metrics_raw_pero.copy()\n",
    "metrics_pero[eval_] = metrics_raw_pero[eval_].multiply(100., axis=1)\n",
    "metrics_pero.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average over runs\n",
    "averaged_pero = metrics_pero.groupby(level=0).apply(lambda grp: grp.groupby(by=\"trainsize\").mean())\n",
    "averaged_pero.set_index([\"trainsize_p\"], append=True,inplace=True)\n",
    "\n",
    "# Keep just the necessary columns\n",
    "averaged_pero=averaged_pero[[\"eval_f1\",'eval_f1-l1l2',\"eval_f1-all\",\"eval_f1-l1\",\"eval_f1-l2\",'eval_f1-das']]\n",
    "\n",
    "# Set pretty names\n",
    "averaged_pero.index.names = ['Model','Trainset Size',\"%\"]\n",
    "averaged_pero.rename(columns={\"eval_f1\":\"P+L1+P+L2 (train)\",\n",
    "                             \"eval_f1-l1l2\":\"L1+L2\",\n",
    "                              'eval_f1-all':\"All\",\n",
    "                            \"eval_f1-l1\":\"Level 1\",\n",
    "                            \"eval_f1-l2\":\"Level 2\",\n",
    "                            'eval_f1-das':\"DAS alignement\"\n",
    "                         }, errors=\"raise\", inplace=True)\n",
    "averaged_pero.rename(mapper={\"camembert_io_pero\": \"CmBERT IO\",\"camembert_iob2_pero\": \"CmBERT IOB2\",\"prtn_camembert_io_pero\": \"Ptrn CmBERT IO\",\"prtn_camembert_iob2_pero\": \"Ptrn CmBERT IOB2\"}, errors=\"ignore\", inplace=True, axis=0)\n",
    "averaged_pero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table_pero = averaged_pero.copy()\n",
    "\n",
    "caption = \"F1 score measured on the fine-tuned models CmBERT, CmBERT+ptrn on noisy dataset with Joint-labelling approach (M2).\"\n",
    "print(latex_table_pero.to_latex(float_format=\"%.1f\", multirow=True, caption=caption))\n",
    "latex_table_pero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 233 - Experiments 1 & 2: table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build averaged table of F1-score for each dataset, each BERT-based model and each annotation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged = pd.concat([averaged_ref,averaged_pero],keys=[\"Reference\",\"Pero OCR\"])\n",
    "averaged = averaged.reset_index(level=[2,3], drop=True)\n",
    "averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = \"F1 score measured on the fine-tuned models CmBERTand CmBERT+ptrn on reference dataset and noisy dataset with Joint-labelling approach (M2).\"\n",
    "print(averaged.to_latex(float_format=\"%.1f\", multirow=True, caption=caption))\n",
    "averaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 234 : Experiments 1 and 2: table by classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pandas import json_normalize\n",
    "\n",
    "def compile_metrics_by_classes(path, classes): \n",
    "    rundirs = [f for f in os.listdir(path)]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for run_dir in rundirs:\n",
    "        if 'run' in run_dir:\n",
    "            run_path = path / run_dir\n",
    "            nrun = re.search(\"\\d+\",run_dir)[0]\n",
    "\n",
    "            files = [f for f in os.listdir(run_path) if \"test_\" in f and 'json' in f]\n",
    "            sizes = [int(re.search(\"\\d+\",f)[0]) for f in files]\n",
    "                \n",
    "            for file, size in zip(files,sizes):\n",
    "                file_path = run_path / file\n",
    "                dftmp = pd.read_json(file_path)\n",
    "                classes_dict = {key: dftmp[key] for key in classes}\n",
    "                dftmp = pd.DataFrame.from_dict(classes_dict)\n",
    "                dftmp = dftmp.T\n",
    "                dftmp['number'] = dftmp['number'].astype(int)\n",
    "                dftmp[\"trainsize\"] = size \n",
    "                dftmp[\"run\"] = nrun\n",
    "                dftmp[\"trainsize_p\"] = round(100 * size / MAX_TRAINSET_SIZE, 1)\n",
    "                df = pd.concat([df, dftmp])\n",
    "                df[\"classe\"] = df.index\n",
    "                \n",
    "    return df.groupby([\"run\",\"classe\"]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['eval_PER','eval_ACT','eval_ACT_L1','eval_ACT_L2','eval_DESC','eval_TITREH','eval_TITREP','eval_SPAT','eval_LOC','eval_CARDINAL','eval_FT'\n",
    " #'eval_TITRE'\n",
    "]\n",
    "\n",
    "# Load models metrics from metrics jsons\n",
    "res = []\n",
    "keys = []\n",
    "\n",
    "if os.path.exists(METRICS_DIR / \"211-camembert-ner-joint-labelling-io/run_2\"):\n",
    "    camembert_ner_io = compile_metrics_by_classes(METRICS_DIR / \"211-camembert-ner-joint-labelling-io\",classes)\n",
    "    res.append(camembert_ner_io)\n",
    "    keys.append(\"CmBERT IO\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR / \"212-camembert-ner-joint-labelling-iob2/run_2\"):\n",
    "    camembert_ner_iob2 = compile_metrics_by_classes(METRICS_DIR / \"212-camembert-ner-joint-labelling-iob2\",classes)\n",
    "    res.append(camembert_ner_iob2)\n",
    "    keys.append(\"CmBERT IOB2\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR / \"213-pretrained-camembert-ner-joint-labelling-io-classes/run_2\"):\n",
    "    ptrn_camembert_ner_io = compile_metrics_by_classes(METRICS_DIR / \"213-pretrained-camembert-ner-joint-labelling-io-classes\",classes)\n",
    "    res.append(ptrn_camembert_ner_io)\n",
    "    keys.append(\"Ptrn CmBERT IO\")\n",
    "    \n",
    "if os.path.exists(METRICS_DIR / \"214-pretrained-camembert-ner-joint-labelling-iob2/run_2\"):\n",
    "    ptrn_camembert_ner_iob2 = compile_metrics_by_classes(METRICS_DIR / \"214-pretrained-camembert-ner-joint-labelling-iob2-classes\",classes)\n",
    "    res.append(ptrn_camembert_ner_iob2)\n",
    "    keys.append(\"Ptrn CmBERT IOB2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_raw_classes = pd.concat(res, keys=keys)\n",
    "metrics_raw_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store p/r/f1 as percentages\n",
    "val = [\"f1\",\"precision\",\"recall\"]\n",
    "metrics_raw_classes = metrics_raw_classes.copy()\n",
    "metrics_raw_classes[val] = metrics_raw_classes[val].multiply(100., axis=1)\n",
    "metrics_raw_classes.head()\n",
    "metrics_raw_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average over runs\n",
    "averaged_classes = metrics_raw_classes.groupby(level=0).apply(lambda grp: grp.groupby(by=\"classe\").mean())\n",
    "averaged_classes.set_index([\"number\"],\n",
    "                   append=True,\n",
    "                   inplace=True)\n",
    "\n",
    "# Keep just the necessary columns\n",
    "averaged_classes=averaged_classes[[\"precision\",\"recall\",\"f1\"]]\n",
    "\n",
    "# Set pretty names\n",
    "averaged_classes.index.names = ['Test','Entity type',\"Count\"]\n",
    "averaged_classes.rename(mapper={'eval_PER':'PER','eval_ACT':'ACT','eval_ACT_L1':'ACT L1 only','eval_ACT_L2':'ACT L2 only','eval_DESC':'DESC','eval_TITREH':'TITREH','eval_TITREP':'TITREP','eval_SPAT':'SPAT','eval_LOC':'LOC','eval_CARDINAL':'CARDINAL','eval_FT':'FT'}, errors=\"ignore\", inplace=True, axis=0)\n",
    "averaged_classes"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DAS22-experiment1-figures.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
