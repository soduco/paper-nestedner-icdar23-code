{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8189029c-0b96-45a0-8d89-6c49ffdda9b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 200 - Datasets generation for nested-NER\n",
    "\n",
    "Outputs:\n",
    "Train, dev & test datasets for multilayers NER experiment 1 (with ref dataset) and experiment 2 (with pero OCR dataset)\n",
    "\n",
    "<b>Experiment 1 : Groundtruth dataset</b>\n",
    "* `m2-experiment_1_prepared_dataset_ref_io_camembert_ner`\n",
    "* `m2-experiment_1_prepared_dataset_ref_io_pretrained_camembert_ner`\n",
    "* `m2-experiment_1_prepared_dataset_ref_iob2_camembert_ner`\n",
    "* `m2-experiment_1_prepared_dataset_ref_iob2_pretrained_camembert_ner`\n",
    "\n",
    "<b>Experiment 2 : Pero OCR dataset</b>\n",
    "* `m2-experiment_2_prepared_dataset_pero_ocr_io_camembert_ner`\n",
    "* `m2-experiment_2_prepared_dataset_pero_ocr_io_pretrained_camembert_ner`\n",
    "* `m2-experiment_2_prepared_dataset_pero_ocr_iob2_camembert_ner`\n",
    "* `m2-experiment_2_prepared_dataset_pero_ocr_iob2_pretrained_camembert_ner`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77ed69-9fa6-4462-9b1b-00ecb3f17392",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff04646-29cc-4cb7-a931-45b18016ed64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "ENV_IS_GOOGLE_COLAB = True if 'google.colab' in str(get_ipython()) else False\n",
    "os.environ[\"ENV_IS_GOOGLE_COLAB\"] = str(ENV_IS_GOOGLE_COLAB)\n",
    "\n",
    "if ENV_IS_GOOGLE_COLAB:\n",
    "  from google.colab import drive\n",
    "  mountpoint = Path(\"/content/drive\")\n",
    "  drive.mount(str(mountpoint)) # Mount gdrive to BASE\n",
    "  base = mountpoint / \"MyDrive/article_icdar_2023\" # Adapt this to your situation\n",
    "  sys.path.append(str(base)) # Add BASE to Python Path\n",
    "  BASE = Path(base).resolve() # Make BASE absolute\n",
    "  DATASETS =  BASE / \"dataset_ICDAR\"\n",
    "  OUT_BASE = BASE / \"res_ICDAR/method_2\"\n",
    "else:\n",
    "  BASE = Path().resolve() # Directory of this approach\n",
    "  #Adapt this to your situation\n",
    "  DATASETS = Path('../dataset_ICDAR').resolve() #Where your data are located befor Dataset object creation\n",
    "  OUT_BASE = Path('../res_ICDAR/method_2').resolve() #Where you save the results of this notebook\n",
    "\n",
    "print(sys.path)\n",
    "print(BASE)\n",
    "print(DATASETS)\n",
    "print(OUT_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3ab2a8-d126-4a36-87c1-5bd934a2f5c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16874c6b-e422-4e07-b31c-856841a71300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GLOBAL CONSTANTS\n",
    "import config\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "config.SPLIT_SEED = 42 # Random seed used in train/dev/test. Do not change it if you want to recreate the paper results.\n",
    "config.DEBUG = False # If true, text versions of the spacy datasets will be saved along with the .spacy files.\n",
    "\n",
    "MODEL_NAME = \"pretrained_camembert_ner\"\n",
    "#camembert_ner OR\n",
    "#pretrained_camembert_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42306065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on disk\n",
    "from camembert_utils.util_IO import _convert_tokenizer\n",
    "print(\"Tokenizer called in util_IO.py\")\n",
    "_convert_tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save on disk\n",
    "from camembert_utils.util_IOB2 import _convert_tokenizer\n",
    "print(\"Tokenizer called in util_IOB2.py\")\n",
    "_convert_tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b49be7",
   "metadata": {},
   "source": [
    "# 01. Experiment #1 : Reference dataset with joint-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394188b9-f83e-4545-b592-60328fdfa661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "GOLD_REF = DATASETS / \"41-ner_ref_from_pero/gold.csv\"\n",
    "assert GOLD_REF.exists()\n",
    "\n",
    "#gold_reference = pd.read_csv(GOLD_REF, header=None, names=[\"ner_xml\",\"book\"],skipinitialspace='True')\n",
    "with open(GOLD_REF,'r',encoding='utf8') as f:\n",
    "    lines = f.readlines()\n",
    "    res = []\n",
    "    for line in lines:\n",
    "        l = line.split('\", \"')\n",
    "        res.append([l[0][1:],l[1][:-2]])\n",
    "gold_reference = pd.DataFrame(res,columns=[\"ner_xml\",\"book\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c9844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TITRE-H and TITRE-P labels to transformers NER Pipeline\n",
    "for i in range(len(gold_reference)):\n",
    "    if '<TITRE-H>' in gold_reference['ner_xml'][i]:\n",
    "        gold_reference['ner_xml'][i] = gold_reference['ner_xml'][i].replace('TITRE-H','TITREH')\n",
    "    if '<TITRE-P>' in gold_reference['ner_xml'][i]:\n",
    "        gold_reference['ner_xml'][i] = gold_reference['ner_xml'][i].replace('TITRE-P','TITREP')\n",
    "gold_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae09bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_util import train_dev_test_split, unwrap # Local imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CONSTANTS\n",
    "MIN_TRAINSET_SIZE = 30\n",
    "\n",
    " # Split 72/8/20% w. stratified sampling on directories names\n",
    "train, dev, test = train_dev_test_split(gold_reference.to_numpy())\n",
    "\n",
    "# Iteratively split the trainset in half to create smaller trainsets\n",
    "exp1_trainsets = [train]\n",
    "t_len = len(train)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        current = exp1_trainsets[-1]\n",
    "        _, groups = unwrap(current)\n",
    "        smaller, rest = train_test_split(\n",
    "            current,\n",
    "            train_size=0.5,\n",
    "            shuffle=True,\n",
    "            random_state=config.SPLIT_SEED,\n",
    "            stratify=groups,\n",
    "        )\n",
    "        t_len = len(rest)\n",
    "        if t_len < MIN_TRAINSET_SIZE:\n",
    "            break\n",
    "        exp1_trainsets.append(smaller)\n",
    "\n",
    "    except ValueError:\n",
    "        # Stop now if we encounter the error \"The least populated class in y has only 1 member\".\n",
    "        break\n",
    "\n",
    "[len(s) for s in exp1_trainsets] # Should be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Dev set should contain 676 examples\n",
    "assert len(dev) == 676\n",
    "\n",
    "# Test set should contain 1685 examples\n",
    "assert len(test) == 1685\n",
    "\n",
    "# Lenghts of exp1_trainsets should be fixed\n",
    "assert sorted([len(s) for s in exp1_trainsets] ) == sorted([6084, 3042, 1521, 760, 380, 190, 95, 47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a497b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camembert_utils.tools import createStatsTab\n",
    "\n",
    "createStatsTab(train,dev,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b00def",
   "metadata": {},
   "source": [
    "## IO Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camembert_utils.util_IO import save_dataset_io # Local import\n",
    "\n",
    "# Save on disk\n",
    "output_directory = OUT_BASE / f\"m2-experiment_1_prepared_dataset_ref_io_{MODEL_NAME}\"\n",
    "output_directory.mkdir(exist_ok=True, parents=True) # Create if necessary\n",
    "print(output_directory)\n",
    "\n",
    "for train in exp1_trainsets:\n",
    "    datasets = [train, dev, test]\n",
    "    save_dataset_io(output_directory, datasets, [\"train\",\"dev\",\"test\"], suffix=len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d8640",
   "metadata": {},
   "source": [
    "### IOB2 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d54b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camembert_utils.util_IOB2 import save_dataset_iob2 # Local import\n",
    "\n",
    "# Save on disk\n",
    "output_directory = OUT_BASE / f\"m2-experiment_1_prepared_dataset_ref_iob2_{MODEL_NAME}\"\n",
    "output_directory.mkdir(exist_ok=True, parents=True) # Create if necessary\n",
    "print(output_directory)\n",
    "\n",
    "for train in exp1_trainsets:\n",
    "    datasets = [train, dev, test]\n",
    "    save_dataset_iob2(output_directory, datasets, [\"train\",\"dev\",\"test\"], suffix=len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5dd8f",
   "metadata": {},
   "source": [
    "# 0.2 # Experiment 2 : Pero OCR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567fe090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "GOLD_REF = DATASETS / \"31-ner_align_pero/gold.csv\"\n",
    "gold_reference = pd.read_csv(GOLD_REF, header=None, names=[\"ner_xml\",\"book\"],skipinitialspace='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ed845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TITRE-H and TITRE-P labels to transformers NER Pipeline\n",
    "for i in range(len(gold_reference)):\n",
    "    if '<TITRE-H>' in gold_reference['ner_xml'][i]:\n",
    "        gold_reference['ner_xml'][i] = gold_reference['ner_xml'][i].replace('TITRE-H','TITREH')\n",
    "    if '<TITRE-P>' in gold_reference['ner_xml'][i]:\n",
    "        gold_reference['ner_xml'][i] = gold_reference['ner_xml'][i].replace('TITRE-P','TITREP')\n",
    "gold_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_util import train_dev_test_split, unwrap # Local imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# CONSTANTS\n",
    "MIN_TRAINSET_SIZE = 30\n",
    "\n",
    " # Split 72/8/20% w. stratified sampling on directories names\n",
    "train, dev, test = train_dev_test_split(gold_reference.to_numpy())\n",
    "\n",
    "# Iteratively split the trainset in half to create smaller trainsets\n",
    "exp1_trainsets = [train]\n",
    "t_len = len(train)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        current = exp1_trainsets[-1]\n",
    "        _, groups = unwrap(current)\n",
    "        smaller, rest = train_test_split(\n",
    "            current,\n",
    "            train_size=0.5,\n",
    "            shuffle=True,\n",
    "            random_state=config.SPLIT_SEED,\n",
    "            stratify=groups,\n",
    "        )\n",
    "        t_len = len(rest)\n",
    "        if t_len < MIN_TRAINSET_SIZE:\n",
    "            break\n",
    "        exp1_trainsets.append(smaller)\n",
    "\n",
    "    except ValueError:\n",
    "        # Stop now if we encounter the error \"The least populated class in y has only 1 member\".\n",
    "        break\n",
    "\n",
    "[len(s) for s in exp1_trainsets] # Should be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b09aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "\n",
    "# Dev set should contain 676 examples\n",
    "assert len(dev) == 676\n",
    "\n",
    "# Test set should contain 1685 examples\n",
    "assert len(test) == 1685\n",
    "\n",
    "# Lenghts of exp1_trainsets should be fixed\n",
    "assert sorted([len(s) for s in exp1_trainsets] ) == sorted([6084, 3042, 1521, 760, 380, 190, 95, 47])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e9ef6",
   "metadata": {},
   "source": [
    "### Save test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save test subset in csv format for qualitative analysis\n",
    "with open(\"./pero_ocr_dataset_test_subset.csv\",'w',encoding='utf8') as tfile:\n",
    "    for line in test:\n",
    "        tfile.write('\"' + line[0] + '\", \"' + line[1] + '\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a82d9a",
   "metadata": {},
   "source": [
    "### Entities count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camembert_utils.tools import createStatsTab\n",
    "\n",
    "createStatsTab(train,dev,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe2bd2d",
   "metadata": {},
   "source": [
    "### IO Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camembert_utils.util_IO import save_dataset_io # Local import\n",
    "\n",
    "# Save on disk\n",
    "output_directory = OUT_BASE / f\"m2-experiment_2_prepared_dataset_pero_ocr_io_{MODEL_NAME}\"\n",
    "output_directory.mkdir(exist_ok=True, parents=True) # Create if necessary\n",
    "print(output_directory)\n",
    "\n",
    "for train in exp1_trainsets:\n",
    "    datasets = [train, dev, test]\n",
    "    save_dataset_io(output_directory, datasets, [\"train\",\"dev\",\"test\"], suffix=len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d45c8",
   "metadata": {},
   "source": [
    "### IOB2 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dff94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camembert_utils.util_IOB2 import save_dataset_iob2 # Local import\n",
    "\n",
    "# Save on disk\n",
    "output_directory = OUT_BASE / f\"m2-experiment_2_prepared_dataset_pero_ocr_iob2_{MODEL_NAME}\"\n",
    "output_directory.mkdir(exist_ok=True, parents=True) # Create if necessary\n",
    "print(output_directory)\n",
    "\n",
    "for train in exp1_trainsets:\n",
    "    datasets = [train, dev, test]\n",
    "    save_dataset_iob2(output_directory, datasets, [\"train\",\"dev\",\"test\"], suffix=len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf80f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
